{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16 - Uncertainty Propagation: Polynomial Chaos I\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Define the Hilbert space of square integrable functions.\n",
    "+ Define orthogonal polynomials (polynomial chaos) as a basis of the aforementioned Hilbert space.\n",
    "+ Demonstrate standard orthogonal polynomials (Hermite, Legendre, etc.).\n",
    "+ Demonstrate basic properties of polynomial expansions.\n",
    "+ Propagate uncertainty through an initial value problem using intrusive polynomial chaos.\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ These notes.\n",
    "\n",
    "**NOTE**\n",
    "+ If you are in class, skip the section on Hilbert spaces. Move directly to the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import orthpol  # This is the package we will use to construct orthogonal polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hilbert Space of Square Integrable Functions\n",
    "\n",
    "### Space of Square Integrable Functions\n",
    "Let $X$ be a random vector with probability density $p(x)$.\n",
    "Consider the space of all square integrable functions of $X$:\n",
    "$$\n",
    "\\mathcal{F}\\equiv \\mathcal{F}_d = \\left\\{f:\\mathbb{R}^d\\rightarrow\\mathbb{R}\\;\\mbox{such that}\\;\\mathbb{E}_p\\left[f^2(X)\\right]<\\infty\\right\\}.\n",
    "$$\n",
    "\n",
    "It is easy to see that $\\mathcal{F}$ is a *vector space* since for any $f,g\\in\\mathcal{F}$ and any $\\mu,\\lambda\\in\\mathbb{R}$, we have:\n",
    "$$\n",
    "\\mu f + \\lambda g \\in \\mathcal{F}.\n",
    "$$\n",
    "\n",
    "Now define the function:\n",
    "$$\n",
    "\\langle\\cdot,\\cdot\\rangle: \\mathbb{F}\\times \\mathbb{F}\\rightarrow\\mathbb{R},\n",
    "$$\n",
    "by\n",
    "$$\n",
    "\\langle f, g\\rangle := \\mathbb{E}_p\\left[f(X)g(X)\\right] = \\int f(x)g(x)p(x)dx.\n",
    "$$\n",
    "Again, it is easy to see that $\\langle\\cdot,\\cdot\\rangle$ defines an *inner product*, i.e., that for any $f, g, h\\in\\mathcal{F}$ and any $\\mu\\in\\mathbb{R}$, we have:\n",
    "$$\n",
    "\\langle f, f \\rangle \\ge 0,\n",
    "$$\n",
    "$$\n",
    "\\langle f, g \\rangle = \\langle g, f \\rangle,\n",
    "$$\n",
    "$$\n",
    "\\langle \\mu f, g\\rangle = \\mu \\langle f, g\\rangle,\n",
    "$$\n",
    "$$\n",
    "\\langle f, g + h\\rangle = \\langle f, g\\rangle + \\langle f, h\\rangle.\n",
    "$$\n",
    "The vector space $\\mathcal{F}$ equipped with the inner product $\\langle\\cdot,\\cdot\\rangle$ is an *inner product* space.\n",
    "\n",
    "### Defining a Norm from an Inner Product\n",
    "A norm $\\parallel\\cdot\\parallel$ measures the \"magnitude\" of an element of a vector space.\n",
    "The norm satisfies the following formulas for any $f\\in\\mathcal{F}$ and $\\mu\\in\\mathbb{R}$:\n",
    "$$\n",
    "\\parallel f\\parallel \\ge 0,\n",
    "$$\n",
    "$$\n",
    "\\parallel f\\parallel = 0\\Rightarrow f = 0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\parallel \\mu f\\parallel = |\\mu|\\parallel f\\parallel.\n",
    "$$\n",
    "Using the dot product one can define a *norm* over $\\mathcal{F}$:\n",
    "$$\n",
    "\\parallel f \\parallel := \\langle f, f\\rangle^{\\frac{1}{2}} = \\left\\{\\int f^2(x)p(x)dx\\right\\}^{\\frac{1}{2}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### Defining a Metric from a Norm\n",
    "A metric $d(\\cdot,\\cdot)$ defines \"distances\" between two elements of a vector space.\n",
    "It is essential for defining convergence of sequences in the vector space.\n",
    "A metric satisfies the following formulas for any $f,g,h\\in\\mathcal{F}$:\n",
    "$$\n",
    "d(f,g) \\ge 0,\n",
    "$$\n",
    "$$\n",
    "d(f,g) = 0 \\Rightarrow f = g,\n",
    "$$\n",
    "$$\n",
    "d(g,f) = d(f,g),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "d(f,h) \\le d(f,g) + d(g,h).\n",
    "$$\n",
    "If you have a norm, you can define a metric:\n",
    "$$\n",
    "d(f,g) := \\parallel f - g\\parallel = \\left\\{\\int\\left(f(x)-g(x)\\right)^2p(x)\\right\\}^{\\frac{1}{2}}.\n",
    "$$\n",
    "\n",
    "### The Space of Square Integrable Functions is a Hilbert Space \n",
    "Now, it is not trivial to show this, but the space of integrable functions equipped with the inner product we defined above is *complete*.\n",
    "In words, the space is complete if any sequence of functions that \"looks like it converges\" does indeed converge to a function that is inside the space.\n",
    "Let $f_1,f_2,\\dots\\in\\mathcal{F}$ be a sequence of functions that \"looks like it converges\".\n",
    "Mathematically, a sequence looks like it converges if subsequent terms come arbitrarily close togther.\n",
    "With symbols $\\forall \\epsilon > 0$, there exists a $n_0 = n_0(\\epsilon)$ such that for all $n,m\\ge n_0$ we have:\n",
    "$$\n",
    "d(f_n, f_m) < \\epsilon.\n",
    "$$\n",
    "We call such a sequence a *Cauchy sequence*.\n",
    "A space is complete if and only if every Cauchy sequence convergence to an $f\\in\\mathcal{F}$.\n",
    "The space of square inegrable functions is complete, but the proof is not trivial.\n",
    "A inner product space that is complete is called a *Hilbert space*.\n",
    "So $\\mathcal{F}$ is a Hilbert space.\n",
    "\n",
    "### Orthonormal Basis\n",
    "A subset of functions $\\phi_1,\\phi_2,\\dots\\in\\mathcal{F}$ is called an *orthgonormal basis* if\n",
    "\n",
    "1. The functions are orthogonal to each other:\n",
    "$$\n",
    "\\langle f_i, f_j\\rangle = \\mathbb{E}_p[f_i(X)f_j(X)] = \\int f_i(x)f_j(x)p(x)dx = 0,\n",
    "$$\n",
    "for all $i\\not=j$.\n",
    "\n",
    "2. The functions are normalized:\n",
    "$$\n",
    "\\parallel f_i\\parallel^2 = \\langle f_i, f_i\\rangle = \\mathbb{E}_p\\left[f_i^2(X)\\right] = \\int f_i^2(x)p(x)dx = 1.\n",
    "$$\n",
    "\n",
    "3. Any functions $f\\in\\mathcal{F}$ can be written as:\n",
    "$$\n",
    "f = \\sum_{i=1}^\\infty c_i \\phi_i.\n",
    "$$\n",
    "\n",
    "Note the first two properties can be written in one equation using the Kronecker delta:\n",
    "$$\n",
    "\\langle f_i, f_j\\rangle = \\delta_{ij} = \\begin{cases}0,&i\\not=j\\\\ 1,&\\text{otherwise}.\\end{cases}\n",
    "$$\n",
    "\n",
    "### Expanding a Vector in an Orthonormal Basis\n",
    "There are some properties of Hilbert spaces that are extremely easy to derive.\n",
    "Let $\\phi_1,\\phi_2,\\dots$ be an orthonormal basis and $f\\in\\mathcal{F}$.\n",
    "The function can be written as\n",
    "$$\n",
    "f = \\sum_{i=1}^\\infty c_i \\phi_i,\n",
    "$$\n",
    "for some coefficients $c_1,c_2,\\dots$.\n",
    "How can we get these coefficients?\n",
    "Using only the properties of the inner product we get: \n",
    "$$\n",
    "\\langle f, \\phi_j\\rangle = \\left\\langle\\sum_{i=1}^\\infty, \\phi_j\\right\\rangle = \\sum_{i=1}^\\infty c_i\\langle \\phi_i, \\phi_j\\rangle = \\sum_{i=1}^\\infty c_i\\delta_{ij} = c_j.\n",
    "$$\n",
    "Therefore, (one way) to get the coefficients is by performing the integral:\n",
    "$$\n",
    "c_j = \\langle f, \\phi_j\\rangle = \\int f(x)\\phi_j(x)p(x)dx.\n",
    "$$\n",
    "This approach will give rise to the so-called \"stochastic collocation methods\" which we will study on Lecture 17.\n",
    "\n",
    "### Parseval's Formula: First Connection to Uncertainty Propagation\n",
    "Remember the definition of the norm of a function.\n",
    "We will show that:\n",
    "$$\n",
    "\\int f^2(x)p(x)dx = \\parallel f\\parallel^2 = \\sum_{i=1}^\\infty c_i^2.\n",
    "$$\n",
    "This is known as Parseval's formula.\n",
    "It is useful in uncertainty propagation because it is a crucial part for the calculation of the variance.\n",
    "The proof is trivial:\n",
    "$$\n",
    "\\parallel f\\parallel^2 = \\langle f, f\\rangle = \\left\\langle \\sum_{i=1}^\\infty c_i\\phi, \\sum_{j=1}^\\infty c_j\\phi_j\\right\\rangle = \\sum_{i=1}^\\infty\\sum_{j=1}^\\infty c_ic_j\\langle \\phi_i,\\phi_j\\rangle = \\sum_{i=1}^\\infty\\sum_{j=1}^\\infty c_ic_j\\delta_{ij} = \\sum_{i=1}^\\infty|c_i|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Space of Square Integrable Scalar Functions and 1D Orthogonal Polynomials\n",
    "\n",
    "Let $X$ be a random variable (a scalar variable) with probability density $p(x)$.\n",
    "In this section, we will construct a polynomial orthonormal basis for the space of square integrable functions $f:\\mathbb{R}\\rightarrow \\mathbb{R}$.\n",
    "Let the underlying vector space be $\\mathcal{F}_1$ with the previously defined inner product.\n",
    "\n",
    "### Definition of 1D Polynomials\n",
    "A *polynomial* of degree $\\rho$ is a scalar function of a real variable:\n",
    "$$\n",
    "g(x;a) = a_0 + a_1 x + a_2 x^2 + \\dots + a_\\rho x^\\rho,\n",
    "$$\n",
    "where $a = (a_0, a_1, \\dots, a_\\rho)$ are the coefficients of the polynomial.\n",
    "\n",
    "\n",
    "### Orthonormal Polynomials\n",
    "A set $\\phi_1,\\phi_2,\\dots$ of 1D polynomials are *orthonormal polynomials* with respect to $p(x)$ if they form an orthonormal basis of the underlying Hilbert space.\n",
    "In other words:\n",
    "\n",
    "1. Each $\\phi_i$ must be a polynomial:\n",
    "$$\n",
    "\\phi_i(x) = a_{0i} + a_{1i}x + \\dots + a_{\\rho_i i}x^{\\rho_i}.\n",
    "$$\n",
    "\n",
    "2. For any $i\\not=j$ we have:\n",
    "$$\n",
    "\\langle\\phi_i, \\phi_j\\rangle = \\mathbb{E}_p[\\phi_i(X)\\phi_j(X)] = \\int_{-\\infty}^\\infty \\phi_i(x)\\phi_j(x)p(x)dx = 0.\n",
    "$$\n",
    "\n",
    "3. They are normalized:\n",
    "$$\n",
    "\\parallel \\phi_i\\parallel^2 = \\langle \\phi_i,\\phi_i\\rangle = \\int_{-\\infty}^\\infty \\phi_i^2(x)p(x)dx = 1.\n",
    "$$\n",
    "\n",
    "For some weird reason the set of such polynomials are called \"polynomial chaos.\"\n",
    "You can call them orthonormal polynomials with respect to $p(x)$...\n",
    "\n",
    "### Constructing Orthogonal Polynomials\n",
    "\n",
    "The easiest way to construct orthogonal polynomials is to start from the monomials $1,x,x^2,\\dots$ and apply the [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram–Schmidt_process) using the inner product we defined previously.\n",
    "With this procedure, the first polynomial will always be the constant:\n",
    "$$\n",
    "\\phi_1(x) = 1.\n",
    "$$\n",
    "\n",
    "Despite the fact that you can carry out the Gram-Schmidt process for many choices of $p(x)$, we will rely on the algorithms developed by [Walter Gautschi](https://www.cs.purdue.edu/homes/wxg/):\n",
    "\n",
    "+ [Gautschi, W. (1994). Algorithm 726: ORTHPOL–a package of routines for generating orthogonal polynomials and Gauss-type quadrature rules. ACM Transactions on Mathematical Software (TOMS), 20(1), 21–62.](https://dl.acm.org/citation.cfm?id=174605).\n",
    "\n",
    "He has developed a quite stable suite of algorithms for calculating these polynomials and he has implemented them in Fortran called [ORTHPOL](https://dl.acm.org/ft_gateway.cfm?id=174605&type=gz&path=%2F180000%2F174605%2Fsupp%2F726%2Egz&supp=1&dwn=1).\n",
    "We will not worry about the details of these calculations or the representation of the polynomials because they are quite involved.\n",
    "We will just use the Python interface to ORTHPOL, [py-orthpol](https://github.com/PredictiveScienceLab/uq-course), developed by Bilionis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: The Standard Normal and the Hermite Polynomials\n",
    "\n",
    "Let $X\\sim\\mathcal{N}(0,1)$. The orthogonal polynomials in this case are known as the [Hermite polynomials](https://en.wikipedia.org/wiki/Hermite_polynomials).\n",
    "They are known analytically.\n",
    "The first few are:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\phi_1(x) &=& 1,\\\\\n",
    "\\phi_2(x) &=& x,\\\\\n",
    "\\phi_3(x) &=& x^2 - 1,\\\\\n",
    "\\phi_4(x) &=& x^3 - 3x,\\\\\n",
    "\\phi_5(x) &=& x^4 - 6x^2 + 3.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The random variable you wish to consider\n",
    "X = st.norm()\n",
    "# The maximum polynomial degree you want\n",
    "degree = 3\n",
    "# Construct the orthogonal polynomials\n",
    "Phi_set = orthpol.OrthogonalPolynomial(degree, X)\n",
    "\n",
    "# Plot the probability density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-3, 3, 200)\n",
    "ax.plot(x, X.pdf(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "# Evaluate the orhtogonal polynomials on all these x's\n",
    "phi_x = Phi_set(x)    # 200 x (degree + 1)\n",
    "# Plot each one of them\n",
    "ax.plot(x, phi_x);\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\phi_i(x)$')\n",
    "ax.set_title('$X\\sim\\mathcal{N}(0,1)$: Hermite Polynomials')\n",
    "plt.legend(['$\\phi_{%d}(x)$' % i for i in range(1, degree + 1)], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: The Exponential and the Laguerre Polynomials\n",
    "\n",
    "Let $X\\sim\\mathcal{E}(1)$. The orthogonal polynomials in this case are known as the [Laguerre polynomials](https://en.wikipedia.org/wiki/Laguerre_polynomials).\n",
    "They are known analytically.\n",
    "The first few are:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\phi_1(x) &=& 1,\\\\\n",
    "\\phi_2(x) &=& -x + 1,\\\\\n",
    "\\phi_3(x) &=& \\frac{1}{2}\\left(x^2 - 4x + 2\\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The random variable you wish to consider\n",
    "X = st.expon()\n",
    "# The maximum polynomial degree you want\n",
    "degree = 3\n",
    "# Construct the orthogonal polynomials\n",
    "Phi_set = orthpol.OrthogonalPolynomial(degree, X)\n",
    "\n",
    "# Plot the probability density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 5, 200)\n",
    "ax.plot(x, X.pdf(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "# Evaluate the orhtogonal polynomials on all these x's\n",
    "phi_x = Phi_set(x)    # 200 x (degree + 1)\n",
    "# Plot each one of them\n",
    "ax.plot(x, phi_x);\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\phi_i(x)$')\n",
    "ax.set_title('$X\\sim\\mathcal{E}(1)$: Laguerre Polynomials')\n",
    "plt.legend(['$\\phi_{%d}(x)$' % i for i in range(1, degree + 1)], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: The Uniform and the Legendre Polynomials\n",
    "\n",
    "Let $X\\sim\\mathcal{U}(-1, 1)$. The orthogonal polynomials in this case are known as the [Laguerre polynomials](https://en.wikipedia.org/wiki/Laguerre_polynomials).\n",
    "They are known analytically.\n",
    "The first few are:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\phi_1(x) &=& 1,\\\\\n",
    "\\phi_2(x) &=& x,\\\\\n",
    "\\phi_3(x) &=& \\frac{1}{2}\\left(3x^2 -1\\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, instead of the random variable let's use the the p(x) derectly\n",
    "p = lambda x: 0.5\n",
    "# The maximum polynomial degree you want\n",
    "degree = 4\n",
    "# Construct the orthogonal polynomials\n",
    "Phi_set = orthpol.OrthogonalPolynomial(degree,\n",
    "                                       wf=p,    # The weight function (or pdf)\n",
    "                                       left=-1, # The left bound\n",
    "                                       right=1  # The right bound\n",
    "                                       )\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "# Evaluate the orhtogonal polynomials on all these x's\n",
    "x = np.linspace(-1, 1, 200)\n",
    "phi_x = Phi_set(x)    # 200 x (degree + 1)\n",
    "# Plot each one of them\n",
    "ax.plot(x, phi_x);\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\phi_i(x)$')\n",
    "ax.set_title('$X\\sim\\mathcal{U}(-1, 1)$: Legendre Polynomials')\n",
    "plt.legend(['$\\phi_{%d}(x)$' % i for i in range(1, degree + 1)], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: We can do it for any probability density\n",
    "\n",
    "We can construct orthonormal for any random variable $X$.\n",
    "Let's do it for a mixture of Gaussians:\n",
    "$$\n",
    "p(x) = \\pi_1 \\mathcal{N}(x|\\mu_1,\\sigma_1^2) + \\pi_2\\mathcal{N}(x|\\mu_2,\\sigma_2^2),\n",
    "$$\n",
    "for $\\pi_1 + \\pi_2 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The random variable you wish to consider\n",
    "X1 = st.norm(loc=-1, scale=0.4)\n",
    "pi_1 = 0.2\n",
    "X2 = st.norm(loc=+1, scale=0.4)\n",
    "pi_2 = 0.8\n",
    "\n",
    "p = lambda x: pi_1 * X1.pdf(x) + pi_2 * X2.pdf(x)\n",
    "\n",
    "# The maximum polynomial degree you want\n",
    "degree = 5\n",
    "# Construct the orthogonal polynomials\n",
    "Phi_set = orthpol.OrthogonalPolynomial(degree, wf=p, left=-np.inf, right=np.inf)\n",
    "\n",
    "# Plot the probability density\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-2, 2, 200)\n",
    "ax.plot(x, p(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "# Evaluate the orhtogonal polynomials on all these x's\n",
    "phi_x = Phi_set(x)    # 200 x (degree + 1)\n",
    "# Plot each one of them\n",
    "ax.plot(x, phi_x);\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\phi_i(x)$')\n",
    "ax.set_title('$X\\sim\\sum_i\\pi_i\\mathcal{N}(\\mu_i,\\sigma_i)$: Whatever Polynomials')\n",
    "plt.legend(['$\\phi_{%d}(x)$' % i for i in range(1, degree + 1)], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "+ Generate the first few orthogonal polynomials for a Beta distribution.\n",
    "\n",
    "+ Generate the first few orthogonal polyonomials for a Log-normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Space of Square Integrable Functions and Higher-Dimensional Orthogonal Polynomials\n",
    "\n",
    "Let $X$ be a random vector with probability density $p(x)$.\n",
    "In this section, we will construct a polynomial orthonormal basis for the space of square integrable functions $f:\\mathbb{R}^d\\rightarrow \\mathbb{R}$.\n",
    "Let the underlying vector space be $\\mathcal{F}_d$ with the previously defined inner product.\n",
    "\n",
    "### Definition of Polynomials\n",
    "A *polynomial* of degree $\\rho$ is a scalar function of the vector variable:\n",
    "$$\n",
    "g(x;a) = \\sum_{|i_1+\\dots +i_d|\\le \\rho} a_{i_1\\dots i_d}x_1^{i_1}\\dots x_d^{i_d},\n",
    "$$\n",
    "where $a = (a_{0\\dots 0},\\dots,)$ are the coefficients of the polynomial.\n",
    "\n",
    "\n",
    "### Orthonormal Polynomials\n",
    "A set $\\phi_1,\\phi_2,\\dots$ of polynomials are *orthonormal polynomials* with respect to $p(x)$ if they form an orthonormal basis of the underlying Hilbert space.\n",
    "In other words:\n",
    "\n",
    "1. Each $\\phi_i$ must be a polynomial:\n",
    "$$\n",
    "\\phi_i(x) = \\sum_{|i_1+\\dots +i_d|\\le \\rho} a_{i_1\\dots i_d}x_1^{i_1}\\dots x_d^{i_d}.\n",
    "$$\n",
    "\n",
    "2. For any $i\\not=j$ we have:\n",
    "$$\n",
    "\\langle\\phi_i, \\phi_j\\rangle = \\mathbb{E}_p[\\phi_i(X)\\phi_j(X)] = \\int_{-\\infty}^\\infty\\dots\\int_{-\\infty}^\\infty\n",
    "\\phi_i(x)\\phi_j(x)p(x)dx_1\\dots dx_d = 0.\n",
    "$$\n",
    "\n",
    "3. They are normalized:\n",
    "$$\n",
    "\\parallel \\phi_i\\parallel^2 = \\langle \\phi_i,\\phi_i\\rangle = \\int_{-\\infty}^\\infty\\dots\\int_{-\\infty}^\\infty \\phi_i^2(x)p(x)dx_1\\dots dx_d = 1.\n",
    "$$\n",
    "\n",
    "For some weird reason the set of such polynomials are called \"polynomial chaos.\"\n",
    "You can call them orthonormal polynomials with respect to $p(x)$...\n",
    "\n",
    "### Constructing Orthogonal Polynomials in Higher-Dimensions\n",
    "\n",
    "The [Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram–Schmidt_process) would work here as well (you can start from all monomials).\n",
    "However, it is not very convenient.\n",
    "\n",
    "Let's take the special case of a random vector $X=(X_1,\\dots,X_d)$ that consists of independent random variables.\n",
    "This may look like a very special case, but (at least in principle) it is always possible to transofrm the original random vector so that this assumption is satisfied, see the [Rosenblatt Transformation](http://doc.openturns.org/openturns-0.13.2/doc/html/ReferenceGuide/output/OpenTURNS_ReferenceGuidesu51.html).\n",
    "Under the independence assumption the probability density of $X$ factorizes:\n",
    "$$\n",
    "p(x) = p(x_1,\\dots,x_d) = \\prod_{i=1}^d p_i(x_i),\n",
    "$$\n",
    "where $p_i(x_i)$ is the PDF of the $i$-th component $X_i$.\n",
    "\n",
    "We can use the ORTHPOL package to construct an orthonormal polynomial basis for each of the $p_i$'s.\n",
    "Say, for $p_i$ we construct $\\phi_{i1},\\phi_{i2},\\dots$.\n",
    "To build an orthonormal basis in $\\mathcal{F}_d$ we take the *tensor product* of all these 1D polynomials.\n",
    "This procedure is quite cumbersome to describe in its full complexity, but the first few higher-dimensional polynomials that we construct are:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\phi_1(x) &=& \\phi_{11}(x_1)\\cdot\\dots\\cdot\\phi_{d1}(x_d) = 1,\\\\\n",
    "\\phi_2(x) &=& \\phi_{11}(x_1)\\cdot\\dots\\cdot\\phi_{d2}(x_d) = \\phi_{d2}(x_d),\\\\\n",
    "&\\dots&\\\\\n",
    "\\phi_{i_1,\\dots,i_d}(x) &=& \\phi_{1i_1}(x_1)\\cdot\\dots\\cdot\\phi_{di_d}(x_d).\n",
    "\\end{array}\n",
    "$$\n",
    "With this procedure, if you have $m$ polynomials for each dimension, then you will end up with $m^d$ high-dimensional polynomials.\n",
    "\n",
    "There is a lot of literature on what is the optimal way to stop trying to keep the number of polynomials low.\n",
    "In any case, this is a clear indication that the curse of dimensionality will creep in here as well. And it does creep in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Multidimensional Orthogonal Polynomials\n",
    "\n",
    "Let's construct orthogonal polynomials for a random vector:\n",
    "$$\n",
    "X = (X_1,X_2),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "X_1\\sim\\mathcal{N}(0,1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "X_2\\sim\\mathcal{E}(1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = st.norm()\n",
    "X2 = st.expon()\n",
    "\n",
    "# The maximum polynomial degree you want\n",
    "degree = 4\n",
    "# Construct the orthogonal polynomials - See documentation for more ways to do create\n",
    "# multi-dimensional polynomials\n",
    "Phi_set = orthpol.ProductBasis((X1, X2), degree=degree)\n",
    "\n",
    "# Let's visualize them\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = np.linspace(0., 3, 100)\n",
    "Xg1, Xg2 = np.meshgrid(x1, x2)\n",
    "Xg_flat = np.hstack([Xg1.flatten()[:, None], Xg2.flatten()[:, None]])\n",
    "phi_g_flat = Phi_set(Xg_flat)\n",
    "\n",
    "for i in range(Phi_set.num_output):\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.contourf(Xg1, Xg2, phi_g_flat[:, i].reshape(Xg1.shape))\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('$\\phi_{%d}(x) = \\phi_{1,%d}(x_1)\\phi_{2,%d}(x_2)$' % (\n",
    "            i, Phi_set.terms[i][0], Phi_set.terms[i][1]))\n",
    "    plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "+ Generate and plot the first few orthogonal polynomials for a random vector with a Normal and a Beta component.\n",
    "\n",
    "+ Generate and plot the first few orthogonal polynomials for a random vector with a Beta and a Log-normal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Connection to Uncertainty Propagation\n",
    "Let $X$ be a random vector with pdf $p(x)$ and assume that we have constructed orthonormal polynomials $\\phi_1 = 1,\\phi_2,\\phi_3,\\dots$.\n",
    "Let $f\\in\\mathcal{F}_d$ be your physical model and think of the model output as a random variable $Y=f(X)$.\n",
    "Assume that we have (somehow) found the coefficients $c_1,c_2,\\dots$ in the expansion:\n",
    "$$\n",
    "f = \\sum_{i=1}^\\infty c_i \\phi_i.\n",
    "$$\n",
    "\n",
    "Now notice that the mean of $Y$ is:\n",
    "$$\n",
    "\\mathbb{E}[Y] = \\mathbb{E}_p[f(X)] = \\mathbb{E}_p[f(X) \\cdot 1] = \\langle f, \\phi_1\\rangle = c_1.\n",
    "$$\n",
    "That is:\n",
    "$$\n",
    "\\mathbb{E}_p[f(X)] = \\langle f, \\phi_1\\rangle = c_1.\n",
    "$$\n",
    "And in words: \"The first coefficient gives the mean of the function.\"\n",
    "\n",
    "For the variance, we will use the common formula:\n",
    "$$\n",
    "\\mathbb{V}[Y] = \\mathbb{E}[Y^2] - \\left\\{\\mathbb{E}[Y]\\right\\}^2.\n",
    "$$\n",
    "We already have the second term.\n",
    "Let's do the first:\n",
    "$$\n",
    "\\mathbb{E}[Y^2] = \\mathbb{E}_p[f^2(X)] = \\parallel f\\parallel^2 = \\sum_{i=1}^\\infty c_i^2.\n",
    "$$\n",
    "Combining everything, we get:\n",
    "$$\n",
    "\\mathbb{V}_p[f(X)] = \\sum_{i=2}^\\infty c_i^2.\n",
    "$$\n",
    "In words: \"The square sum of all the coefficients except the first one gives the variance.\"\n",
    "\n",
    "Thinking like this, you can derive approximation formulas for pretty much any statistic of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrusive Uncertainty Propagation Methods: Dynamical Systems\n",
    "\n",
    "Let $X$ be a random vector with probability density $p(x)$.\n",
    "Let $\\phi_1,\\phi_2,\\dots$ be an orthonormal basis with respect to $p(x)$.\n",
    "Consider the stochastic dynamical system:\n",
    "Consider the $m$-dimensional dynamical system:\n",
    "$$\n",
    "\\frac{dy}{dt} = g(y;X),\n",
    "$$\n",
    "with initial conditions\n",
    "$$\n",
    "y(0) = y_0(X).\n",
    "$$\n",
    "\n",
    "Assume that the solution $y(t;x)$ is square integrable.\n",
    "Then, at a given timestep $t$, we take the solution and we expand it in the polynomial basis:\n",
    "$$\n",
    "y(t;x) = \\sum_{i=1}^\\infty c_i(t)\\phi_i(x).\n",
    "$$\n",
    "Note that the coefficients are functions of time.\n",
    "According to our discussion above, if you find these $c_i(t)$'s, the expected value of the dynamical system will be:\n",
    "$$\n",
    "\\mathbb{E}[y(t;X)] = c_1(t),\n",
    "$$\n",
    "and the variance will be:\n",
    "$$\n",
    "\\mathbb{V}[y(t;X)] = \\sum_{i=2}^\\infty c_i^2(t).\n",
    "$$\n",
    "\n",
    "### Derivation of the Dynamical System for the Polynomial Coefficients\n",
    "We will derive a dynamical system that the coefficients must satisfy.\n",
    "At the initial conditions we have:\n",
    "$$\n",
    "y(0;x) = y_0(x)\\Rightarrow \\sum_{i=1}^\\infty c_i(0)\\phi_i(x) = y_0(x),\n",
    "$$\n",
    "so we get that:\n",
    "$$\n",
    "c_i(0) = \\langle \\phi_i, y_0\\rangle.\n",
    "$$\n",
    "\n",
    "Now, take the derivative of $y(t;x)$ with respect to $t$:\n",
    "$$\n",
    "\\frac{dy}{dt} = \\sum_{i=1}^\\infty \\frac{dc_i}{dt}\\phi_i(x).\n",
    "$$\n",
    "This looks good, but notice that $\\frac{dy}{dt} = g(y;x)$ is also a function of $y(t; x)$.\n",
    "We must think of $g(y;x)$ as a function of $x$ with a fixed $y$.\n",
    "Then we get:\n",
    "$$\n",
    "\\frac{dc_i}{dt} = \\left\\langle \\phi_i, g\\left(\\sum_{j=1}^\\infty c_j\\phi_j, \\cdot\\right)\\right\\rangle.\n",
    "$$\n",
    "\n",
    "Thus, the dynamical systme that we need to solve to find the coefficents at any time is the following:\n",
    "$$\n",
    "\\frac{dc_i}{dt} = \\left\\langle \\phi_i, g\\left(\\sum_{j=1}^\\infty c_j\\phi_j, \\cdot\\right)\\right\\rangle,\n",
    "$$\n",
    "with initial conditions:\n",
    "$$\n",
    "c_i(0) = \\langle \\phi_i, y_0\\rangle,\n",
    "$$\n",
    "for $i=1,2,\\dots$ (in practice, we truncate at a given order).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Dynamical System with Uncertain Parameters\n",
    "Take the random vector:\n",
    "$$\n",
    "X = (X_1, X_2),\n",
    "$$\n",
    "and assume that the components are independent Gaussian:\n",
    "$$\n",
    "X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2).\n",
    "$$\n",
    "So, for the full random vector we have a mean:\n",
    "$$\n",
    "\\mu = (\\mu_1, \\mu_2),\n",
    "$$\n",
    "and a covariance matrix:\n",
    "$$\n",
    "\\Sigma = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2).\n",
    "$$\n",
    "\n",
    "Consider the ODE:\n",
    "  \\begin{align*}\n",
    "    &\\dot{y} = \\frac{d y(t)}{dt} =-X_1y(t) \\equiv g(y,X),\\\\\n",
    "    &\\qquad y(0) = X_2 \\equiv y_0(X).\n",
    "  \\end{align*}\n",
    "\n",
    "Let's see if we can carry out the inner produces that are required for setting up the dynamical system for the polynomial coefficients:\n",
    "$$\n",
    "c_i(0) = \\langle \\phi_i, y_0\\rangle = \\langle \\phi_i, x_2\\rangle.\n",
    "$$\n",
    "\n",
    "We should be able to do this numerically with some simple quadrature rule $\\{(w_q,x_q)\\}_{q=1}^{N_q}$:\n",
    "$$\n",
    "c_{i0} = \\langle \\phi_i, y_0\\rangle \\approx \\sum_{q=1}^{N_q}w_q \\phi_i(x_q)x_{q,2}.\n",
    "$$\n",
    "\n",
    "The other integrals that we need are:\n",
    "$$\n",
    "\\langle \\phi_i, g\\rangle = \\langle \\phi_i, -x_1 \\sum_{j=1}^\\infty c_j \\phi_j\\rangle = -\\sum_{j=1}^\\infty c_j \\langle \\phi_i, x_1\\phi_j\\rangle.\n",
    "$$\n",
    "We can approximate all the integrals inside the summation by:\n",
    "$$\n",
    "A_{ij} = \\langle \\phi_i, x_1\\phi_j\\rangle \\approx \\sum_{q=1}^{N_q}w_q\\phi_i(x_q)x_{q1}\\phi_j(x_q). \n",
    "$$\n",
    "\n",
    "With these definitions, the dynamical system that we need to solve is:\n",
    "$$\n",
    "\\frac{dc_i}{dt} = -\\sum_{j=1}^\\infty c_j A_{ij},\n",
    "$$\n",
    "with initial conditions:\n",
    "$$\n",
    "c_i(0) = c_{i0},\n",
    "$$\n",
    "for $i=1,2,\\dots$ (we will truncate).\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION WITH ORTHOGONAL POLYNOMIALS\n",
    "\n",
    "# Construct the random variables - It is not very stable to work with the original \n",
    "# random varibales (too little uncertainty).\n",
    "# So, we work with scaled versions.\n",
    "mu1 = 0.05; sigma1 = 0.01\n",
    "sX1 = st.norm()\n",
    "mu2 = 8; sigma2 = 0.01\n",
    "sX2 = st.norm()\n",
    "sX = (sX1, sX2)\n",
    "mu = np.array([mu1, mu2])\n",
    "Sigma = np.diag([sigma1 ** 2, sigma2 ** 2])\n",
    "\n",
    "# Construct the orthonormal polynomials\n",
    "degree = 1\n",
    "Phi_set = orthpol.ProductBasis((sX1, sX2), degree=degree, ncap=1000)\n",
    "Phi_set.polynomials[0].normalize()\n",
    "Phi_set.polynomials[1].normalize()\n",
    "# Get a quadrature rule - we will talk about the quadrature rules in Lecture 17.\n",
    "sXq = np.random.randn(1000000, 2)\n",
    "w = np.ones((sXq.shape[0],)) / sXq.shape[0]\n",
    "Xq = np.ndarray(sXq.shape)\n",
    "Xq[:, 0] = sXq[:, 0] * sigma1 + mu1\n",
    "Xq[:, 1] = sXq[:, 1] * sigma2 + mu2\n",
    "\n",
    "# Evaluate the integrals needed for defining the dynamical system\n",
    "# Evaluate the orthogonal polynomials on the quadrature points\n",
    "phi_q = Phi_set(sXq)\n",
    "c0 = np.einsum('q,q,qj->j', w, Xq[:, 1], phi_q)\n",
    "# Evaluate the integrals giving rise to the matrix A\n",
    "A = np.einsum('q,q,qi,qj->ij', w, Xq[:, 0], phi_q, phi_q)\n",
    "\n",
    "# Define the dynamical system\n",
    "pc_rhs = lambda c, t: -np.dot(A, c)\n",
    "\n",
    "# Solve the system\n",
    "t = np.linspace(0, 100, 500)\n",
    "c = scipy.integrate.odeint(pc_rhs, c0, t)\n",
    "\n",
    "# Extract the mean\n",
    "y_pc_m = c[:, 0]\n",
    "\n",
    "# Extract the variance\n",
    "y_pc_v = np.sum(c[:, 1:] ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION WITH SAMPLING (FOR COMPARISON)\n",
    "import scipy.integrate\n",
    "\n",
    "class Ex1Solver(object):\n",
    "    \"\"\"\n",
    "    An object that can solver the afforementioned ODE problem.\n",
    "    It will work just like a multivariate function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nt=100, T=5):\n",
    "        \"\"\"\n",
    "        This is the initializer of the class.\n",
    "        \n",
    "        Arguments:\n",
    "            nt - The number of timesteps.\n",
    "            T  - The final time.\n",
    "        \"\"\"\n",
    "        self.nt = nt\n",
    "        self.T = T\n",
    "        self.t = np.linspace(0, T, nt) # The timesteps on which we will get the solution\n",
    "        # The following are not essential, but they are convenient\n",
    "        self.num_input = 2             # The number of inputs the class accepts\n",
    "        self.num_output = nt           # The number of outputs the class returns\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        This special class method emulates a function call.\n",
    "        \n",
    "        Arguments:\n",
    "            x - A 1D numpy array with 2 elements. This represents the stochastic input x = (x1, x2).\n",
    "        \"\"\"\n",
    "        # The dynamics of the adjoint z = y, dy/dx1, dy/dx2\n",
    "        def g(z, t, x):\n",
    "            return -x[0] * z[0], -x[0] * z[1] - z[0], -x[0] * z[2]\n",
    "        # The initial condition\n",
    "        y0 = (x[1], 0, 1)\n",
    "        # We are ready to solve the ODE\n",
    "        y = scipy.integrate.odeint(g, y0, self.t, args=(x,))\n",
    "        return y\n",
    "    \n",
    "import design\n",
    "num_lhs = 10000\n",
    "X_lhs = design.latin_center(num_lhs, 2) # These are uniformly distributed - Turn them to standard normal\n",
    "X_samples = mu + np.dot(st.norm.ppf(X_lhs), np.sqrt(Sigma))\n",
    "solver = Ex1Solver(nt=500, T=100)\n",
    "s = 0.\n",
    "s2 = 0.\n",
    "for x in X_samples:\n",
    "    y = solver(x)[:, 0]\n",
    "    s += y\n",
    "    s2 += y ** 2\n",
    "y_mu_lhs = s / num_lhs\n",
    "y_var_lhs = s2 / num_lhs - y_mu_lhs ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the figure\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the mean and compare to LHS\n",
    "ax1.plot(solver.t, y_mu_lhs, color=sns.color_palette()[0], label='LHS mean ($n=%d$)' % num_lhs)\n",
    "ax1.plot(t, y_pc_m, '--', color=sns.color_palette()[1], label=r'PC mean ($\\rho=%d$)' % degree)\n",
    "ax1.set_xlabel('$t$')\n",
    "ax1.set_ylabel('$\\mu(t)$', color=sns.color_palette()[0])\n",
    "ax1.tick_params('y', colors=sns.color_palette()[0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot variance and compare to LHS\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(solver.t, y_var_lhs, color=sns.color_palette()[2], label='LHS variance ($n=%d$)' % (num_lhs))\n",
    "ax2.plot(solver.t, y_pc_v, '--', color=sns.color_palette()[3], label=r'PC variance ($\\rho=%d$)' % degree)\n",
    "ax2.set_ylabel('$\\sigma^2(t) = k(t, t)$', color=sns.color_palette()[2])\n",
    "ax2.tick_params('y', colors=sns.color_palette()[2])\n",
    "plt.legend(loc='center right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do 95% intervals\n",
    "s = np.sqrt(y_pc_v)\n",
    "l = y_pc_m - 2 * s\n",
    "u = y_pc_m + 2 * s\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, y_pc_m)\n",
    "ax.fill_between(t, l, u, alpha=0.25)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's take some sample paths\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "for _ in range(5):\n",
    "    s_x_s = np.random.randn(2)\n",
    "    y_s = np.dot(c, Phi_set(s_x_s[None, :]).T)\n",
    "    ax.plot(t, y_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "+ Repeat the analysis with higher polynomial degrees. What $\\rho$ do you need to get convergent results?\n",
    "\n",
    "+ Modify the code above so that you solve the problem with $X_1$ and $X_2$ that are Log-Normally distributed (choose your own mean and variance)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
