{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20 - Inverse Problems/Model Calibration: Bayesian Approache\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Demonstrate that classical inverse problems are ill-posed.\n",
    "+ Formulate inverse problems as Bayesian inference problems.\n",
    "+ Remember the Laplace approximation.\n",
    "+ Demonstrate approach by applying it to the calibration of a reaction kinetics problem.\n",
    "+ Highlight the shortcomings of the Laplace approximation.\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ These notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "import design\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import orthpol "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Demonstrate that the classical approach does not identify noise\n",
    "\n",
    "The classical approach does not provide a way to estimate the measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data\n",
    "import pandas as pd\n",
    "catalysis_data = pd.read_csv('catalysis.csv', index_col=0)\n",
    "\n",
    "# The experimental data as a matrix\n",
    "Y = catalysis_data[1:].get_values()\n",
    "\n",
    "# The experimental as a vector\n",
    "y = Y.flatten()\n",
    "\n",
    "# The model\n",
    "from demos.catalysis import CatalysisModel\n",
    "solver = CatalysisModel()  \n",
    "\n",
    "\n",
    "# The loss function\n",
    "def L_with_jac(x, y):\n",
    "    k = np.exp(x) / 180.\n",
    "    sol = solver(k)\n",
    "    y = y / 500.\n",
    "    f = sol['f'][0][5:] / 500\n",
    "    dfdk = sol['f_grad'][0][5:, :] / 500.\n",
    "    dfdx = np.einsum('ij,j->ij', dfdk, k)\n",
    "    tmp = (f - y)\n",
    "    dLdx = np.einsum('ij,i->j', dfdx, tmp)\n",
    "    L = 0.5 * np.sum(tmp ** 2)\n",
    "    return L, dLdx\n",
    "\n",
    "# For making predictions\n",
    "import scipy.integrate\n",
    "\n",
    "def A(x):\n",
    "    \"\"\"\n",
    "    Return the matrix of the dynamical system.\n",
    "    \"\"\"\n",
    "    # Scale back to the k's\n",
    "    k = np.exp(x) / 180.\n",
    "    res = np.zeros((6,6))\n",
    "    res[0, 0] = -k[0]\n",
    "    res[1, 0] = k[0]\n",
    "    res[1, 1] = -(k[1] + k[3] + k[4])\n",
    "    res[2, 1] = k[1]\n",
    "    res[2, 2] = -k[2]\n",
    "    res[3, 2] = k[2]\n",
    "    res[4, 1] = k[4]\n",
    "    res[5, 1] = k[3]\n",
    "    return res\n",
    "    \n",
    "\n",
    "def g(z, t, x):\n",
    "    \"\"\"\n",
    "    The right hand side of the dynamical system.\n",
    "    \"\"\"\n",
    "    return np.dot(A(x), z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The full solution of the dynamical system\n",
    "def Z(x, t):\n",
    "    \"\"\"\n",
    "    Returns the solution for parameters x at times t.\n",
    "    \"\"\"\n",
    "    # The initial conditions\n",
    "    z0 = np.array([500., 0., 0., 0., 0., 0.])\n",
    "    return scipy.integrate.odeint(g, z0, t, args=(x,))\n",
    "\n",
    "\n",
    "# Initial guess for x\n",
    "x0 = np.random.randn(5)\n",
    "\n",
    "# Minimize the loss function\n",
    "res = scipy.optimize.minimize(L_with_jac, x0, jac=True, args=(y))\n",
    "\n",
    "print res\n",
    "\n",
    "# Make predictions\n",
    "t = np.linspace(0, 180, 200)\n",
    "Yp = Z(res.x, t)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "catalysis_data.plot(ax=ax, style='s')\n",
    "ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], label='Model NO3-')\n",
    "ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], label='Model NO2-')\n",
    "ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], label='Model X')\n",
    "ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], label='Model N2')\n",
    "ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], label='Model NH3')\n",
    "ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], label='Model N2O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good, but there is no estimate of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Issues of the Classical Formulation\n",
    "\n",
    "+ The solution may not exist. (In the catalysis example, try calibrating a model that does not include the intermediate element $X$. It is not possible because the total mass will not be conserved.)\n",
    "\n",
    "+ Multiple solutions may exist. (In the catalysis example, try adding one more fictitious element product Y. You will probably fit the data very well. But which model is the right one?)\n",
    "\n",
    "+ No estimate of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Formulation of Inverse Problems\n",
    "\n",
    "Let us reintroduce everything we had before:\n",
    "\n",
    "+ $x\\in\\mathcal{X}\\in\\mathbb{R}^d$ are the unknown parameters.\n",
    "+ $y\\in\\mathcal{Y}\\in\\mathbb{R}^m$ are the experimental measurements.\n",
    "+ $f:\\mathcal{X}\\rightarrow \\mathcal{Y}$ is your model.\n",
    "\n",
    "For the Bayesian approach to inverse problems we need two ingredients:\n",
    "\n",
    "+ A *prior* probability density for $x$, $p(x)$.\n",
    "\n",
    "+ A *likelihood model* $p(y|x, f)$.\n",
    "Typically the form of the likelihood model is:\n",
    "$$\n",
    "p(y|x, f) = p(y|f(x)),\n",
    "$$\n",
    "and the most common choice is, of course, the Gaussian:\n",
    "$$\n",
    "p(y|x,f,\\sigma^2) = \\mathcal{N}\\left(y|f(x), \\sigma^2I\\right).\n",
    "$$\n",
    "Of course, we would like to estimate $\\sigma^2$ also.\n",
    "For this reason, we need to introduce a prior probability density $p(\\sigma)$ as well.\n",
    "\n",
    "**Note:** Remember that the likelihood model is your choice and it should reflect what you know about the noise process.\n",
    "\n",
    "Let's assume that we have the Gaussian likelihood.\n",
    "All we need to do now is apply the Bayes rule:\n",
    "$$\n",
    "p(x, \\sigma^2|y, f) \\propto p(y|x,f,\\sigma^2)p(x)p(\\sigma).\n",
    "$$\n",
    "That's it!\n",
    "This is our *posterior state of knowledge* about all the parameters.\n",
    "Now the only problem is to characterize this probability density.\n",
    "\n",
    "### Remarks\n",
    "\n",
    "+ The Bayesian solution to inverse problems is no longer a point estimate. It is a distribution.\n",
    "\n",
    "+ The solution always exists.\n",
    "\n",
    "+ The solution is unique.\n",
    "\n",
    "+ The probability mass in the posterior automatically quantifies uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Approximation\n",
    "If you have a linear model and a Gaussian likelihood and the prior $p(x)$ is Gaussian, then this is just a Gaussian.\n",
    "In general, you can only chacterize it through sampling or some approximation.\n",
    "We will discuss sampling from the posterior in Lecture 21 when we will introduce Markov Chain Monte Carlo.\n",
    "For now, let us discuss the Laplace approximation to the posterior.\n",
    "\n",
    "The Laplace approximation finds a Gaussian approximation to the posterior.\n",
    "For simplicity we will work with a Gaussian likelihood, but these ideas can be applied to any likelihood.\n",
    "To avoid over complicating things on this first attempt, let us assume that we know the value of $\\sigma$.\n",
    "We will generalize our approach later.\n",
    "Our posterior is (for just $x$) is:\n",
    "$$\n",
    "p(x|y,\\sigma) \\propto p(y|x,\\sigma)p(x) = \\mathcal{N}\\left(y|f(x),\\sigma^2\\right)p(x) \\propto \\exp\\left\\{-\\frac{\\parallel y - f(x) \\parallel_2^2}{2\\sigma^2}\\right\\}p(x).\n",
    "$$\n",
    "To implement the Laplace approximation define:\n",
    "$$\n",
    "L(x) = -\\frac{\\parallel y - f(x) \\parallel_2^2}{2\\sigma^2} + \\log p(x).\n",
    "$$\n",
    "We need to find the maximum of this:\n",
    "$$\n",
    "\\mu = \\arg\\max_{x\\in\\mathcal{X}} L(x),\n",
    "$$\n",
    "the matrix\n",
    "$$\n",
    "\\Sigma = -\\left(\\nabla^2 L(\\mu)\\right)^{-1},\n",
    "$$\n",
    "and the approximation is:\n",
    "$$\n",
    "p(x|y,\\sigma) \\approx \\mathcal{N}(x|\\mu,\\Sigma).\n",
    "$$\n",
    "\n",
    "The second derivative is:\n",
    "$$\n",
    "\\frac{\\partial ^2 L(\\mu)}{\\partial x_j \\partial x_r} = \\frac{1}{\\sigma^2}\\sum_{i=1}^m\\frac{\\partial f_i(\\mu)}{\\partial x_j}\\frac{\\partial f_i(\\mu)}{\\partial x_r}-\\frac{1}{\\sigma^2}\\sum_{i=1}^m(y_i-f_i(\\mu))\\frac{\\partial^2 f_i(\\mu)}{\\partial x_j\\partial x_r} + \\frac{\\partial^2\\log p(\\mu)}{\\partial x_r\\partial x_j}.\n",
    "$$\n",
    "So, you need the second derivatives of your model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The Laplace Approximation for the Catalysis Problem\n",
    "\n",
    "Let us assume that $\\sigma = 5$ and take\n",
    "$$\n",
    "p(x) = \\mathcal{N}(0,\\gamma^2I).\n",
    "$$\n",
    "We need this derivative:\n",
    "$$\n",
    "\\nabla^2 \\log p(x) = -\\gamma^{-2}I.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function that we need to maximize\n",
    "def minus_log_post(x, y, sigma, gamma):\n",
    "    k = np.exp(x) / 180.\n",
    "    sigma = sigma\n",
    "    sol = solver(k)\n",
    "    f = sol['f'][0][5:]\n",
    "    dfdk = sol['f_grad'][0][5:, :]\n",
    "    dfdx = np.einsum('ij,j->ij', dfdk, k)\n",
    "    tmp = (y - f)\n",
    "    dLdx = -np.einsum('ij,i->j', dfdx, tmp) / sigma ** 2 + x / gamma ** 2\n",
    "    L = 0.5 * np.sum(tmp ** 2) / sigma ** 2 + 0.5 * np.sum(x ** 2) / gamma ** 2\n",
    "    return L, dLdx\n",
    "\n",
    "\n",
    "# The inverse of the second derivative\n",
    "def compute_post_cov(mu, y, sigma, gamma):\n",
    "    k = np.exp(mu) / 180.\n",
    "    sol = solver(k)\n",
    "    f = sol['f'][0][5:]\n",
    "    dfdk = sol['f_grad'][0][5:, :]\n",
    "    d2fdk2 = sol['f_grad_2'][0][5:, :, :]\n",
    "    # Chain rule\n",
    "    dfdx = np.einsum('ij,j->ij', dfdk, k)\n",
    "    # Chain rule\n",
    "    d2fdx2 = np.einsum('ijr,j,r->ijr', d2fdk2, k, k) + \\\n",
    "        np.einsum('ij,j,jr->ijr', dfdk, k, np.eye(k.shape[0]))\n",
    "    tmp = (y - f)\n",
    "    d2Ldx2 = - np.einsum('i,ijr->jr', tmp, d2fdx2) / sigma ** 2 \\\n",
    "             + np.einsum('ij,ir->jr', dfdx, dfdx) / sigma ** 2  \\\n",
    "             + np.eye(mu.shape[0]) / gamma ** 2\n",
    "    # We need to invert this to see our Sigma:\n",
    "    Sigma = np.linalg.inv(d2Ldx2) # We should not ever invert - but we make an exception here\n",
    "    return Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigma choice\n",
    "sigma = 10. # Variance of noise\n",
    "\n",
    "# gamma choice\n",
    "gamma = 10.\n",
    "\n",
    "# Initial guess for x\n",
    "x0 = np.random.randn(5)\n",
    "\n",
    "# Minimize the loss function\n",
    "res_la = scipy.optimize.minimize(minus_log_post, x0, jac=True, args=(y, sigma, gamma))\n",
    "\n",
    "print res_la\n",
    "\n",
    "# The mean of the x's:\n",
    "mu = res_la.x\n",
    "\n",
    "# The covariance matrix of the x's:\n",
    "Sigma = compute_post_cov(mu, y, sigma, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize the uncertainty about the response by taking some samples\n",
    "num_samples = 50\n",
    "t = np.linspace(0, 180, 200)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "catalysis_data.plot(ax=ax, style='s')\n",
    "Yp = Z(mu, t)\n",
    "ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], label='Model NO3-')\n",
    "ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], label='Model NO2-')\n",
    "ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], label='Model X')\n",
    "ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], label='Model N2')\n",
    "ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], label='Model NH3')\n",
    "ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], label='Model N2O')\n",
    "for i in range(num_samples):\n",
    "    # Take a sample from the posterior\n",
    "    x = np.random.multivariate_normal(mu, Sigma)\n",
    "    Yp = Z(x, t)\n",
    "    ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], lw=1, label='Model NO3-')\n",
    "    ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], lw=1, label='Model NO2-')\n",
    "    ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], lw=1, label='Model X')\n",
    "    ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], lw=1, label='Model N2')\n",
    "    ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], lw=1, label='Model NH3')\n",
    "    ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], lw=1, label='Model N2O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Investigate what happens as you go from a very large $\\sigma$ (say $20$) to a very small one (say $1$.) Is there a sweet spot?\n",
    "\n",
    "+ Investigate what happens as you change $\\gamma$ in the same way.\n",
    "\n",
    "+ How elese can you propagate uncertainty through the solver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Noise Level with the Laplace Approximation\n",
    "\n",
    "We can estiamte the noise level with the Laplace approximation.\n",
    "We need a prior on $\\sigma$.\n",
    "Let's pick:\n",
    "$$\n",
    "p(\\sigma) \\propto \\frac{1}{\\sigma}.\n",
    "$$\n",
    "This is known as the [Jeffrey's prior](https://en.wikipedia.org/wiki/Jeffreys_prior).\n",
    "\n",
    "Also, because of the nature of the parameterization, it probably makes sense to work with $\\log \\sigma$ instead of $\\sigma$.\n",
    "So, let's introduce the following unknown vector to be inferred from the data:\n",
    "$$\n",
    "z = (x, \\theta),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = \\log\\sigma.\n",
    "$$\n",
    "\n",
    "### Be careful when you change variables\n",
    "We need to be a little bit careful with $p(\\theta)$.\n",
    "We need to use the [change of variables formula](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables).\n",
    "Define:\n",
    "$$\n",
    "\\theta := g(\\sigma) = \\log \\sigma.\n",
    "$$\n",
    "The inverse is:\n",
    "$$\n",
    "g^{-1}(\\theta) = e^{\\theta}.\n",
    "$$\n",
    "The formula is:\n",
    "$$\n",
    "p(\\theta) = \\left|\\frac{d}{d\\theta}\\left(g^{-1}(\\theta)\\right)\\right|\\cdot p(\\sigma=e^{\\theta}) \\propto e^\\theta e^{-\\theta} = 1.\n",
    "$$\n",
    "So\n",
    "$$\n",
    "p(\\theta) \\propto 1.\n",
    "$$\n",
    "\n",
    "Now, we just derive the posterior of $z$:\n",
    "$$\n",
    "p(z|y) \\propto p(y|z)p(z) \\propto e^{-m\\theta}\\exp\\left\\{-\\frac{\\parallel y - f(x) \\parallel_2^2}{2e^{2\\theta}}\\right\\}p(x),\n",
    "$$\n",
    "and we apply the Laplace approximation using:\n",
    "$$\n",
    "L(z) = -m\\theta - \\frac{\\parallel y - f(x) \\parallel_2^2}{2}e^{-2\\theta} + \\log p(x).\n",
    "$$\n",
    "The first and second mixed derivative with respect to $x$ are just like before.\n",
    "We need the derivatives with respect to $\\theta$:\n",
    "$$\n",
    "\\frac{\\partial L(z)}{\\partial \\theta} = -m + \\parallel y - f(x) \\parallel_2^2e^{-2\\theta},\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial^2 L(z)}{\\partial \\theta^2} = -2\\parallel y - f(x) \\parallel_2^2e^{-2\\theta},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial^2 L(z)}{\\partial x_j\\partial\\theta} = 2e^{-2\\theta}\\sum_{i=1}^m(y_i-f_i(x))\\frac{\\partial f_i(x)}{\\partial x_j}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function that we need to maximize\n",
    "def minus_log_post_w_noise(z, y, gamma):\n",
    "    m = y.shape[0]\n",
    "    x = z[:5]\n",
    "    theta = z[5]\n",
    "    sigma = np.exp(theta)\n",
    "    k = np.exp(x) / 180.\n",
    "    sol = solver(k)\n",
    "    f = sol['f'][0][5:]\n",
    "    dfdk = sol['f_grad'][0][5:, :]\n",
    "    dfdx = np.einsum('ij,j->ij', dfdk, k)\n",
    "    tmp = (y - f)\n",
    "    dLdx = -np.einsum('ij,i->j', dfdx, tmp) / sigma ** 2 + x / gamma ** 2\n",
    "    dLdtheta = m - np.sum(tmp ** 2) / sigma ** 2\n",
    "    L = m * theta + 0.5 * np.sum(tmp ** 2) / sigma ** 2 + 0.5 * np.sum(x ** 2) / gamma ** 2\n",
    "    dLdz = np.hstack([dLdx, [dLdtheta]])\n",
    "    return L, dLdz\n",
    "\n",
    "\n",
    "# The inverse of the second derivative\n",
    "def compute_post_cov_w_noise(mu, y, gamma):\n",
    "    m = y.shape[0]\n",
    "    mu_x = mu[:5]\n",
    "    sigma = np.exp(mu[5])\n",
    "    k = np.exp(mu_x) / 180.\n",
    "    sol = solver(k)\n",
    "    f = sol['f'][0][5:]\n",
    "    dfdk = sol['f_grad'][0][5:, :]\n",
    "    d2fdk2 = sol['f_grad_2'][0][5:, :, :]\n",
    "    # Chain rule\n",
    "    dfdx = np.einsum('ij,j->ij', dfdk, k)\n",
    "    # Chain rule\n",
    "    d2fdx2 = np.einsum('ijr,j,r->ijr', d2fdk2, k, k) + \\\n",
    "        np.einsum('ij,j,jr->ijr', dfdk, k, np.eye(k.shape[0]))\n",
    "    tmp = (y - f)\n",
    "    d2Ldx2 = - np.einsum('i,ijr->jr', tmp, d2fdx2) / sigma ** 2 \\\n",
    "             + np.einsum('ij,ir->jr', dfdx, dfdx) / sigma ** 2  \\\n",
    "             + np.eye(mu_x.shape[0]) / gamma ** 2\n",
    "    d2Ldxdtheta = 2 * np.einsum('i,ij->j', tmp, dfdx) / sigma ** 2\n",
    "    d2Ldtheta2 = 2 * np.sum(tmp ** 2) / sigma ** 2\n",
    "    Lam = np.bmat([[d2Ldx2, d2Ldxdtheta[:, None]], [d2Ldxdtheta[None, :], [[d2Ldtheta2]]]])\n",
    "    # We need to invert this to see our Sigma:\n",
    "    Sigma = np.linalg.inv(Lam) # We should not ever invert - but we make an exception here\n",
    "    return Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gamma choice\n",
    "gamma = 10.\n",
    "\n",
    "# Initial guess for x\n",
    "z0 = np.random.randn(6)\n",
    "z0[-1] = 10.\n",
    "\n",
    "# Minimize the loss function\n",
    "res = scipy.optimize.minimize(minus_log_post_w_noise, z0, jac=True, args=(y, gamma))\n",
    "\n",
    "# The mean of the x's:\n",
    "mu = res.x\n",
    "\n",
    "# The covariance matrix of the x's:\n",
    "Sigma = compute_post_cov_w_noise(mu, y, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize the uncertainty about the response by taking some samples\n",
    "num_samples = 50\n",
    "t = np.linspace(0, 180, 200)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "catalysis_data.plot(ax=ax, style='s')\n",
    "Yp = Z(mu[:5], t)\n",
    "ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], label='Model NO3-')\n",
    "ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], label='Model NO2-')\n",
    "ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], label='Model X')\n",
    "ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], label='Model N2')\n",
    "ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], label='Model NH3')\n",
    "ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], label='Model N2O')\n",
    "for i in range(num_samples):\n",
    "    # Take a sample from the posterior\n",
    "    x = np.random.multivariate_normal(mu, Sigma)\n",
    "    Yp = Z(x[:5], t)\n",
    "    ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], lw=1, label='Model NO3-')\n",
    "    ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], lw=1, label='Model NO2-')\n",
    "    ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], lw=1, label='Model X')\n",
    "    ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], lw=1, label='Model N2')\n",
    "    ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], lw=1, label='Model NH3')\n",
    "    ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], lw=1, label='Model N2O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The sigma we found is:\n",
    "sigma = np.exp(mu[5])\n",
    "print sigma, '+-', 2. * np.sqrt(Sigma[-1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Investigate what happens as you change $\\gamma$ from smaller to bigger values.\n",
    "\n",
    "+ Is the uncertainty we visualized above epistemic or aleatory?\n",
    "\n",
    "+ The uncertainty visualized concerns only the model. What if you wanted to include the measurement noise in this visualization?\n",
    "\n",
    "+ The assumption of Gaussian noise is not very good. Generate two different assumptions that you could try for the noise."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
