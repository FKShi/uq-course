{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15 - Perturbative Methods\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ to remember the Taylor expansion of multivaritae real functions\n",
    "+ to use the Laplace approximation to approximate arbitrary probability densities as Gaussians\n",
    "+ to use the Taylor expansion and the Laplace approximation to (approximately) propagate uncertainties through models\n",
    "+ to use the method of adjoints to calculate derivatives of dynamical systems or partial differential equations with respect to uncertain parameters\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taylor Expansion in 1D\n",
    "Let $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a smooth real function and $x_0\\in\\mathbb{R}$.\n",
    "The [Taylor series expansion](https://en.wikipedia.org/wiki/Taylor_series) of $g(x)$ abount $x_0$ is:\n",
    "$$\n",
    "g(x) = g(x_0) + \\frac{dg(x_0)}{dx}(x-x_0) + \\frac{1}{2}\\frac{d^2g(x_0)}{dx^2}(x-x_0)^2+\\dots + \\frac{1}{n!}\\frac{d^ng(x_0)}{dx^n}(x-x_0)^n+\\dots\n",
    "$$\n",
    "You can use the Taylor expansion to approximate any function as a polynomial.\n",
    "\n",
    "### Example: 1D Taylor Series Expansion\n",
    "Take $g(x) = \\sin(x)$ and $x_0=0$.\n",
    "We have:\n",
    "$$\n",
    "\\frac{dg(0)}{dx} = \\cos(x)|_{x=0} = 1,\n",
    "$$\n",
    "$$\n",
    "\\frac{d^2g(0)}{dx^2} = -\\sin(x)_{x=0} = 0,\n",
    "$$\n",
    "$$\n",
    "\\frac{d^3g(0)}{dx^3} = -\\cos(x)_{x=0} = -1,\n",
    "$$\n",
    "and in general:\n",
    "$$\n",
    "\\sin(x) = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!}x^{2k+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_taylor_order = 3\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-np.pi, np.pi, 100)\n",
    "y_true = np.sin(x)\n",
    "y_taylor = np.zeros(x.shape)\n",
    "for k in range(max_taylor_order):\n",
    "    y_taylor += (-1.) ** k * 1. / math.factorial(2 * k + 1) * x ** (2 * k + 1)\n",
    "ax.plot(x, y_true, label='$\\sin(x)$')\n",
    "ax.plot(x, y_taylor, '--', label='Truncated Taylor series')\n",
    "plt.legend(loc='best')\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_title('Maximum Taylor order: %d' % max_taylor_order)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$g(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Start increasing the maximum order of the series expansion until you get a satisfactory approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taylor Expansion in Higher Dimensions\n",
    "Let $g:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ and $x_0\\in\\mathbb{R}^d$.\n",
    "The Taylor series expansion is:\n",
    "$$\n",
    "g(x) = g(x_0) + \\sum_{i=1}^d\\frac{\\partial g(x_0)}{\\partial x_i}(x_i-x_{0i})+\\frac{1}{2}\\sum_{i,j=1}^d\\frac{\\partial^2g(x_0)}{\\partial x_i\\partial x_j}(x_i-x_{0i})(x-x_{0j}) + \\dots\n",
    "$$\n",
    "Another way of writing this is:\n",
    "$$\n",
    "g(x) = g(x_0) + \\nabla g(x_0)^T(x-x_0) + \\frac{1}{2}(x-x_0)^T\\nabla^2 g(x_0) (x-x_0),\n",
    "$$\n",
    "where the *Jacobian* is defined as:\n",
    "$$\n",
    "\\nabla g(x_0) = \\left(\\frac{\\partial g(x_0)}{\\partial x_1},\\dots,\\frac{\\partial g(x_0)}{\\partial x_1}\\right),\n",
    "$$\n",
    "and the *Hessian* is:\n",
    "$$\n",
    "\\nabla^2 g(x_0) = \\left(\\frac{\\partial^2g(x_0)}{\\partial x_i\\partial x_j}\\right)_{i,j=1}^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Approximation in 1D\n",
    "If you are not interested in probabilities of rare events, you may approximate any probability density as a Gaussian (assuming that it is sufficiently narrow).\n",
    "Let $X$ be a random variable with probability density $p(x)$.\n",
    "Because $p(x)$ is positive, it is better to work with its logarithm.\n",
    "First, we find the maximum of $\\log p(x)$ which is called the *nominal value* (or just maximum):\n",
    "$$\n",
    "x_0 = \\arg\\max_x \\log p(x).\n",
    "$$\n",
    "Then, we take the Taylor expansion of $\\log p(x)$ about $x=x_0$:\n",
    "$$\n",
    "\\log p(x) = \\log p(x_0) + \\frac{d\\log p(x_0)}{dx} (x-x_0) + \\frac{1}{2}\\frac{d^2\\log p(x_0)}{dx^2} (x-x_0)^2 + \\dots.\n",
    "$$\n",
    "Since $x_0$ is a critical point of $\\log p(x)$, we must have that:\n",
    "$$\n",
    "\\frac{d\\log p(x_0)}{dx} = 0.\n",
    "$$\n",
    "So, the expansion becomes:\n",
    "$$\n",
    "\\log p(x) = \\frac{1}{2}\\frac{d^2\\log p(x_0)}{dx^2} (x-x_0)^2 + \\text{const}.\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "p(x) \\propto \\exp\\left\\{ \\frac{1}{2}\\frac{d^2\\log p(x_0)}{dx^2} (x-x_0)^2 \\right\\}.\n",
    "$$\n",
    "Since $x_0$ is a maximum of $\\log p(x)$, the matrix $\\frac{d^2\\log p(x_0)}{dx^2}$ must be a negative number.\n",
    "Therefore, the number:\n",
    "$$\n",
    "c^2 = -\\left[\\frac{d^2\\log p(x_0)}{dx^2}\\right]^{-1},\n",
    "$$\n",
    "is positive.\n",
    "By inspection then, we see that:\n",
    "$$\n",
    "p(x) \\propto \\exp\\left\\{-\\frac{(x-x_0)^2}{2c^2}\\right\\}.\n",
    "$$\n",
    "Ignoring all higher order terms, we conclude:\n",
    "$$\n",
    "p(x) \\approx \\mathcal{N}\\left(x|x_0, -\\left[\\frac{d^2\\log p(x_0)}{dx^2}\\right]^{-1}\\right).\n",
    "$$\n",
    "This is the Laplace approximation in one dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Let's just approximate a Gamma distribution with a Gaussian.\n",
    "We will do a case that works and a case that does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the ``true random variable``\n",
    "alpha = 10\n",
    "X = st.gamma(a=alpha)\n",
    "# Find the maximum using an optimization method (the minus is because we will use a minimization method)\n",
    "minus_log_pdf_true = lambda x: -np.log(X.pdf(x))\n",
    "res = scipy.optimize.minimize_scalar(minus_log_pdf_true, bounds=[1., 8])\n",
    "# This is the maximum of the pdf\n",
    "x_0 = res.x\n",
    "# This is the value of the pdf at the maximum:\n",
    "p_0 = np.exp(-res.fun)\n",
    "# The derivative of the pdf at the maximum should be exaclty zero.\n",
    "# Let's verify it using numerical integration\n",
    "grad_m_log_p_0 = scipy.misc.derivative(minus_log_pdf_true, x_0, dx=1e-6, n=1)\n",
    "print 'grad minus log p at x0 is: ', grad_m_log_p_0\n",
    "# We need the second derivative.\n",
    "# We will get it using numerical integration as well:\n",
    "grad_2_m_log_p_0 = scipy.misc.derivative(minus_log_pdf_true, x_0, dx=1e-6, n=2)\n",
    "# The standard deviation of the Gaussian approximation to p(x) is:\n",
    "c = np.sqrt(1. / grad_2_m_log_p_0)\n",
    "# Using scipy code, the random variable that approximates X is:\n",
    "Z = st.norm(loc=x_0, scale=c)\n",
    "# Let's plot everything\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(x_0 - 6 * c, x_0 + 6 * c, 200)\n",
    "pdf_true = X.pdf(x)\n",
    "# Plot the truth:\n",
    "ax.plot(x, pdf_true)\n",
    "# Mark the location of the maximum:\n",
    "# Plot the approximation\n",
    "ax.plot(x, Z.pdf(x), '--')\n",
    "ax.plot([x_0] * 10, np.linspace(0, p_0, 10), ':')\n",
    "plt.legend(['PDF of Gamma(%.2f)' % alpha, 'Laplace approximation'], loc='best')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('PDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace Approximation in Many Dimensions\n",
    "Then, we take the Taylor expansion of $\\log p(x)$ about $x=x_0$:\n",
    "$$\n",
    "\\log p(x) = \\log p(x_0) + \\nabla \\log p(x_0) (x-x_0) + \\frac{1}{2}(x-x_0)^T\\nabla^2 \\log p(x_0) (x-x_0) + \\dots.\n",
    "$$\n",
    "Since $x_0$ is a critical point of $\\log p(x)$, we must have that:\n",
    "$$\n",
    "\\nabla \\log p(x_0) = 0.\n",
    "$$\n",
    "So, the expansion becomes:\n",
    "$$\n",
    "\\log p(x) = \\frac{1}{2}(x-x_0)^T\\nabla^2 \\log p(x_0) (x-x_0) + \\text{const}.\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "p(x) \\propto \\exp\\left\\{ \\frac{1}{2}(x-x_0)^T\\nabla^2 \\log p(x_0) (x-x_0) \\right\\}.\n",
    "$$\n",
    "Since $x_0$ is a maximum of $\\log p(x)$, the matrix $\\nabla^2 \\log p(x_0)$ must be negative definite.\n",
    "Therefore, the matrix:\n",
    "$$\n",
    "C = -(\\nabla^2\\log p(x_0))^{-1},\n",
    "$$\n",
    "is positive definite.\n",
    "By inspection then, we see that:\n",
    "$$\n",
    "p(x) \\propto \\exp\\left\\{-\\frac{1}{2}(x-x_0)^TC^{-1}(x-x_0)\\right\\}.\n",
    "$$\n",
    "Ignoring all higher order terms, we conclude:\n",
    "$$\n",
    "p(x) \\approx \\mathcal{N}\\left(x|x_0, -(\\nabla^2\\log p(x_0))^{-1}\\right).\n",
    "$$\n",
    "This is the Laplace approximation in many dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with various values of alpha for the true Gamma distribution. What happens when alpha is smaller than one. Why doesn't the method work at for that parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Approach in 1D\n",
    "\n",
    "Let $X\\sim p$ be a random vector and $Y=f(X)$ a scalar quantity of interest that depends on $X$.\n",
    "Our goal is to estimate the first order statistics of $Y$.\n",
    "In particular, the mean:\n",
    "$$\n",
    "\\mathbb{E}[f(X)] = \\int f(x) p(x)dx,\n",
    "$$\n",
    "and the variance:\n",
    "$$\n",
    "\\mathbb{V}[f(X)] = \\int \\left(f(x)-\\mathbb{E}[f(X)]\\right)^2p(x)dx.\n",
    "$$\n",
    "First, we take the laplace approximation for the random vector and approximate its density as a Gaussian.\n",
    "That is, we write:\n",
    "$$\n",
    "p(x) \\approx \\mathcal{N}\\left(x|\\mu, \\sigma^2\\right),\n",
    "$$\n",
    "with $\\mu$ and $\\Sigma$ as given by the Laplace approximation.\n",
    "Now, we take the Taylor expansion of $f(x)$ around $x=\\mu$:\n",
    "$$\n",
    "f(x) = f(\\mu) + \\frac{df(\\mu)}{dx} (x-\\mu)+\\dots\n",
    "$$\n",
    "Now, let's take the expecation over the Laplace approximation of $p(x)$.\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbb{E}[f(X)] &=& \\mathbb{E}\\left[f(\\mu) + \\frac{df(\\mu)}{dx} (X-\\mu) + \\dots\\right]\\\\\n",
    "&=& f(\\mu) + \\frac{df(\\mu)}{dx}\\mathbb{E}[X-\\mu] + \\dots\\\\\n",
    "&=& f(\\mu) + \\dots\n",
    "\\end{array}\n",
    "$$\n",
    "So, to *second order* we have:\n",
    "$$\n",
    "\\mathbb{E}[f(X)] \\approx f(\\mu).\n",
    "$$\n",
    "You can, of course, construct higher order approximations in this way.\n",
    "\n",
    "Let's now do the variance.\n",
    "We will use the formula:\n",
    "$$\n",
    "\\mathbb{V}[f(X)] = \\mathbb{E}[f^2(X)] - \\left[\\mathbb{E}[f(X)]\\right]^2.\n",
    "$$\n",
    "Keeping up to second order terms, we get:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbb{E}[f^2(X)] &=& \\mathbb{E}\\left[\\left(f(\\mu) + \\frac{df(\\mu)}{dx} (X-\\mu) + \\dots\\right)^2\\right]\\\\\n",
    "&=& f^2(\\mu) + \\left(\\frac{df(\\mu)}{dx}\\right)^2\\mathbb{E}[(X-\\mu)^2] + 2f(\\mu)\\frac{df(\\mu)}{dx}\\mathbb{E}[X-\\mu] + \\dots\\\\\n",
    "&=& f^2(\\mu) + \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2 + \\dots\n",
    "\\end{array}\n",
    "$$\n",
    "Therefore, for the variance we get:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathbb{V}[f(X)] &=& f^2(\\mu) + \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2 + \\dots - \\left(f(\\mu) + \\dots\\right)^2\\\\\n",
    "&=& f^2(\\mu) + \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2 - f^2(\\mu) + \\dots\\\\\n",
    "&=& \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2 + \\dots\n",
    "\\end{array}\n",
    "$$\n",
    "So, to *first order* we have:\n",
    "$$\n",
    "\\mathbb{V}[f(X)] \\approx \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2\n",
    "$$\n",
    "And, of course, we can go to higher order approximations.\n",
    "\n",
    "Now, having approximations for the mean and the variance, we may approximate the full density of the random variable $Y=f(X)$ by a Gaussian:\n",
    "$$\n",
    "p(y) \\approx \\mathcal{N}\\left(y|f(\\mu), \\left(\\frac{df(\\mu)}{dx}\\right)^2\\sigma^2\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: 1D Propagation of Uncertainty\n",
    "\n",
    "Let's solve a (very simple) uncertainty propagation problem using these techniques.\n",
    "Consider:\n",
    "$$\n",
    "X\\sim \\mathcal{N}(\\pi/2, 0.1^2),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "f(x) = \\sin(x) e^{-0.5x}.\n",
    "$$\n",
    "We have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the densit of X:\n",
    "mu = 1.1\n",
    "sigma = 0.1\n",
    "X = st.norm(loc=mu, scale=sigma)\n",
    "# The function through which we want to propagate uncertainties\n",
    "f = lambda x: np.sin(x) * np.exp(-0.5 * x)\n",
    "fig1, ax1 = plt.subplots()\n",
    "x = np.linspace(0., np.pi, 200)\n",
    "ax1.plot(x, f(x))\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$f(x)$', color=sns.color_palette()[0])\n",
    "ax1.tick_params('y', colors=sns.color_palette()[0])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, X.pdf(x), color=sns.color_palette()[1])\n",
    "ax2.set_ylabel('$p(x)$', color=sns.color_palette()[1])\n",
    "ax2.tick_params('y', colors=sns.color_palette()[1])\n",
    "# The function at mu\n",
    "f_at_mu = f(mu)\n",
    "# The first derivative of the function at mu\n",
    "df_at_mu = scipy.misc.derivative(f, mu, dx=1e-8, n=1)\n",
    "# We can now construct a Gaussian approximation to the distribution of f(X)\n",
    "exp_f_X = f_at_mu\n",
    "var_f_X = f_at_mu ** 2 * sigma ** 2\n",
    "f_X = st.norm(loc=exp_f_X, scale=np.sqrt(var_f_X))\n",
    "# We will compare to the histogram of 10,000 samples\n",
    "y_samples = [f(X.rvs()) for _ in range(10000)]\n",
    "fig, ax = plt.subplots()\n",
    "y = np.linspace(exp_f_X - 6 * np.sqrt(var_f_X), exp_f_X + 6 * np.sqrt(var_f_X), 100)\n",
    "ax.plot(y, f_X.pdf(y))\n",
    "ax.set_xlim(0., 0.6)\n",
    "ax.hist(y_samples, bins=10, normed=True, alpha=0.5);\n",
    "ax.set_xlabel('$y$')\n",
    "ax.set_ylabel('$p(y)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Approach in Many Dimensions\n",
    "\n",
    "Let $X\\sim\\mathcal{N}(\\mu,\\Sigma)$ be a d-dimensional Gaussian random vector.\n",
    "If $X$ was not Gaussian, we could use the Laplace approximation.\n",
    "Let $f(x)$ be a m-dimensional function of $x$:\n",
    "$$\n",
    "f:\\mathbb{R}^d\\rightarrow \\mathbb{R}^m.\n",
    "$$\n",
    "To get the Taylor series expansion of $f(x)$, you need to work out the Taylor series expansion of each output component and put everything back together.\n",
    "You will get:\n",
    "$$\n",
    "f(x) = f(\\mu) + \\nabla f(\\mu) (x - \\mu) + \\dots,\n",
    "$$\n",
    "where $\\nabla f(\\mu)\\in\\mathbb{R}^{m\\times d}$ is the Jacobian matrix defined by:\n",
    "$$\n",
    "\\nabla f(\\mu) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial f_1(\\mu)}{\\partial x_1} & \\dots & \\frac{\\partial f_1(\\mu)}{\\partial x_d}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_m(\\mu)}{\\partial x_1} & \\dots & \\frac{\\partial f_m(\\mu)}{\\partial x_d}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "To first order, the expectation of $Y = f(X)$ is:\n",
    "$$\n",
    "\\mathbb{E}[f(X)] \\approx f(\\mu),\n",
    "$$\n",
    "while the covariance matrix is:\n",
    "$$\n",
    "\\mathbb{C}[f(X)] \\approx \\nabla f(\\mu)\\Sigma \\left(\\nabla f(\\mu)\\right)^T,\n",
    "$$\n",
    "\n",
    "Putting these two things together, we may approximate the density of $Y=f(X)$ with a multivariate Gaussian:\n",
    "$$\n",
    "p(y) \\approx \\mathcal{N}\\left(y|f(\\mu), \\nabla f(\\mu)\\Sigma \\left(\\nabla f(\\mu)\\right)^T\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Approach for Stochastic Dynamical Systems with Low Dimensional, Small Uncertainties\n",
    "Potentially using the Laplace approximation, assume that we have a $d$-dimensional random vector:\n",
    "$$\n",
    "X\\sim\\mathcal{N}(\\mu, \\Sigma).\n",
    "$$\n",
    "Consider the $m$-dimensional dynamical system:\n",
    "$$\n",
    "\\frac{dy}{dt} = g(y;X),\n",
    "$$\n",
    "with initial conditions\n",
    "$$\n",
    "y(0) = y_0(X).\n",
    "$$\n",
    "Since $X$ is a random vector, the response of the dynamical system $Y(t)$ is a stochastic process:\n",
    "$$\n",
    "Y(t) = y(t; X),\n",
    "$$\n",
    "where $y(t;X)$ is the solution of the initial value problem for a specific realization of the random vector $X$. \n",
    "We will approximate this stochastic process with a Gaussian process:\n",
    "$$\n",
    "Y(\\cdot) \\sim \\mbox{GP}\\left(m(\\cdot), k(\\cdot, \\cdot)\\right).\n",
    "$$\n",
    "This can easily be done, using the results of the previous sections.\n",
    "The mean is:\n",
    "$$\n",
    "m(t) = \\mathbb{E}[Y(t)] = \\mathbb{E}[y(t;X)] = y(t;\\mu).\n",
    "$$\n",
    "The covariance between two times is:\n",
    "$$\n",
    "k(t,t') = \\mathbb{C}[Y(t), Y(t')] = \\mathbb{C}[y(t;X), y(t';X)] = \\nabla_x y(t;\\mu) \\Sigma \\left(\\nabla_x y(t';\\mu)\\right)^T.\n",
    "$$\n",
    "The only complicating factor, is that we need a way to get $\\nabla_x y(t;\\mu)$.\n",
    "For this, we need the method of adjoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Sensitivities in Dynamical Systems using the Method of Ajdoints\n",
    "We will show how we can compute $\\nabla_x y(t;\\mu)$ using the method of adjoints.\n",
    "The idea is simple.\n",
    "We will derive the differential equation that $\\nabla_x y(t;\\mu)$ satisfies and we will be solving it as we solve for $y(t;\\mu)$.\n",
    "Let $i\\in\\{1,\\dots,m\\}$ and $j\\in\\{1,\\dots,d\\}$.\n",
    "We start with the initial conditions.\n",
    "We have:\n",
    "$$\n",
    "\\frac{\\partial y_i(0;\\mu)}{\\partial x_j} = \\frac{\\partial y_{0i}(\\mu)}{\\partial x_j}.\n",
    "$$\n",
    "Now, let's do the derivatives:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\frac{d}{dt}\\frac{\\partial y_i(t;\\mu)}{\\partial x_j} &=& \\frac{\\partial }{\\partial x_j}\\frac{dy_i(t,\\mu)}{dt} \\\\\n",
    "&=& \\frac{\\partial }{\\partial x_j} g_i(y(t;\\mu); \\mu)\\\\\n",
    "&=& \\sum_{r=1}^m\\frac{\\partial g_i(y(t;\\mu); \\mu)}{\\partial y_r}\\frac{\\partial y_r(t;\\mu)}{\\partial x_j} + \\frac{\\partial g_i(y(t;\\mu); \\mu)}{\\partial x_j}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So, we see that the dynamical system that $\\nabla_x y(t;\\mu)$ satisfies is:\n",
    "$$\n",
    "\\frac{d}{dt}\\nabla_x y(t;\\mu) =  \\left((\\nabla_y g(y;\\mu)\\right)^T \\nabla_x y(t;\\mu) + \\nabla_x g(y;\\mu),\n",
    "$$\n",
    "with initial conditions\n",
    "$$\n",
    "\\nabla_x y(0;\\mu) = \\nabla_x y_0(\\mu).\n",
    "$$\n",
    "This is known as the *adjoint dynamical* system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: Dynamical System with Uncertain Paramters\n",
    "\n",
    "Take the random vector:\n",
    "$$\n",
    "X = (X_1, X_2),\n",
    "$$\n",
    "and assume that the components are independent Gaussian:\n",
    "$$\n",
    "X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2).\n",
    "$$\n",
    "So, for the full random vector we have a mean:\n",
    "$$\n",
    "\\mu = (\\mu_1, \\mu_2),\n",
    "$$\n",
    "and a covariance matrix:\n",
    "$$\n",
    "\\Sigma = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2).\n",
    "$$\n",
    "\n",
    "Consider the ODE:\n",
    "  \\begin{align*}\n",
    "    &\\dot{y} = \\frac{d y(t)}{dt} =-X_1y(t) \\equiv g(y,X),\\\\\n",
    "    &\\qquad y(0) = X_2 \\equiv y_0(X).\n",
    "  \\end{align*}\n",
    "The adjoint system describes the evolution of the derivative:\n",
    "$$\n",
    "\\nabla_x y(t;\\mu) = \\left(\\frac{\\partial y(t;\\mu)}{\\partial x_1}, \\frac{\\partial y(t;\\mu)}{\\partial x_2}\\right).\n",
    "$$\n",
    "According to the formulas, we wrote before we need:\n",
    "$$\n",
    "\\nabla_x y_0(\\mu) = (0, 1),\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial g(y;\\mu)}{\\partial y} = -\\mu_1,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla_x g(y;\\mu) = (-y, 0).\n",
    "$$\n",
    "\n",
    "Putting everything together, the adjoint dynamical system is:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\frac{d}{dt}\\frac{\\partial y}{\\partial x_1} &=& -x_1 \\frac{\\partial y}{\\partial x_1} - y,\\\\\n",
    "\\frac{d}{dt}\\frac{\\partial y}{\\partial x_2} &=& -x_1 \\frac{\\partial y}{\\partial x_2},\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "with initial conditions:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\frac{d}{dt}\\frac{\\partial y(0)}{\\partial x_1} &=& 0,\\\\\n",
    "\\frac{d}{dt}\\frac{\\partial y(0)}{\\partial x_2} &=& 1.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's extend the solver object we had before so that it also gives the adjoint solution\n",
    "import scipy.integrate\n",
    "\n",
    "class Ex1Solver(object):\n",
    "    \"\"\"\n",
    "    An object that can solver the afforementioned ODE problem.\n",
    "    It will work just like a multivariate function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nt=100, T=5):\n",
    "        \"\"\"\n",
    "        This is the initializer of the class.\n",
    "        \n",
    "        Arguments:\n",
    "            nt - The number of timesteps.\n",
    "            T  - The final time.\n",
    "        \"\"\"\n",
    "        self.nt = nt\n",
    "        self.T = T\n",
    "        self.t = np.linspace(0, T, nt) # The timesteps on which we will get the solution\n",
    "        # The following are not essential, but they are convenient\n",
    "        self.num_input = 2             # The number of inputs the class accepts\n",
    "        self.num_output = nt           # The number of outputs the class returns\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        This special class method emulates a function call.\n",
    "        \n",
    "        Arguments:\n",
    "            x - A 1D numpy array with 2 elements. This represents the stochastic input x = (x1, x2).\n",
    "        \"\"\"\n",
    "        # The dynamics of the adjoint z = y, dy/dx1, dy/dx2\n",
    "        def g(z, t, x):\n",
    "            return -x[0] * z[0], -x[0] * z[1] - z[0], -x[0] * z[2]\n",
    "        # The initial condition\n",
    "        y0 = (x[1], 0, 1)\n",
    "        # We are ready to solve the ODE\n",
    "        y = scipy.integrate.odeint(g, y0, self.t, args=(x,))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = np.array([0.05, 8.])\n",
    "Sigma = np.diag([0.01 ** 2, 0.01 ** 2])\n",
    "solver = Ex1Solver(nt=500, T=100)\n",
    "z_at_mu = solver(mu)\n",
    "y_at_mu = z_at_mu[:, 0]\n",
    "grad_x_y_at_mu = z_at_mu[:, 1:]\n",
    "C = np.dot(grad_x_y_at_mu, np.dot(Sigma, grad_x_y_at_mu.T))\n",
    "\n",
    "# For comparison, let's compute the mean and the variance with 1,000 LHS samples\n",
    "import design\n",
    "num_lhs = 1000\n",
    "X_lhs = design.latin_center(num_lhs, 2) # These are uniformly distributed - Turn them to standard normal\n",
    "X_samples = mu + np.dot(st.norm.ppf(X_lhs), np.sqrt(Sigma))\n",
    "s = 0.\n",
    "s2 = 0.\n",
    "for x in X_samples:\n",
    "    y = solver(x)[:, 0]\n",
    "    s += y\n",
    "    s2 += y ** 2\n",
    "y_mu_lhs = s / num_lhs\n",
    "y_var_lhs = s2 / num_lhs - y_mu_lhs ** 2\n",
    "\n",
    "# Make the figure\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the mean and compare to LHS\n",
    "ax1.plot(solver.t, y_mu_lhs, color=sns.color_palette()[0], label='LHS mean ($n=%d$)' % num_lhs)\n",
    "ax1.plot(solver.t, y_at_mu, '--', color=sns.color_palette()[0], label='Perturbation mean')\n",
    "ax1.set_xlabel('$t$')\n",
    "ax1.set_ylabel('$\\mu(t)$', color=sns.color_palette()[0])\n",
    "ax1.tick_params('y', colors=sns.color_palette()[0])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot variance and compare to LHS\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(solver.t, y_var_lhs, color=sns.color_palette()[1], label='LHS variance ($n=%d$)' % num_lhs)\n",
    "ax2.plot(solver.t, np.diag(C), '--', color=sns.color_palette()[1], label='Perturbation variance')\n",
    "ax2.set_ylabel('$\\sigma^2(t) = k(t, t)$', color=sns.color_palette()[1])\n",
    "ax2.tick_params('y', colors=sns.color_palette()[1])\n",
    "plt.legend(loc='center right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's do 95% intervals\n",
    "s = np.sqrt(np.diag(C))\n",
    "l = y_at_mu - 2 * s\n",
    "u = y_at_mu + 2 * s\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(solver.t, y_at_mu)\n",
    "ax.fill_between(solver.t, l, u, alpha=0.25)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take some sample paths\n",
    "L = np.linalg.cholesky(C + 1e-10 * np.eye(C.shape[0])) # add something to the diagonal for numerical stability\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "for _ in range(5):\n",
    "    xi = np.random.randn(L.shape[0])\n",
    "    y = y_at_mu + np.dot(L, xi)\n",
    "    ax.plot(solver.t, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
