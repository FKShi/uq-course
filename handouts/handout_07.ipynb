{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb51d93e-8113-4531-b387-a144c0855c5d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting Generalized Linear Models (Part I)\n",
    "> All models are wrong but some are useful. \n",
    "> *([George Box, 1976](http://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949#.VrkhMMd6FBw))*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cec58143-3558-4f98-87d7-ee345a9dc752"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "+ Generalized linear models\n",
    "+ Least squares\n",
    "+ Maximum Likelihood Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2d3386f0-0193-4b06-9e73-9f043ecfcc86"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Readings\n",
    "\n",
    "Before coming to class, please read the following:\n",
    "\n",
    "+ [Ch. 3 of Bishop, 2006](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)\n",
    "\n",
    "+ [Ohio State University, Bayesian Linear Regression](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0ahUKEwikxsiPuJPKAhVE32MKHRoMCtsQFggyMAI&url=http%3A%2F%2Fweb.cse.ohio-state.edu%2F~kulis%2Fteaching%2F788_sp12%2Fscribe_notes%2Flecture5.pdf&usg=AFQjCNFvxuyBfFkRN8bdJAvd_dlZdsShEw&sig2=UqakvfANehNUUK1J9rXIiQ)\n",
    "\n",
    "You can also check out this 10 minutes short Youtube video on Bayesian Linear Regression - \n",
    "+ [Mathematicalmonk, Bayesian Linear Regression](https://www.youtube.com/watch?v=dtkGq9tdYcI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2c80cfd4-ab6b-4b52-a924-fc73d7803597"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning - Regression\n",
    "Say that you observe $n$, $d$-dimensional, *inputs*:\n",
    "$$\n",
    "\\mathbf{x}_{1:n} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\},\n",
    "$$\n",
    "and *outputs*:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} = \\{y_1,\\dots,y_n\\}.\n",
    "$$\n",
    "The regression problem consits of using the data $\\mathbf{x}_{1:n}$ and $\\mathbf{y}_{1:n}$ to find\n",
    "the map that connects the inputs to the outputs.\n",
    "\n",
    "We will be playing with the following dataset consisting of motorcycle crash data.\n",
    "Namely, it records the observed head acceleration (in g) at different times (in miliseconds) after the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('motor.dat')\n",
    "X = data[:, 0][:, None]\n",
    "Y = data[:, 1]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e037055-71d5-4fd4-9788-568b7d6b2a1b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression model\n",
    "+ Any model that connects $\\mathbf{x}$ to $y$ through the use of some parameters.\n",
    "+ Linear regression:\n",
    "$$\n",
    "y(\\mathbf{x}; \\mathbf{w}) = w_{0} + w_{1}x_{1} + ... + w_{D}x_{D},\n",
    "$$\n",
    "+ Generalized linear model:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}\n",
    "$$\n",
    "where $\\mathbf{w} = (w_{1}, ... , w_{m})^{T}$ and $\\boldsymbol{\\phi} = (\\phi_{1}, ..., \\phi_{m})^{T}$ are arbitrary basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "00c51f95-ee09-45d6-adff-4f99c404a91d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naming conventions\n",
    "+ $\\mathbf{x}$: input\n",
    "+ $\\phi_i(\\mathbf{x})$: feature or basis function\n",
    "+ $y$: output or target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "79ad628b-59c5-4a2a-808b-7be68eba1a91"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized linear model\n",
    "+ Note that the *generalized linear model*:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\phi(x)},\n",
    "$$\n",
    "is linear in $\\mathbf{w}$ not $\\mathbf{x}$.\n",
    "+ $\\boldsymbol{\\phi}(\\mathbf{x})$ can be non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8dfba629-73cf-49d2-bb00-5292b159d463"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of generalized linear models:\n",
    "Remember:\n",
    "$$\n",
    "y(x;\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\phi(x)}.\n",
    "$$\n",
    "Some examples of $\\phi_j(\\mathbf{x})$'s are:\n",
    "+ Linear, $\\phi_j(\\mathbf{x}) = x_j$.\n",
    "+ Polynomials, $\\phi_j(x) = \\sum_{\\alpha\\in\\mathcal{A}_j}\\beta_{\\alpha}\\mathbf{x}^{\\alpha}$.\n",
    "+ Radial basis function, $\\phi_j(x) = \\exp\\left\\{-\\frac{\\parallel \\mathbf{x} - \\mathbf{x}_j\\parallel^2}{2\\ell^2}\\right\\}$.\n",
    "+ Fourier series, $\\phi_{2j}(x) = \\cos\\left(\\frac{2j\\pi}{L}x\\right)$ and $\\phi_{2j+1}(x)=\\sin\\left(\\frac{2j\\pi}{L}x\\right)$.\n",
    "+ $\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d41e547c-a72a-4ca1-9063-e58ca31eb136"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting GLMs using Least Squares\n",
    "+ First published by [Legendre](https://en.wikipedia.org/wiki/Adrien-Marie_Legendre) in 1805.\n",
    "+ The idea is to find the best $\\mathbf{w}$ by minimizing a quadratic loss function:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w})\\equiv\\mathcal{L}(\\mathbf{w};\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}) = \\sum_{i=1}^n\\left[y(\\mathbf{x}_i;\\mathbf{w}) - y_i\\right]^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4db99232-c103-4ad6-9c3c-c1b0a086f014"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expressing the Loss Function using Matrix-Vector multiplication\n",
    "+ The loss function can be re-expressed as:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\mathcal{L}(\\mathbf{w}) &=& \\lVert\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\rVert^2\\\\\n",
    "&=& \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\right)^T\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\right).\n",
    "\\end{array}\n",
    "$$\n",
    "+ Here $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{n\\times m}$ is the **design matrix**:\n",
    "$$\n",
    "\\Phi_{ij} = \\phi_j(\\mathbf{x}_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "df134093-4171-4994-a50c-f0bfb92df77b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing the Loss Function\n",
    "+ Take the derivative of $\\mathcal{L}(\\mathbf{w})$ with respect to $\\mathbf{w}$.\n",
    "+ Set it equal to zero and solve for $\\mathbf{w}$.\n",
    "+ You will get [(Bishop, 2006)](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738):\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{LS}} = \\left(\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^{T}\\mathbf{y}_{1:n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "73e5c535-33a8-41a9-a93b-d1b3cbd2b1e2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How NOT to Solve a Linear Least Squares Problem\n",
    "+ Our goal is to find:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{LS}} = \\left(\\mathbf{\\Phi}^{T}\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^{T}\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "+ The worst thing you can do is find $\\mathbf{\\Phi^{T}\\Phi}$ and invert it.\n",
    "+ Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "541036c4-83e9-4b77-b109-549e9ea20a5b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Solve a Linear Least Squares Problem\n",
    "+ You compute the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) of $\\mathbf{\\Phi}$, i.e.,\n",
    "$$\n",
    "\\mathbf{\\Phi} = \\mathbf{QR},\n",
    "$$\n",
    "where $\\mathbf{Q}\\in\\mathbb{R}^{n\\times n}$ is [orthogonal](https://en.wikipedia.org/wiki/Orthogonal_matrix), i.e.,\n",
    "$\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}$, and $\\mathbf{R}\\in\\mathbb{R}^{n\\times m}$ upper [triangular](https://en.wikipedia.org/wiki/Triangular_matrix).\n",
    "+ Then, solve the following problem using [backward substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_substitution):\n",
    "$$\n",
    "\\mathbf{R}\\mathbf{w}_{\\mbox{LS}} = \\mathbf{Q}\\mathbf{y}_{1:n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d37717cc-1cf3-4ff5-9c84-7fac5fcd26e0"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Actually Solve a Linear Least Squares Problem\n",
    "+ Just use:\n",
    "\n",
    "> [numpy.linalg.lstsq](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.lstsq.html)\n",
    "\n",
    "+ You give it $\\mathbf{\\Phi}$ and $\\mathbf{y}_{1:n}$ and it returns $\\mathbf{w}_{\\mbox{LS}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f7a6d550-a9a2-47b9-a038-a9b8d0c96f4f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example - Linear Least Squares - Linear Features\n",
    "We will use two features ($m=2$):\n",
    "$$\n",
    "\\phi_1(\\mathbf{x}) = 1,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\phi_2(\\mathbf{x}) = x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f0d6dfc3-f56b-4e37-912f-5223121d4e5f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implementation of the basis functions\n",
    "class LinearBasis(object):\n",
    "    \"\"\"\n",
    "    Represents a 1D linear basis.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_basis = 2 # The number of basis functions\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        ``x`` should be a 1D array.\n",
    "        \"\"\"\n",
    "        return [1., x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eeac469a-7828-4ce3-84ff-cf4a4be0bc56"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This can be used as a function, for example:\n",
    "phi = LinearBasis()\n",
    "print phi([4.])    # The input has to be an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c3a1d5e4-8e87-45e0-8308-2de2e55c620e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We need a generic function that computes the design matrix\n",
    "def compute_design_matrix(X, phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    X   -  The observed inputs (1D array)\n",
    "    phi -  The basis functions.\n",
    "    \"\"\"\n",
    "    num_observations = X.shape[0]\n",
    "    num_basis = phi.num_basis\n",
    "    Phi = np.ndarray((num_observations, num_basis))\n",
    "    for i in xrange(num_observations):\n",
    "        Phi[i, :] = phi(X[i, :])\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "21c0c887-647c-4608-8b8c-c13e9f53336f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's test this:\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "print 'Phi:'\n",
    "print Phi[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "86040e79-5c8b-4de0-b28d-fe7564f872b5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ready to do least squares:\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "print 'w_LS:'\n",
    "print w_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "23455f07-6dbf-4a3a-a1fd-a226494ba76f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the result on these points:\n",
    "X_p = np.linspace(0, 60, 100)[:, None]\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, w_LS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='LS Prediction (Linear Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9109fb5-f192-424a-9e8f-ba42f77f8177"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try Polynomials of Higher Degree\n",
    "\n",
    "$$\n",
    "\\phi_j(x) = x^{j-1}, j=1,\\dots,m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "82469b64-59da-4e3d-a84d-be2e61fecda9"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a class for the polynomials:\n",
    "class PolynomialBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    degree  -  The degree of the polynomial.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        self.num_basis = degree + 1\n",
    "    def __call__(self, x):\n",
    "        return np.array([x[0] ** i for i in range(self.degree + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "3788a7e5-4526-44fb-9579-7b0005e4a22f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize the basis functions\n",
    "degree = 2\n",
    "phi = PolynomialBasis(degree)\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "plt.plot(X_p, Phi_p)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$\\phi_i(x)$')\n",
    "# It does not look very pretty because the polynomials grow very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6dde20d0-6589-4e18-a2c4-c4e3e0f0ad17"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do the regression\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "Y_p = np.dot(Phi_p, w_LS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='LS Prediction (Polynomial Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "823a103b-3b00-4d7d-b416-7d0a41ba6041"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with polynomials of degree 4, 5, 10, 20\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which degree (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8d4456f2-f546-4fd6-841a-e37495bd6f48"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try a Fourier Basis\n",
    "$$\n",
    "\\phi_{2j}(x) = \\cos\\left(\\frac{2j\\pi}{L}x)\\right),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\phi_{2j+1}(x) = \\sin\\left(\\frac{2j\\pi}{L}x)\\right),\n",
    "$$\n",
    "for $j=1,\\dots,m/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c4043123-7260-4ff0-b16f-f72c57deebe1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a class for the Fourier basis:\n",
    "class FourierBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    num_terms  -  The number of Fourier terms.\n",
    "    L          -  The period of the function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_terms, L):\n",
    "        self.num_terms = num_terms\n",
    "        self.L = L\n",
    "        self.num_basis = 2 * num_terms\n",
    "    def __call__(self, x):\n",
    "        res = np.ndarray((self.num_basis,))\n",
    "        for i in xrange(num_terms):\n",
    "            res[2 * i] = np.cos(2 * i * np.pi / self.L * x[0])\n",
    "            res[2 * i + 1] = np.sin(2 * (i+1) * np.pi / self.L * x[0])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "cd3085c5-1e66-43fa-bd1a-a209ad314dbf"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize the basis functions\n",
    "num_terms = 2\n",
    "L = 60.\n",
    "phi = FourierBasis(num_terms, L)\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "plt.plot(X_p, Phi_p);\n",
    "plt.ylabel(r'$\\phi_i(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d29405cf-f76c-4943-9e87-e8e5ca59ecd3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do the regression\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "Y_p = np.dot(Phi_p, w_LS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='LS Prediction (Fourier Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9d01c1e6-1cc8-4843-981c-7622d906cbaf"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with 4, 10, 20, 40, terms.\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3c544bb0-8327-4849-8cff-d7f056aa5d05"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try a radial basis function approximation:\n",
    "$$\n",
    "\\phi_i(x) = \\exp\\left\\{-\\frac{(x-x_i^c)^2}{2\\ell^2}\\right\\},\n",
    "$$\n",
    "where $x_i^c$ are points about each the basis functions are centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "17381342-ea47-4d49-82d5-b76cae99c709"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a class for the polynomials:\n",
    "class RadialBasisFunctions(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the radial basis functions.\n",
    "    ell -  The assumed lengthscale.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, ell):\n",
    "        self.X = X\n",
    "        self.ell = ell\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        return np.exp(-.5 * (x - self.X) ** 2 / self.ell ** 2).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "51f80b62-e964-487d-9004-43b78726f762"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ell = 5.\n",
    "num_terms = 10\n",
    "Xc = np.linspace(0, 60, num_terms)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "plt.plot(X_p, Phi_p);\n",
    "plt.ylabel(r'$\\phi_i(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ee1903cc-8be0-40e4-b777-97fadbb3fca6"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do the regression\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "Y_p = np.dot(Phi_p, w_LS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='LS Prediction (Radial Basis Functions)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0f845bdd-4283-42a6-b0e1-9240fb831a99"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with different values of ell and centers.\n",
    "+ When are we underfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "565fa7c7-ffe3-4b46-bd29-831cc0e360ea"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try an approximation using step functions:\n",
    "$$\n",
    "\\phi_i(x) = \\begin{cases} 1,& x\\ge x_i^c\\\\\n",
    "                          0,&\\;\\mbox{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "where $x_i^c$ are points about each the basis functions are centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7de2995b-945b-45df-8a38-8b79600620d1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here is a class for the polynomials:\n",
    "class StepFunctionBasis(object):\n",
    "    \"\"\"\n",
    "    A set of step functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the step functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        res = np.ones((self.num_basis, ))\n",
    "        res[x < self.X.flatten()] = 0.\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "80dd9d81-9465-4cef-8271-62e3aebaab04"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xc = np.linspace(0, 60, 5)\n",
    "phi = StepFunctionBasis(Xc)\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "plt.plot(X_p, Phi_p)\n",
    "plt.ylim([-.1, 1.1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8819046f-0dfd-4342-b42d-dc2aea040d8c"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do the regression\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_LS = np.linalg.lstsq(Phi, Y, rcond=None)[0]\n",
    "Y_p = np.dot(Phi_p, w_LS)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='LS Prediction (Step Function Basis)')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2b0e3102-b7e0-42fb-adbf-5907c8862250"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with different number of centers.\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bcc25b56-10f6-4765-9dc4-6df275ae75d5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Issues with Least Squares\n",
    "+ How do I quantify the measurement noise?\n",
    "+ How many basis functions should I use?\n",
    "+ Which basis functions should I use?\n",
    "+ How do I pick the parameters of the basis functions, e.g., the lengthscale $\\ell$ of the RBFs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3152b401-5400-4b95-937b-0e83811b53f5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic Regression - Version 1\n",
    "\n",
    "+ We wish to model the data using some **fixed** basis/features:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "+ We *model the measurement process* using a **likelihood** function:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w} \\sim p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5293fe4-2cf1-4316-8e6c-b72ae908535b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation\n",
    "+ $p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w})$ tells us:\n",
    "\n",
    "> How plausible is it to observe $\\mathbf{y}_{1:n}$ at inputs $\\mathbf{x}_{1:n}$, if we know that the model parameters are $\\mathbf{w}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1442dd37-f05a-408c-939e-49caa69f0371"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the Likelihood\n",
    "If the measurements are independent, then (probability theory):\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}) = \\prod_{i=1}^np(y_i|\\mathbf{x}_i, \\mathbf{w}),\n",
    "$$\n",
    "where $p(y_i|\\mathbf{x}_i,\\mathbf{w})$ is the likelihood of a single measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3ccf5b60-4ea0-40f3-8950-71ce885c6155"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Likelihood\n",
    "+ We assign:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(y_i|\\mathbf{x}_i, \\mathbf{w}, \\sigma) &=& \\mathcal{N}\\left(y_i| y(\\mathbf{x}_i;\\mathbf{w}), \\sigma^2\\right)\\\\\n",
    "&=& \\mathcal{N}\\left(y_i | \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)}, \\sigma^2\\right),\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\sigma$ models the **noise**.\n",
    "+ We say:\n",
    "\n",
    "> We beleive that our measurement is around the model prediction $\\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}$\n",
    "but it is contaminated with Gaussian noice of variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0dab27d1-2676-4e3d-9016-8e387f915fe7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Likelihood of Many Independent Observations\n",
    "We have:\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) = \\mathcal{N}\\left(\\mathbf{y}_{1:n} | \\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}_n\\right).\n",
    "$$\n",
    "Let's look at the function form ([Wiki](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)):\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) \n",
    "= (2\\pi)^{-\\frac{n}{2}}\\sigma^{-n} e^{-\\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cc85d310-8b96-4802-b539-aa4d0d4edd8e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimate of $\\mathbf{w}$\n",
    "\n",
    "+ Once we have a likelihood, we can train the model by maximizing the likelihood:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} = \\arg\\max_{\\mathbf{w}} p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma).\n",
    "$$\n",
    "+ For Gaussian likelihood\n",
    "$$\n",
    "{\\tiny\n",
    "\\log p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) =\n",
    "-\\frac{n}{2}\\log(2\\pi)\n",
    "-n\\log\\sigma\n",
    "- \\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2.\n",
    "}\n",
    "$$\n",
    "+ And we find that:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} \\equiv \\mathbf{w}_{\\mbox{LS}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c0097f83-24f6-4af6-9e87-4b304e915371"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimate of $\\sigma$\n",
    "+ You just have to maximize likelihood also over $\\sigma$.\n",
    "+ For Gaussian likelihood:\n",
    "    + Take the derivative of $p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n},\\mathbf{w}_{\\mbox{MLE}},\\sigma)$ with respect to $\\sigma$.\n",
    "    + Set to zero, and solve for $\\sigma$.\n",
    "    + You will get:\n",
    "$$\n",
    "{\\tiny\n",
    "\\sigma_{\\mbox{MLE}}^2 = \\frac{\\lVert \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\rVert^2}{n}.\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "196ab55c-4504-4a03-b9d5-f3803496b4bb"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making Predictions\n",
    "+ How do we make predictions about $y$ at a new point $\\mathbf{x}$?\n",
    "+ We just use the laws of probability...\n",
    "+ For the Gaussian likelihood, the **predictive distribution** is:\n",
    "$$\n",
    "{\\tiny\n",
    "p(y|\\mathbf{x}, \\mathbf{w}_{\\mbox{MLE}}, \\sigma^2_{\\mathbf{\\mbox{MLE}}}) = \n",
    "\\mathcal{N}\\left(y\\middle|\\mathbf{w}_{\\mbox{MLE}}^T\\mathbf{\\phi}(\\mathbf{x}), \\sigma_{\\mbox{MLE}}^2\\right).\n",
    "}\n",
    "$$\n",
    "+ Let's try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2cc38b5c-5278-4838-a60c-6801a34b35a2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "28427bc1-059f-43da-b2d4-1318e913e7e5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's do it\n",
    "degree = 1\n",
    "phi = PolynomialBasis(degree)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_MLE, res_MLE = np.linalg.lstsq(Phi, Y, rcond=None)[0:2] # Note that we\n",
    "                                              # now also use the second\n",
    "                                              # output of lstsq...\n",
    "sigma_MLE = np.sqrt(res_MLE / X.shape[0])\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, w_MLE)\n",
    "Y_l = Y_p - 2. * sigma_MLE # Lower predictive bound\n",
    "Y_u = Y_p + 2. * sigma_MLE # Upper predictive bound\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='MLE Prediction (Polynomial Basis)')\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f61ebd96-65e6-4ade-8637-8967b616a23f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1be50984-f55f-4565-b836-eef796530eab"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ell = 2.\n",
    "Xc = np.linspace(0, 60, 20)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "w_MLE, res_MLE = np.linalg.lstsq(Phi, Y, rcond=None)[0:2] # Note that we\n",
    "                                              # now also use the second\n",
    "                                              # output of lstsq...\n",
    "sigma_MLE = np.sqrt(res_MLE / X.shape[0])\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, w_MLE)\n",
    "Y_l = Y_p - 2. * sigma_MLE # Lower predictive bound\n",
    "Y_u = Y_p + 2. * sigma_MLE # Upper predictive bound\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='MLE Prediction (Radial Basis Functions)')\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "23b8789b-bf0e-46a3-a3ed-c96f44b84e0b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Issues with Maximum Likelihood\n",
    "+ How many basis functions should I use?\n",
    "+ Which basis functions should I use?\n",
    "+ How do I pick the parameters of the basis functions, e.g., the lengthscale $\\ell$ of the RBFs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "Plot the MLE noise error bars for:\n",
    "\n",
    "+ A Fourier-basis model.\n",
    "+ The stepfunction-basis model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "010979b2-9edc-4853-97dc-d1a6fb9d6ba1",
    "theme": {
     "010979b2-9edc-4853-97dc-d1a6fb9d6ba1": {
      "id": "010979b2-9edc-4853-97dc-d1a6fb9d6ba1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
