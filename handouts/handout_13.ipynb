{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 13 - Uncertainty Propagation (Sampling Methods I)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ to understand the emergence of the curse of dimensionality\n",
    "+ to estimate multi-dimensional integrals using Monte Carlo\n",
    "+ to quantify the epistemic uncertainty in Monte Carlo estimates\n",
    "+ to propagate uncertainty through a boundary value problem\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ Monte Carlo Strategies in Scientific Computing (Jun S. Liu, 2001): Chapters 1 and 2.\n",
    "\n",
    "+ Monte Carlo Statistical Methods (Christian P. Robert and George Casella, 1999): Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "Let $X$ be a random variable with probability density $p(x)$ and $f(X)$ be a function of $X$.\n",
    "We would like to calculate the integral:\n",
    "$$\n",
    "\\mathbb{E}\\left[f(X)\\right] := \\int f(x)p(x)dx.\n",
    "$$\n",
    "This may be trivial when $X$ is low dimensional, but it is an extremely difficult problem when it is higher dimensional.\n",
    "\n",
    "To appreciate the emergence of this *curse of dimensionality*, let us start with a concrete example.\n",
    "Let us assume that $X\\sim\\mathcal{U}\\left([0,1]^d\\right)$, where $d\\ge 1$.\n",
    "So $p(x) = 1$ in this case.\n",
    "A simple way to compute the expectation $\\mathbb{E}[f(X)] = \\int_{[0,1]^d}f(x)dx$ is to the following:\n",
    "\n",
    "+ Pick a set of $n$ equidistant points in $[0,1]$.\n",
    "\n",
    "+ Use these points to construct a regular grid in $[0,1]^d$.\n",
    "\n",
    "+ The grid consists of a total of $n^d$ boxes each one with volume $n^{-d}$.\n",
    "Let the center of the $m$-th box be $x_{c,j}, j=1,\\dots,n^d$.\n",
    "\n",
    "+ Evaluate the function values at the centers of the blocks $f(x_{c,j})$.\n",
    "\n",
    "+ Since each box contributes $n^{-d}f(x_{c,j})$ to the ingegral, you may approximate the whole thing by:\n",
    "$$\n",
    "I_n = n^{-d}\\sum_{j=1}^{n^d}f(x_{c,j}).\n",
    "$$\n",
    "\n",
    "Something is seriously wrong here.\n",
    "Let's think about the number of function evaluations that we will have to do if we used $n=10$ per input dimension.\n",
    "For the sake of the argument suppose that it takes one milisecond to evaluate the function.\n",
    "How much time would you need to carry out the calculation:\n",
    "\n",
    "+ $d=2$ gives $10^2 = 100$ function evaluations. So, you need 1 second.\n",
    "\n",
    "+ $d=3$ gives $10^3 = 1,000$ function evaluations. So, you need 10 seconds. \n",
    "\n",
    "+ $d=5$ gives $10^5 = 10,000$ function evaluations. So, you need 100 seconds.\n",
    "\n",
    "+ $d=6$ gives $10^6 = 100,000$ function evaluations. So, you need 1000 seconds or about 16 minutes.\n",
    "\n",
    "+ $d=10$ gives $10^{10}$ function evaluations. So, you need 11 days...\n",
    "\n",
    "+ $d=100$ gives $10^{100}$ function evaluations. So, you need ...\n",
    "\n",
    "+ forget it...\n",
    "\n",
    "This is the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Integration\n",
    "So, is there a way to evaluate the expectation faster?\n",
    "Yes, but it is not obvious immediatly why it works.\n",
    "There is a theorem from statistics called the *Strong Law of Large Numbers*.\n",
    "This theorem states that if you have a sequence of iid random variables $X_1,X_2,...$ with probability density $p(x)$, then under fairely general conditions the empirical average:\n",
    "$$\n",
    "I_n = \\frac{1}{n}\\sum_{j=1}^nf(X_j)\\longrightarrow \\mathbb{E}[f(X)],\n",
    "$$\n",
    "almost surely as $n\\rightarrow\\infty$.\n",
    "\n",
    "Let's try it out with a test function in 1D (Example 3.4 of Casella).\n",
    "Assume that $X\\sim\\mathcal{U}([0,1])$ and pick:\n",
    "$$\n",
    "f(x) = \\left(\\cos(50x) + \\sin(20x)\\right)^2.\n",
    "$$\n",
    "The correct value for the expectation can be found analytically and it is:\n",
    "$$\n",
    "\\mathbb{E}[f(x)] = 0.965.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function\n",
    "f = lambda x: (np.cos(50 * x) + np.sin(20 * x)) ** 2\n",
    "\n",
    "# Let's visualize is first\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0, 1, 300)\n",
    "ax.plot(x, f(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's implement the monte carlo procedure in a generic way\n",
    "def get_mc_estimate(max_n=1000, func=f, dim=1, sampler=np.random.rand):\n",
    "    \"\"\"\n",
    "    Return the the MC estimate of the 1D integral using the previously defined f function.\n",
    "    Note that this only samples from the uniform distribution in ``dim`` dimensions.\n",
    "    \n",
    "    :param max_n:   Maximum number of samples.\n",
    "    :param func:    The function to integrate.\n",
    "    :parma dim:     The dimension of the input.\n",
    "    :param sampler: A function that samples x's.\n",
    "    \"\"\"\n",
    "    I = np.ndarray((max_n,))    # A running estimate of the expecation\n",
    "    s = 0.                      # A variable to keep track of the sum\n",
    "    for n in range(max_n):\n",
    "        # sample an x\n",
    "        x = sampler(dim)\n",
    "        # evaluate f\n",
    "        y = func(x)\n",
    "        # Update the sum\n",
    "        s += y\n",
    "        # The current approximation of the integral is\n",
    "        I[n] = s / (n + 1)\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try running it\n",
    "num_mc = 1   # How many times do you want to run MC\n",
    "max_n = 100  # Maximum number of samples to take per MC run\n",
    "fig, ax = plt.subplots()\n",
    "for _ in range(num_mc):\n",
    "    I = get_mc_estimate(max_n=max_n, func=f, dim=1)\n",
    "    ax.plot(np.arange(1, max_n+1), I)\n",
    "ax.plot(np.arange(1, max_n+1), [0.965] * max_n, color='r')\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$I_n$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Increase ``max_n`` until you get an answer that is close enough to the correct answer (the red line).\n",
    "+ Reduce ``max_n`` back to a small number, say 1,000. Run the code 2-3 times to observe that every time you get a slightly different answer...\n",
    "+ Set ``num_mc`` to 10 (or higher). Observe how different MC runs envelop the correct answer. This is epistemic uncertainty. How can we get it without running this repeatedly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the Epistemic Uncertainty of an MC Estimate\n",
    "The *Central Limit Theorem* comes to rescue.\n",
    "It states that if $\\mathbb{V}[f(X)]$ exists, then the empirical average:\n",
    "$$\n",
    "I_n:=\\frac{1}{n}\\sum_{j=1}^nf(X_j) \\rightarrow \\mathcal{N}\\left(\\mathbb{E}[f(X)], \\frac{1}{n}\\mathbb{V}[f(X)]\\right),\n",
    "$$\n",
    "in distribution as $n\\rightarrow \\infty$.\n",
    "This suggests that, the true expectation must be within two standard deviations of $I_n$, i.e.,\n",
    "$$\n",
    "I := \\mathbb{E}[f(X)] = I_n \\pm \\frac{2}{\\sqrt{m}}\\mathbb{V}[f(X)].\n",
    "$$\n",
    "This is perfect, except from the fact that we don't really know what $\\mathbb{V}[f(X)]$ is...\n",
    "Well, let's approximate it also with MC!\n",
    "Set:\n",
    "$$\n",
    "V_n = \\frac{1}{n}\\sum_{j=1}^n\\left(f(X_j) - I_n\\right)^2.\n",
    "$$\n",
    "Then we can say that:\n",
    "$$\n",
    "I = I_n \\pm \\frac{2}{\\sqrt{n}}\\sqrt{V_n}.\n",
    "$$\n",
    "Let's see if this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's modify the previous function so that it also returns the variance estimate\n",
    "def get_mc_estimate(max_n=1000, func=f, dim=1, sampler=np.random.rand):\n",
    "    \"\"\"\n",
    "    Return the the MC estimate of the 1D integral using the previously defined f function.\n",
    "    Note that this only samples from the uniform distribution in ``dim`` dimensions.\n",
    "    \n",
    "    :param max_n: Maximum number of samples.\n",
    "    :param func:  The function to integrate.\n",
    "    :parma dim:   The dimension of the input.\n",
    "    \"\"\"\n",
    "    I = np.ndarray((max_n,))    # A running estimate of the expecation\n",
    "    V = np.ndarray((max_n,))    # A running estimate of the epistemic variance\n",
    "    s = 0.                      # A variable to keep track of the sum\n",
    "    s2 = 0.                     # A variable to keep track of the sum of square centered differences\n",
    "    for n in range(max_n):\n",
    "        # sample an x\n",
    "        x = sampler(dim)\n",
    "        # evaluate f\n",
    "        y = func(x)\n",
    "        # Update the sum\n",
    "        s += y\n",
    "        # The current approximation of the integral is\n",
    "        I[n] = s / (n + 1)\n",
    "        # Update s2:\n",
    "        s2 += (y - I[n]) ** 2\n",
    "        # The current approximation of the epistemic variance\n",
    "        V[n] = s2 / (n + 1)\n",
    "    return I, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try running it\n",
    "num_mc = 10   # How many times do you want to run MC\n",
    "max_n = 1000  # Maximum number of samples to take per MC run\n",
    "fig, ax = plt.subplots()\n",
    "all_ns = np.arange(1, max_n+1)\n",
    "for k in range(num_mc):\n",
    "    I, V = get_mc_estimate(max_n=max_n, func=f, dim=1)\n",
    "    ax.plot(all_ns, I, color='b', lw=1)\n",
    "    # Plot the error bar only for the first mc run:\n",
    "    if k == 0:\n",
    "        # The lower bound\n",
    "        l = I - 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "        # The upper bound\n",
    "        u = I + 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "        ax.fill_between(np.arange(1, max_n+1), l, u, color=sns.color_palette()[0], alpha=0.25)\n",
    "ax.plot(np.arange(1, max_n+1), [0.965] * max_n, color='r')\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$I_n$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Questions\n",
    "\n",
    "+ Increase ``max_n`` until you get an answer that is close enough to the correct answer (the red line). Notice how the epistemic error bars shrink around the true value.\n",
    "+ Reduce ``max_n`` back to a small number, say 1,000. Set ``num_mc`` to 10 (or higher). Observe how different MC runs envelop the correct answer. If the epistemic error bars were spot on, only 5 in 100 trajectories would be off. For which $n$ do the epistemic error bars look less accurate (small $n$ or big $n$)? Why do you think that is? Argue using the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exaple: Propagating Uncertainty Through a Differential Equation\n",
    "Consider the steady state heat equation on a heterogeneous rod with no heat sources:\n",
    "$$\n",
    "\\frac{d}{dx}\\left(c(x)\\frac{d}{dx}T(x)\\right) = 0,\n",
    "$$\n",
    "and boundary values:\n",
    "$$\n",
    "T(0) = 1\\;\\mbox{and}\\;T(1) = 0.\n",
    "$$\n",
    "We are interested in cases in which we are uncertain about the conductivity, $c(x)$.\n",
    "Before we proceed, we need to put together all our prior beliefs and come up with a stochastic model for $c(x)$ that represents our uncertainty.\n",
    "This requires assigning a probability measure on a function space (the subject of Lectures 7 to 12).\n",
    "For now, we will just give you a model.\n",
    "We will model $c = c(x;\\boldsymbol{\\xi})$ as:\n",
    "$$\n",
    "c(x;\\boldsymbol{\\xi}) = c_0(x)\\exp\\{g(x;\\boldsymbol{\\xi})\\},\n",
    "$$\n",
    "where $c_0(x)$ is a \"mean\" conductivity level and $g(x;\\boldsymbol{\\xi})$ is a random field.\n",
    "The reason for the exponential is that $c(x;\\boldsymbol{\\xi})$ must be positive.\n",
    "We will assume that the random field ia a [Wiener-Lévy process](https://en.wikipedia.org/wiki/Wiener_process).\n",
    "This is a field that it is no-where continuous and it is actually a fractal (when you zoom in the spatial dimension, the field resembles itself at a larger scale).\n",
    "The Karhunen-Loeve expansion of the field is:\n",
    "$$\n",
    "g(x;\\boldsymbol{\\xi}) = \\sum_{i=1}^\\infty\\xi_i\\phi_i(x),\n",
    "$$\n",
    "where $\\phi_i(x)$ are the eigenfunctions of $x$ and $\\xi_i$ are independent standard normal random variables with zero mean and unit variance.\n",
    "For this particular example, we will assume that:\n",
    "$$\n",
    "\\phi_i(x) = \\frac{\\sqrt{2}\\sigma}{(i - \\frac{1}{2})\\pi}\\sin\\left((i-\\frac{1}{2})\\pi x\\right),\n",
    "$$\n",
    "where $\\sigma>0$ is a parameter controlling the variance of the random field.\n",
    "For the record, this corresponds to a random field known as the \n",
    "Since, we cannot actually work with the infinite sums, let's truncate at $i=d$ and define:\n",
    "$$\n",
    "g_d(x;\\boldsymbol{\\xi}) = \\sum_{i=1}^d\\xi_i\\phi_i(x).\n",
    "$$\n",
    "Let's implement the field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This computes the random field given a xi\n",
    "def g(x, xi, sigma=1.):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x     -   One dimensional vector containing the points at which you wish to evaluate the field.\n",
    "    xi    -   One dimensional vector of the random variables. The dimensionality of this vector\n",
    "              corresponds to the ``d`` in the math above.\n",
    "    sigma -   This is the variance of the field.\n",
    "    \"\"\"\n",
    "    res = np.zeros(x.shape[0])\n",
    "    d = xi.shape[0]\n",
    "    for i in xrange(d):\n",
    "        res += xi[i] * np.sqrt(2) * sigma / (i - .5) / np.pi * np.sin((i - .5) * np.pi * x)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 500)\n",
    "d = 1000\n",
    "fig, ax = plt.subplots()\n",
    "for i in xrange(10):\n",
    "    xi = np.random.randn(d)\n",
    "    ax.plot(x, np.exp(g(x, xi)), linewidth=1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$c(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ To get an accurate description of the field, you need to include a lot of dimensions. $d=10$ is not enough. Start increasing $d$ from $10$ to $100$ to $1000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Solver for the Boundary Value Problem\n",
    "\n",
    "Let's return to our stochastic boundary value problem.\n",
    "We need to create a solver. We will develop a solver based on the [finite volume method](https://en.wikipedia.org/wiki/Finite_volume_method) using [FiPy](http://www.ctcms.nist.gov/fipy).\n",
    "You have to install the following packages:\n",
    "```\n",
    "pip install ez_setup\n",
    "pip install fipy\n",
    "```\n",
    "Here is the solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fipy\n",
    "\n",
    "class SteadyStateHeat1DSolver(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solves the 1D steady state heat equation with dirichlet boundary conditions.\n",
    "    It uses the stochastic model we developed above to define the random conductivity.\n",
    "    \n",
    "    Arguments:\n",
    "    g           -    The random field the describes the conductivity.\n",
    "    nx          -    Number of grid points\n",
    "    value_left  -    The value at the left side of the boundary.\n",
    "    value_right -    The value at the right side of the boundary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, g=g, nx=100, value_left=1., value_right=0.):\n",
    "        self.g = g\n",
    "        self.nx = nx\n",
    "        self.dx = 1. / nx\n",
    "        self.mesh = fipy.Grid1D(nx=self.nx, dx=self.dx)\n",
    "        self.phi = fipy.CellVariable(name='$T(x)$', mesh=self.mesh, value=0.)\n",
    "        self.C = fipy.FaceVariable(name='$C(x)$', mesh=self.mesh, value=1.)\n",
    "        self.phi.constrain(value_left, self.mesh.facesLeft)\n",
    "        self.phi.constrain(value_right, self.mesh.facesRight)\n",
    "        self.eq = fipy.DiffusionTerm(coeff=self.C)\n",
    "        \n",
    "    def __call__(self, xi):\n",
    "        \"\"\"\n",
    "        Evaluates the code at a specific xi.\n",
    "        \"\"\"\n",
    "        x = self.mesh.faceCenters.value.flatten()\n",
    "        g_val = self.g(x, xi)\n",
    "        self.C.setValue(np.exp(g_val))\n",
    "        self.eq.solve(var=self.phi)\n",
    "        return x, self.phi.faceValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver = SteadyStateHeat1DSolver(nx=500)\n",
    "d = 1000\n",
    "fig, ax = plt.subplots()\n",
    "for i in xrange(5):\n",
    "    xi = np.random.randn(1000)\n",
    "    x, y = solver(xi)\n",
    "    ax.plot(x, y, lw=1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$T(x,\\xi)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use Monte Carlo to estimate the mean of $T(x)$ at a single point, say $x=0.5$:\n",
    "$$\n",
    "T_m(x=0.5) = \\mathbb{E}_\\xi[T(x=0.5,\\xi)].\n",
    "$$\n",
    "We will slightly modify our previous code.\n",
    "We start by defining a function $f (\\xi) = T(x=0.5, \\xi)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_solver = lambda xi: solver(xi)[1][solver.nx / 2]    # Solve the problem at xi, grab the mid point of the solution array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to repeat our MC analysis. This is going to take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 1000      # Dimensionality of the random field\n",
    "max_n = 1000  # Maximum number of samples to take per MC run\n",
    "fig, ax = plt.subplots()\n",
    "all_ns = np.arange(1, max_n+1)\n",
    "I, V = get_mc_estimate(max_n=max_n, func=f_solver, dim=d, sampler=np.random.randn) # Notice that I changed the sampler\n",
    "ax.plot(all_ns, I, lw=1)\n",
    "# The lower bound\n",
    "l = I - 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "# The upper bound\n",
    "u = I + 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "ax.fill_between(np.arange(1, max_n+1), l, u, color=sns.color_palette()[0], alpha=0.25)\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$I_n$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something more exiting.\n",
    "Let's find upper and lower bounds for the solution.\n",
    "To achieve this, we will use the empirical percentiles of a large number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 1000 # Dimensionality of the field\n",
    "max_n = 1000 # How many samples do you want to use?\n",
    "# Array to store the data we collect\n",
    "data = np.ndarray((max_n, solver.nx + 1))\n",
    "for i in range(max_n):\n",
    "    xi = np.random.randn(d)\n",
    "    data[i, :] = solver(xi)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us find the median of $T(x,\\xi)$ for each $x$.\n",
    "The median $\\mu(x)$ is defined so that\n",
    "$$\n",
    "\\mathbb{P}_\\xi\\left[T(x,\\xi) \\le \\mu(x)\\right] = 0.5.\n",
    "$$\n",
    "We can find it by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.median(data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the $q$-th percentile is defined by:\n",
    "$$\n",
    "\\mathbb{P}_\\xi\\left[T(x,\\xi)\\le \\mu_q(x)\\right] = \\frac{q}{100}.\n",
    "$$\n",
    "Of particular interest are the $2.5\\%$ and the $97.5\\%$ quantiles:\n",
    "$$\n",
    "\\mathbb{P}_\\xi\\left[T(x,\\xi)\\le \\mu_{2.5}(x)\\right] = \\frac{2.5}{100},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbb{P}_\\xi\\left[T(x,\\xi)\\le \\mu_{97.5}(x)\\right] = \\frac{97.5}{100},\n",
    "$$\n",
    "respectively.\n",
    "These can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_025 = np.percentile(data, 2.5, axis=0)\n",
    "mu_975 = np.percentile(data, 97.5, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using them, you can define a $95\\%$ *prediction interval* for your estimate since:\n",
    "$$\n",
    "\\mathbb{P}_\\xi(\\mu_{2.5}(x) \\le T(x,\\xi) \\le \\mu_{97.5}(x)) = 0.95.\n",
    "$$\n",
    "Of course, you should keep in mind that since we are using a finite number of samples, there is residual epistemic uncertainty in these estimates.\n",
    "You should only trust them if you have convergent results.\n",
    "Let's visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, mu)\n",
    "ax.fill_between(x, mu_025, mu_975, color=sns.color_palette()[0], alpha=0.25)\n",
    "# Let's take a couple of samples and see if they fall inside:\n",
    "for _ in range(5):\n",
    "    xi = np.random.randn(d)\n",
    "    y = solver(xi)[1]\n",
    "    ax.plot(x, y, lw=1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$T(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Monte Carlo seems kind of slow. Where do you think is the bottleneck? What can be done to accelerate it?\n",
    "+ One idea is to parallelize Monte Carlo. How would you go about doing this?\n",
    "+ Another idea would be to replace the solver with a regression surface. Have we learned any techniques in class so far that can be used for this purpose? Do these techniques work when the input is high-dimensional?\n",
    "+ Modify the code above to plot a series of quantiles that will give you an idea of the underlying probability density on the space of $T(x)$'s. Plot the $5, 10, 15, 20, 25, 30, 35,\\ldots, 80, 85, 90, 95\\%$ quantiles.\n",
    "+ Use the data you have collected in ``data`` to estimate the probability that $T(x=0.5)$ is greater than $0.8$:\n",
    "$$\n",
    "\\mathbb{P}_\\xi\\left[T(x=0.5,\\xi)\\ge 0.8\\right] = \\mathbb{E}_\\xi\\left[1_{[0.8,\\infty]}(T(x=0.5,\\xi))\\right].\n",
    "$$\n",
    "Write your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(xi):\n",
    "    \"\"\"\n",
    "    Define this to be the indicator function in order to take the expectation you need.\n",
    "    \"\"\"\n",
    "    # Here is how you can get T(x=0.5, xi)):\n",
    "    y = solver(xi)[1][solver.nx / 2]\n",
    "    # Write your code below here:\n",
    "    pass\n",
    "\n",
    "max_n = 1000  # Maximum number of samples to take per MC run\n",
    "fig, ax = plt.subplots()\n",
    "all_ns = np.arange(1, max_n+1)\n",
    "I, V = get_mc_estimate(max_n=max_n, func=h, dim=d, sampler=np.random.randn) # Notice that I changed the sampler\n",
    "ax.plot(all_ns, I, lw=1)\n",
    "# The lower bound\n",
    "l = I - 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "# The upper bound\n",
    "u = I + 2. / np.sqrt(all_ns) * np.sqrt(V)\n",
    "ax.fill_between(np.arange(1, max_n+1), l, u, color=sns.color_palette()[0], alpha=0.25)\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$I_n$');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
