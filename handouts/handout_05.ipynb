{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Random Variables and How to Sample Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-random Number Generators (PRNG)\n",
    "\n",
    "PRNG's are used to generate random integers between zero and a maximum number, say $m$.\n",
    "\n",
    "### The middlesquare algorithm (Von Neumann)\n",
    "\n",
    "1. Take a number and square it.\n",
    "2. Pad the result with zeros to get to the desired number of digits.\n",
    "3. Take the middle digits of the resulting number.\n",
    "4. Repeat.\n",
    "\n",
    "Here is an implementation using strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def middlesquare(s, digits=4):\n",
    "    # Square the number\n",
    "    s2 = s ** 2\n",
    "    # Turn the resulting number into a string padding with zeros to get to the desired number of digits\n",
    "    s2_str = str(s2).zfill(2*digits)\n",
    "    # Keep only the middle\n",
    "    middle_str = s2_str[digits/2:][:-digits/2]\n",
    "    return int(middle_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "s = seed\n",
    "for _ in range(20):\n",
    "    s = middlesquare(s, digits=4)\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the middlesquare algorithms results in periodic sequences with very small period. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 540\n",
    "s = seed\n",
    "for _ in range(20):\n",
    "    s = middlesquare(s, digits=4)\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Congruential Generator (LCG)\n",
    "The linear congruential generator works as follows. You pick three big integers $a$, $b$ and $m$.\n",
    "Pick a seed $x_0$.\n",
    "Then iterate:\n",
    "$$\n",
    "x_{i+1} = (a x_i + b)\\mod m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcg(x, a=123456, b=978564, m=6012119):\n",
    "    return (a * x + b) % m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "s = seed\n",
    "for _ in range(20):\n",
    "    s = lcg(s)\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The good thing about LCG is that you can prove a lot of stuff about it using group theory and that you know that the maximum possible number is $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mersenne Twister PRNG\n",
    "Numpy uses the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) to generate random numbers.\n",
    "Its details are more complicated than LCG, but it is still initialized by an integer seed.\n",
    "You can test it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(12345)\n",
    "# print 5 integers from 0 to 6012119\n",
    "for _ in range(5):\n",
    "    print np.random.randint(0, 6012119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the seed does - Here is what happens if you rerun the code above:\n",
    "for _ in range(5):\n",
    "    print np.random.randint(0, 6012119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here is what happens if you reset the seed to its original value and rerun the code\n",
    "np.random.seed(12345)\n",
    "for _ in range(5):\n",
    "    print np.random.randint(0, 6012119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, resetting the seed gives you the same sequence. In your numerical simulations you should always set the seed by hand in order to ensure the reproducibility of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the uniform distribution\n",
    "\n",
    "If we have a PRNG that samples between zero and a big integer, say $m$, we can create a generator that samples from the uniform distribution.\n",
    "If $d$ is the sample from the PRNG, then\n",
    "$$\n",
    "x = \\frac{d}{m},\n",
    "$$\n",
    "is approximately uniformly distributed.\n",
    "Let's experiment with this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum integer\n",
    "m = 6012119\n",
    "\n",
    "# First a uniform random generator based on lcg\n",
    "lcg_seed = 123456 # A seed of lcg\n",
    "lcg_state = lcg_seed # Internal state of lcg\n",
    "def unif_lcg():\n",
    "    global lcg_state\n",
    "    lcg_state = lcg(lcg_state)\n",
    "    return lcg_state / (1. * m) # The 1. in the denominator ensures\n",
    "                                # that the division is done in floating point arithmetic\n",
    "print 'LCG Uniform Samples:'\n",
    "for _ in range(5):\n",
    "    print unif_lcg()\n",
    "\n",
    "# And let's also do it with Mersenne Twister from numpy\n",
    "np.random.seed(123456)\n",
    "def unif_mt():\n",
    "    return np.random.randint(0, m) / (1. * m)\n",
    "print '\\nMT Uniform Samples:'\n",
    "for _ in range(5):\n",
    "    print unif_mt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one of the two is better? There are many statistical tests that we would like our uniform random number generator to go through. First (and most importantly) the empirical histograms of the generated numbers should be uniform. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many numbers to sample:\n",
    "N = 100\n",
    "lcg_X = [unif_lcg() for _ in range(N)]\n",
    "mt_X = [unif_mt() for _ in range(N)]\n",
    "# Plot the histograms\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(lcg_X, normed=True, alpha=0.5, label='LGC_unif')\n",
    "ax.hist(mt_X, normed=True, alpha=0.5, label='MT_unif')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 01\n",
    "+ Hmm, we probably need to increase the number of samples to observe this statistic better. Increase $N$ from 100 to $1,000$ and then to $10,000$. How do the distributions look like now?\n",
    "\n",
    "+ A second thing that we would like to test is whether or not consecutive numbers are all independent (Idependent identically distributed). Unfortunately, we need more theory than we know to do this.\n",
    "\n",
    "+ For future reference, note that you should not really use ``unif_mt`` to generate uniform random numbers. Numpy already implements this in ``numpy.random.rand``. We provide an example right below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random numbers with numpy's unif_mt:\n",
    "X = np.random.rand(10)\n",
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bernoulli Distribution\n",
    "The Bernoulli distribution arises from a binary random variable representing the outcome of an experiment with a given probability of success.\n",
    "Let us encode success with 1 and failure with 0.\n",
    "Then, we say that the random variable\n",
    "$$\n",
    "X\\sim\\mathcal{B}(\\theta),\n",
    "$$\n",
    "is a Bernoulli random variable with parameter $\\theta$ if:\n",
    "$$\n",
    "X = \\begin{cases}\n",
    "1,\\;\\text{with probability}\\;\\theta,\\\\\n",
    "0,\\;\\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "Another way to write the same thing is through the probability density function of $X$:\n",
    "$$\n",
    "p(x) = \\theta \\delta(x-1) + (1-\\theta)\\delta(x),\n",
    "$$\n",
    "where we used Dirac's delta to talk about point masses.\n",
    "To sample from it, we do the following steps:\n",
    "\n",
    "+ Sample a uniform number $u$ (i.e., a number of $\\mathcal{U}([0,1])$).\n",
    "\n",
    "+ If $u\\le \\theta$, then set $x = 1$.\n",
    "\n",
    "+ Otherwise, set $x = 0$.\n",
    "\n",
    "Let's see if this process does indeed produce the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_bernoulli(theta):\n",
    "    u = np.random.rand()\n",
    "    if u <= theta:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "for _ in range(10):\n",
    "    print sample_bernoulli(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a histogram like before\n",
    "N = 1000\n",
    "X = [sample_bernoulli(0.3) for _ in range(N)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X, alpha=0.5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it looks fine. About $\\theta N$ samples went to 1 and $(1-\\theta)N$ samples went to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sampling Discrete Distributions\n",
    "Consider a generic discrete random variable $X$ taking $m$ different values.\n",
    "Without loss of generality, you may assume that these values are integers $\\{0, 1,2,\\dots,m-1\\}$ (they are just the labels of the discrete objects anyway).\n",
    "Let us assume that\n",
    "$$\n",
    "p(X=k) = p_k,\n",
    "$$\n",
    "where, of course, we must have:\n",
    "$$\n",
    "p_k \\ge 0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum_{k=0}^{m-1} p_k = 1.\n",
    "$$\n",
    "Remember, that an succint way to write this is using the Dirac delta:\n",
    "$$\n",
    "p(x) = \\sum_{k=0}^{m-1}p_k\\delta(x-k).\n",
    "$$\n",
    "In any case, here is how you sample from such a distribution:\n",
    "\n",
    "+ Draw a uniform sample $u$.\n",
    "+ Find the index $j\\in\\{0,1,\\dots,m-1\\}$ such that:\n",
    "$$\n",
    "\\sum_{k=0}^{j-1}p_k \\le u < \\sum_{k=0}^j.\n",
    "$$\n",
    "+ Then, your sample is $j$.\n",
    "\n",
    "Let's code it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_discrete(p):\n",
    "    \"\"\"\n",
    "    Sample from a discrete probability density.\n",
    "    \n",
    "    :param p: An array specifying the probability of each possible state.\n",
    "              The number of states ``m=len(p)``.\n",
    "    :returns: A random integer.\n",
    "    \n",
    "    (btw this is how you document a python function)\n",
    "    \"\"\"\n",
    "    m = len(p)\n",
    "    u = np.random.rand()\n",
    "    c = 0.\n",
    "    for j in range(m):\n",
    "        c += p[j]\n",
    "        if u <= c:\n",
    "            return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it with a four-state discrete random variable with probabilities\n",
    "p = [0.2, 0.3, 0.4, 0.1]\n",
    "# Let's take 1,000 samples\n",
    "N = 1000\n",
    "X = [sample_discrete(p) for _ in range(N)]\n",
    "# and do the empirical histrogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X, alpha=0.5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, numpy already implements this functionality. Here is how to do the same thing numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_np = np.random.choice(np.arange(4), # The objects that you want to sample (here integers, 0,1,2,3)\n",
    "                        p=p,          # The probability of sampling each object\n",
    "                        size=N        # How many samples you want  \n",
    "                       )\n",
    "# Let's compare the two histograms\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X, alpha=0.5, label='Our implementation')\n",
    "ax.hist(X_np, alpha=0.5, label='Numpy implementation')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Binomial Distribution\n",
    "\n",
    "The Binomial distribution gives you the number of successes in $N$ tries of a random experiment with probability of success $\\theta$.\n",
    "We write:\n",
    "$$\n",
    "X\\sim \\mathcal{B}(N,\\theta).\n",
    "$$\n",
    "You can easily simulate it (excersize) by noticing that:\n",
    "$$\n",
    "X = \\sum_{i=1}^N X_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "X_i \\sim \\mathcal{B}(\\theta),\n",
    "$$\n",
    "are indepdent Bernoulli trials.\n",
    "We can also show that:\n",
    "$$\n",
    "p(X=k) = \\left(\\begin{array}{c}N\\\\ k\\end{array}\\right)\\theta^k(1-\\theta)^{N-k}.\n",
    "$$\n",
    "Let's plot this distribution for various $N$'s.\n",
    "We will use the built-in ``scipy.stats`` functionality for this one.\n",
    "For your future reference, you can find it [here](https://docs.scipy.org/doc/scipy/reference/stats.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def plot_binom_pdf(N, theta):\n",
    "    k = np.arange(N) + 1. # From 1 to N\n",
    "    p_k = st.binom(N, theta).pmf(k) # pmf is short for probability mass function\n",
    "                                    # which is the right terminology for a discrete variable\n",
    "                                    # (i.e., we use 'mass' instead of 'density')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(k, p_k, 'o', color='b')\n",
    "    ax.vlines(k, 0, p_k, colors='b', lw=5, alpha=0.5)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "    ax.set_title(r'$\\mathcal{B}(N=%d, \\theta=%.2f)$' % (N, theta)) # the 'r' is required to render\n",
    "                                                                   # '\\' character correctly\n",
    "    \n",
    "plot_binom_pdf(4, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's play with $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binom_pdf(10, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 02\n",
    "+ Start increasing $N$. Try really big numbers. Does the result remind you a familiar distribution?\n",
    "\n",
    "+ Play a little bit with $\\theta$. What happes as you move it around?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Sampling\n",
    "How do you sample an arbitrary univariate continuous random variable $X$ with CDF $F(x)$.\n",
    "In this scenario, *inverse sampling* is the way to go.\n",
    "It relies on the observation that the random variable\n",
    "$$\n",
    "Y = F^{-1}(U),\n",
    "$$\n",
    "where $F^{-1}$ is the inverse of the CDF of $X$ and $U\\sim\\mathcal{U}([0,1])$ has exactly the same distribution as $X$.\n",
    "\n",
    "We will demonstrate this by exmaple. To this end, let us consider an exponential random variable:\n",
    "$$\n",
    "T \\sim \\mathcal{r},\n",
    "$$\n",
    "where $r > 0$ is known as the *rate parameter*.\n",
    "The exponential distribution describes the time it passes between random events that occur at a constnat rate $r$.\n",
    "Its CDF is:\n",
    "$$\n",
    "p(t) = re^{-rt},\n",
    "$$\n",
    "and its CFG is:\n",
    "$$\n",
    "F(t) = p(T\\le t) = 1 - e^{-rt}.\n",
    "$$\n",
    "We plot it next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = .5 # Events occur every 0.5 minutes\n",
    "fig, ax = plt.subplots()\n",
    "t = np.linspace(0., 5. / r, 100)\n",
    "ax.plot(t, st.expon(scale=1./r).cdf(t))\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel(r'$F(t) = p(T <= t)$')\n",
    "ax.set_title(r'$T\\sim\\mathcal{E}(r=%.2f)$' % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample $T$ using inverse sampling, we need the inverse of the CDF. This is easily shown to be:\n",
    "$$\n",
    "F^{-1}(u) = -\\frac{\\ln(1-u)}{r}.\n",
    "$$\n",
    "Let's see if this is going to give us the right samples.\n",
    "We will compare the empirical histogram obtained by inverse sampling to the actual PDF $p(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_exp(r):\n",
    "    u = np.random.rand()\n",
    "    return -np.log(1. - u) / r\n",
    "\n",
    "N = 10000\n",
    "T = [sample_exp(r) for _ in range(N)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(T, alpha=0.5, normed=True, bins=100, label='Histogram of samples')\n",
    "ax.plot(t, st.expon(scale=1./r).pdf(t))\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$p(t)')\n",
    "ax.set_title(r'$T\\sim\\mathcal{E}(r=%.2f)$' % r)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 03\n",
    "\n",
    "+ Implement inverse sampling for a univariate Gaussian with zero mean and unit variance. Use ``scipy.stats`` to find the inverse CDF of the Gaussian (It is ``st.norm.ippf``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "Consider, $X_1,X_2,\\dots$ be iid random variables with mean $\\mu$ and variance $\\sigma^2$.\n",
    "Define their sum:\n",
    "$$\n",
    "S_N = \\frac{X_1+\\dots+X_N}{N}.\n",
    "$$\n",
    "The Central Limit Theorem (CLT), states that:\n",
    "$$\n",
    "S_N \\sim \\mathcal{N}(S_N|\\mu, \\frac{\\sigma^2}{N}),\n",
    "$$\n",
    "for large $N$.\n",
    "That is, they start to look like Gaussian.\n",
    "Let's test it for the Exponential distribution.\n",
    "We will use ``numpy.random.exponential`` to sample from the exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.5\n",
    "N = 5   # How many iid variables are we going to sum\n",
    "M = 10000 # How many times do you want to sample\n",
    "Ts = np.random.exponential(scale=1./r, size=(N, M))  # Notice that it uses the inverse of the rate.\n",
    "                                                     # It is always a good idea to look at the documentation\n",
    "                                                     # if you are unsure.\n",
    "# These are the samples of SN:\n",
    "SN = np.sum(Ts, axis=0) / N  # Notice that I am only summing the rows\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(SN, bins=100, normed=True, alpha=0.5, label='Empirical histogram of $S_N$')\n",
    "mu_CLT = 1. / r             # CLT mean\n",
    "sigma_CLT = np.sqrt(1. / (N * r**2)) # CLT standard deviation\n",
    "Ss = np.linspace(SN.min(), SN.max(), 100)\n",
    "ax.plot(Ss, st.norm(loc=mu_CLT, scale=sigma_CLT).pdf(Ss), label='CLT Gaussian')\n",
    "ax.set_xlabel('$S_N$')\n",
    "ax.set_ylabel('$p(S_N)$')\n",
    "ax.set_title('CLT: Exponential by Gaussian (N=%d)' % N)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 04\n",
    "\n",
    "+ Start increase $N$ and observe the convergence.\n",
    "+ Go back to the Bernoulli distribution. What are its mean and variance? What is the mean and the variance of the Gaussian approximating the sum of idenpdent Bernoulli distributions? Verify this result numerically (copy paste the code above and make the appropriate changes)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
