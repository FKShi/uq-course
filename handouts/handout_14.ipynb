{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 - Uncertainty Propagation (Sampling Methods II)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ to use variance reduction techniques to accelerate MC\n",
    "+ to use importance sampling to accelerate MC\n",
    "+ to use latin hyper-cube designs to accelerate MC in moderate dimensions\n",
    "+ to propagate uncertainty through an initial value problem\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ Monte Carlo Strategies in Scientific Computing (Jun S. Liu, 2001): Chapters 1 and 2.\n",
    "\n",
    "+ Monte Carlo Statistical Methods (Christian P. Robert and George Casella, 1999): Chapter 3.\n",
    "\n",
    "+ McKay, M.D.; Beckman, R.J.; Conover, W.J. (May 1979). A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code. Technometrics (JSTOR Abstract) (American Statistical Association) 21 (2): 239â€“245. doi:10.2307/1268522. ISSN 0040-1706. JSTOR 1268522. OSTI 5236110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling\n",
    "Let $X$ be a random variable with probability density $p(x)$ and $f(x)$ be a function.\n",
    "As in the lecture 13, we wish to estimate the expectation:\n",
    "$$\n",
    "I = \\mathbb{E}_p[f(X)]=\\int f(x)p(x)dx.\n",
    "$$\n",
    "Note that we have explicitly marked the expectation operator with $p$.\n",
    "This is because in a while we will change $p$ to something else.\n",
    "\n",
    "The idea in *importance sampling* is to re-write $I$ as an expectation with respect to another probability density, say $q(x)$.\n",
    "The only requirement is that the support of $q(x)$ must be broader than the support of $p(x)$ ($q(x)$ must be positive wherever $p(x)$ is positive).\n",
    "Here we go:\n",
    "$$\n",
    "I = \\mathbb{E}_p[f(X)] = \\int f(x)p(x)dx = \\int \\frac{f(x)p(x)}{q(x)}q(x)dx = \\mathbb{E}_q\\left[\\frac{f(x)p(x)}{q(x)}\\right].\n",
    "$$\n",
    "This suggests that you can estimate $I$ by sampling from $q(x)$ instead of $p(x)$.\n",
    "A trivial estimator is:\n",
    "$$\n",
    "I_{n} = \\frac{1}{n}\\sum_{j=1}^n\\frac{f(X_j)p(X_j)}{q(X_j)}, \n",
    "$$\n",
    "where $X_1,\\dots,X_n\\sim q$ iid.\n",
    "\n",
    "Why on earth is this useful?\n",
    "\n",
    "+ First, it may be much easier to sample from $q(x)$ than it is to sample from $p(x)$.\n",
    "+ You can reuse samples from $q(x)$ with different $f(x)$ and different $p(x)$.\n",
    "+ With the right choice of $q(x)$ you can actually reduce the variance of the MC estimator.\n",
    "\n",
    "Let's see how it works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Small Tail Probabilities (Casella 3.11)\n",
    "Let $X\\sim \\mathcal{N}(0,1)$ and assume that we want to compute:\n",
    "$$\n",
    "I = \\mathbb{P}[X > 4.5].\n",
    "$$\n",
    "The standard MC estimator is:\n",
    "$$\n",
    "I_n = \\frac{1}{n}\\sum_{j=1}^n1_{[4.5,+\\infty]}(X_j),\n",
    "$$\n",
    "for $X_j\\sim \\mathcal{N}(0,1)$ iid.\n",
    "Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_mc = 10000\n",
    "s = 0.\n",
    "s2 = 0.\n",
    "for i in range(num_mc):\n",
    "    x = np.random.randn()\n",
    "    if x > 4.5:\n",
    "        s += 1.\n",
    "        s2 += 1.\n",
    "I_n = s / num_mc\n",
    "V_n = (s2 / num_mc - I_n ** 2) / num_mc\n",
    "print('I_n = %1.2e +- %1.2e' % (I_n, 2 * np.sqrt(V_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look right for $n=10,000$. We did not count a single event. You would have to go to $n=1,000,000$ to hit one or two events.\n",
    "To get an accurate answer you probably need hundrends of millions of samples.\n",
    "\n",
    "Let's see if we can remedy this situtation with importance sampling.\n",
    "Ideally, we only want to sample $X$'s that are greater than $4.5$.\n",
    "Let us introduce a random variable $Y\\sim \\mathcal{TE}(4.5, 1)$ following an exponential distribution with rate parameter $1$ which is truncated at $4.5$.\n",
    "The density of $Y$ is:\n",
    "$$\n",
    "q(y) = \\frac{e^{-y}}{c}1_{[4.5,+\\infty)}(y),\n",
    "$$\n",
    "where $c$ is a normalization constant.\n",
    "To find the normalization constant we impose:\n",
    "$$\n",
    "\\int q(y) dy = 1 \\Rightarrow c = \\int_0^{+\\infty}e^{-y}1_{[4.5,+\\infty)}(y)dy = \\int_{4.5}^{+\\infty}e^{-y}dy = \\left[-e^{-y}\\right]_{4.5}^{+\\infty}=e^{-4.5}.\n",
    "$$\n",
    "So, the density becomes:\n",
    "$$\n",
    "q(y) = e^{-(y-4.5)}1_{[4.5,+\\infty)}(y).\n",
    "$$\n",
    "Let's visualize it comparing it to the standard normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-4., 10., 200)\n",
    "p = st.norm.pdf(x)\n",
    "q = np.exp(-(x-4.5))\n",
    "q[x < 4.5] = 0.\n",
    "ax.plot(x, p, label='$p(x)$')\n",
    "ax.plot(x, q, label='$q(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('Probability density')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance sampling estimator is:\n",
    "$$\n",
    "I_{2,n} = \\frac{1}{n}\\sum_{j=1}^n\\frac{f(X_j)p(X_j)}{q(X_j)}=\\frac{1}{n\\sqrt{2\\pi}}\\sum_{j=1}^ne^{-\\frac{1}{2}X_j^2+X_j-4.5},\n",
    "$$\n",
    "for $X_1,\\dots,X_n\\sim q$ iid.\n",
    "Let's see what we are going to get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_mc = 10000\n",
    "s = 0.\n",
    "s2 = 0.\n",
    "for i in range(num_mc):\n",
    "    x = np.random.exponential() + 4.5 # Just add 4.5 to a standard exponential\n",
    "    f = np.exp(-0.5 * x ** 2 + x - 4.5) / np.sqrt(2. * np.pi)\n",
    "    s += f\n",
    "    s2 += f ** 2\n",
    "I_n = s / num_mc\n",
    "V_n = (s2 / num_mc - I_n ** 2) / num_mc\n",
    "print('I_n = %1.4e +- %1.4e' % (I_n, 2*np.sqrt(V_n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite variance concerns\n",
    "The variance of an importance sampling estimator is:\n",
    "$$\n",
    "V_n = \\mathbb{V}[I_n] = \\frac{1}{n}\\mathbb{V}_q\\left[\\frac{f(X_j)p(X_j)}{q(X_j)}\\right]=\\frac{1}{n}\\left(\\mathbb{E}_q\\left[\\frac{f^2(X_j)p^2(X_j)}{q^2(X_j)}\\right]-\\left(\\mathbb{E}_q\\left[\\frac{f(X_j)p(X_j)}{q(X_j)}\\right]\\right)^2\\right).\n",
    "$$\n",
    "So, in order to have a finite variance estimator, we must have that:\n",
    "$$\n",
    "\\mathbb{E}_q\\left[\\frac{f^2(X_j)p^2(X_j)}{q^2(X_j)}\\right] = \\int f^2(x)\\frac{p^2(x)}{q(x)}dx < \\infty.\n",
    "$$\n",
    "Unless, this condition is satisfied, you will get garbage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Infinite Variance Estimator.\n",
    "It is absolutely crucial that the variance of your estimator is finite.\n",
    "Otherwise, you cannot trust it even if it theoretically converges.\n",
    "Let us demonstrate this by constructing a really stupid importance sampling desnity.\n",
    "Let $X\\sim\\mathcal{E}(1)$ (exponential with unit rate) so that the density is:\n",
    "$$\n",
    "p(x) = e^{-x}_{[0,\\infty)}(x).\n",
    "$$\n",
    "Pick $f(x)=x$ so that we want to compute the mean $\\mathbb{E}[f(X)] = \\mathbb{E}_p[X]=1$ (analytically known).\n",
    "As importance sampling density let's use the density of a $\\mathcal{N}(0,1)$:\n",
    "$$\n",
    "q(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}.\n",
    "$$\n",
    "Let's look at that integral that dominates the variance:\n",
    "$$\n",
    "\\int f^2(x)\\frac{p^2(x)}{q(x)}dx = \\int_0^\\infty x^2 \\frac{e^{-2x}}{\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}}dx = \\infty.\n",
    "$$\n",
    "Let's implement an MC estimate using this importance sampling density and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many times to try\n",
    "num_tries = 10\n",
    "# How many MC steps to take per try \n",
    "num_mc = 1000\n",
    "# For plotting purposes\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(num_tries):\n",
    "    I = np.ndarray((num_mc, ))\n",
    "    s = 0.\n",
    "    for j in range(num_mc):\n",
    "        x = np.random.randn()\n",
    "        if x >= 0.:\n",
    "            y = x * np.exp(-x) / st.norm.pdf(x)\n",
    "            s += y\n",
    "        I[j] = s / (j + 1)\n",
    "    plt.plot(np.arange(1, num_mc + 1), I, color=sns.color_palette()[0], lw=1)\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$I_n$')\n",
    "plt.plot(np.arange(1, num_mc + 1), np.ones((num_mc, )), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Do a single trial for a large number of samples. Does it converge? Remember, that in reality you never know the truth (red line). If you think that you have found a number of samples that guarantees convergence, run it once more. Does it converge now?\n",
    "\n",
    "+ Increase the number of tries to 10, 100, 1000 (decrease the number of samples per trial if it becomes to slow). What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Best Importance Sampling Distribution\n",
    "So, the choice of $q(x)$ is completely up to us.\n",
    "What if we picked it so that we minimize the variance of the estimator?\n",
    "That is, we would like to find the $q^*(x)$ that solves minimizes:\n",
    "$$\n",
    "\\min_{q}\\int f^2(x)\\frac{p^2(x)}{q(x)}dx.\n",
    "$$\n",
    "We will do it by taking the derivative of the functional:\n",
    "$$\n",
    "J[q] = \\int f^2(x)\\frac{p^2(x)}{q(x)}dx,\n",
    "$$\n",
    "with respect to an arbitrary direction and setting it equal to zero.\n",
    "To this end, let $\\eta(x)$ be any non-negative function such that\n",
    "$$\n",
    "\\int \\eta(x)dx = 0.\n",
    "$$\n",
    "Then, for any $\\epsilon>0$, we have that the pertubation $q(x) + \\epsilon\\eta(x)$ is a valid probability density since:\n",
    "$$\n",
    "\\int (q(x) + \\epsilon\\eta(x)) dx = \\int q(x) dx + \\epsilon\\int\\eta(x)dx = 1.\n",
    "$$\n",
    "The first variation of $J[q]$ in the direction of $\\eta(x)$ is:\n",
    "$$\n",
    "\\begin{array}[ccc]\n",
    "\\delta J[q]/\\delta \\eta &=& \\lim_{\\epsilon\\rightarrow 0}\\frac{J[q + \\epsilon\\eta]-J[q]}{\\epsilon}\\\\\n",
    "&=& \\lim_{\\epsilon\\rightarrow 0}\\frac{\\int \\left[f^2(x)\\frac{p^2(x)}{q(x)+\\epsilon\\eta(x)}-f^2(x)\\frac{p^2(x)}{q(x)}\\right]dx}{\\epsilon}\\\\\n",
    "&=& \\lim_{\\epsilon\\rightarrow 0}\\frac{\\int f^2(x)p^2(x)\\left[\\frac{1}{q(x)+\\epsilon\\eta(x)}-\\frac{1}{q(x)}\\right]dx}{\\epsilon}\\\\\n",
    "&=& \\lim_{\\epsilon\\rightarrow 0}\\frac{\\int f^2(x)p^2(x)\\left[\\frac{q(x)-q(x)-\\epsilon\\eta(x)}{\\left(q(x)+\\epsilon\\eta(x)\\right)q(x)}\\right]dx}{\\epsilon}\\\\\n",
    "&=&-\\lim_{\\epsilon\\rightarrow 0}\\int f^2(x)p^2(x)\\left[\\frac{\\eta(x)}{\\left(q(x)+\\epsilon\\eta(x)\\right)q(x)}\\right]dx\\\\\n",
    "&=&-\\int f^2(x)p^2(x)\\left[\\frac{\\eta(x)}{q^2(x)}\\right]dx.\n",
    "\\end{array}\n",
    "$$\n",
    "For an extremum, the first variation must be equal to zero (necessary condition) for all possible $\\eta$'s.\n",
    "We see that this is trivially satisfied for:\n",
    "$$\n",
    "q^*(x) \\propto |f(x)|p(x).\n",
    "$$\n",
    "To show that the above defined $q^*$ is a minimum, we need to take the second variation of $J[q]$ and show that it is positive for all $\\eta$'s.\n",
    "We leave this as an excersise.\n",
    "\n",
    "### Questions\n",
    "\n",
    "+ Is it really possible to use the minimum variance importance sampling density? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Latin Hyper-Cube Designs\n",
    "\n",
    "Latin hyper-cube designs (LHS) are quasi-random sequences that resemble uniform random nummbers, but have better convergence properties than truly random numbers.\n",
    "\n",
    "+ A **latin square** is a square grid containing samples only one sample in each row and each column.\n",
    "+ A **latin hypercube** is the generalisation of this concept to many dimensions.\n",
    "\n",
    "Here is how they look like in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import design # Library the implements latin hyper-cube designs you must install it: pip install py-design\n",
    "fig, ax = plt.subplots()\n",
    "ax.grid(which='major')\n",
    "for i in range(1):\n",
    "    X = design.latin_center(5, 2)\n",
    "    ax.scatter(X[:,0], X[:,1], 50., color=sns.color_palette()[i])\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to uniform samples in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "fig, ax = plt.subplots()\n",
    "X_lhs = design.latin_center(num_samples, 2)\n",
    "ax.scatter(X_lhs[:,0], X_lhs[:,1], 50., color=sns.color_palette()[0])\n",
    "X_lhs = design.latin_center(num_samples, 2)\n",
    "ax.grid(which='major')\n",
    "ax.scatter(X_lhs[:,0], X_lhs[:,1], 50., color=sns.color_palette()[2])\n",
    "ax.set_title('Latin hypercube samples')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');\n",
    "fig2, ax2 = plt.subplots()\n",
    "X_unif = np.random.rand(num_samples, 2)\n",
    "ax2.grid(which='major')\n",
    "ax2.scatter(X_unif[:,0], X_unif[:,1], 50., color=sns.color_palette()[1])\n",
    "X_unif = np.random.rand(num_samples, 2)\n",
    "ax2.scatter(X_unif[:,0], X_unif[:,1], 50., color=sns.color_palette()[3])\n",
    "ax2.set_title('Uniform samples')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform Sampling vs Latin Hyper-cubes\n",
    "Consider the ODE:\n",
    "  \\begin{align*}\n",
    "    \\dot{y} = \\frac{d y(t)}{dt} & =-ay(t),\\qquad y(0) = y_0.\n",
    "  \\end{align*}\n",
    "The solution is: $y(t) = Ie^{-at}$.\n",
    "Let's make $a$ and $I$ random and use MC and LHS to find the mean, $E[y(t)]$, and the variance $V[y(t)]$.\n",
    "We'll take:\n",
    "$$\n",
    "a \\sim \\mathcal{U}(0, 0.1),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "y_0 \\sim \\mathcal{U}(8, 10).\n",
    "$$\n",
    "For convenience, let's map these random variables to standardized uniform random variables:\n",
    "$$\n",
    "x_i \\sim \\mathcal{U}(0, 1), i=1,2.\n",
    "$$\n",
    "This can be done by defining:\n",
    "$$\n",
    "a = 0.1 x_1.\n",
    "$$\n",
    "Using the new random variables, the ODE can be written as:\n",
    "$$\n",
    "\\dot{y} = -0.1 x_1 y, y(0) = 8 + 2x_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Solver\" object\n",
    "Let's develop a solver for this problem. We'll make the solver work as a nice function that accepts a vector $\\mathbf{x} = (x_1, x_2)$ as an input, and returns the solution of the ODE on a finite set of timesteps $0=t_1<t_2<\\dots<t_{n_t}=T$. The solver, will make use of the functionality of [scipy.integrate.odeint](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.integrate.odeint.html). This is a trivial problem, but studying its structure will teach you a few useful Python tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "\n",
    "class Ex1Solver(object):\n",
    "    \"\"\"\n",
    "    An object that can solver the afforementioned ODE problem.\n",
    "    It will work just like a multivariate function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nt=100, T=5):\n",
    "        \"\"\"\n",
    "        This is the initializer of the class.\n",
    "        \n",
    "        Arguments:\n",
    "            nt - The number of timesteps.\n",
    "            T  - The final time.\n",
    "        \"\"\"\n",
    "        self.nt = nt\n",
    "        self.T = T\n",
    "        self.t = np.linspace(0, T, nt) # The timesteps on which we will get the solution\n",
    "        # The following are not essential, but they are convenient\n",
    "        self.num_input = 2             # The number of inputs the class accepts\n",
    "        self.num_output = nt           # The number of outputs the class returns\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        This special class method emulates a function call.\n",
    "        \n",
    "        Arguments:\n",
    "            x - A 1D numpy array with 2 elements. This represents the stochastic input x = (x1, x2).\n",
    "        \"\"\"\n",
    "        def rhs(y, t, x1):\n",
    "            \"\"\"\n",
    "            This is the right hand side of the ODE.\n",
    "            \"\"\"\n",
    "            return -.1 * x1 * y\n",
    "        # The initial condition\n",
    "        y0 = [8 + 2 * x[1]]\n",
    "        # We are ready to solve the ODE\n",
    "        y = scipy.integrate.odeint(rhs, y0, self.t, args=(x[0],)).flatten()\n",
    "        # The only strange thing here is the use of ``args`` to pass they x1 argument to the rhs().\n",
    "        # That's it\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the first time we see a Python class, let's play with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is how you can initialize it:\n",
    "solver = Ex1Solver()\n",
    "# You can access its arguments:\n",
    "print \"Number of timesteps:\", solver.nt\n",
    "print \"Final time:\", solver.T\n",
    "print \"Timesteps:\", solver.t\n",
    "print \"Num inputs:\", solver.num_input\n",
    "print \"Num outputs:\", solver.num_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you wish, you may intiialize it with a different initial arguments:\n",
    "solver = Ex1Solver(nt=200, T=50)\n",
    "print \"Number of timesteps:\", solver.nt\n",
    "print \"Final time:\", solver.T\n",
    "print \"Timesteps:\", solver.t\n",
    "print \"Num inputs:\", solver.num_input\n",
    "print \"Num outputs:\", solver.num_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's evaluate the solver at a specific input.\n",
    "# You can just use it as a function\n",
    "x = [0.5, 0.5]\n",
    "y = solver(x)\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot it:\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(solver.t, y)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's just plot a few random samples.\n",
    "fig, ax = plt.subplots()\n",
    "for i in xrange(10):\n",
    "    x = np.random.rand(2)\n",
    "    y = solver(x)\n",
    "    plt.plot(solver.t, y)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagating Uncertainties with MC\n",
    "\n",
    "Let's propagate the uncertainties through the ODE using MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now do Monte Carlo to compute the mean and the variance\n",
    "# This is to accumulate the sum of all outputs\n",
    "y_mc = np.zeros(solver.num_output)\n",
    "# This is to accumlate the square of all outputs\n",
    "y2_mc = np.zeros(solver.num_output)\n",
    "# Pick the number of samples you wish to do:\n",
    "num_samples = 10000\n",
    "# Let's do it\n",
    "data_mc = []\n",
    "for i in xrange(num_samples):\n",
    "    if i % 1000 == 0:\n",
    "        print 'sample', i + 1, 'from', num_samples\n",
    "    x = np.random.rand(2)\n",
    "    y = solver(x)\n",
    "    y_mc += y\n",
    "    y2_mc += y ** 2\n",
    "    data_mc.append(y)\n",
    "data_mc = np.array(data_mc)\n",
    "# Now we are ready for the mean estimate:\n",
    "y_m_mc = y_mc / num_samples\n",
    "# And the variance estimate\n",
    "y_v_mc = y2_mc / num_samples - y_m_mc ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the mean\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(solver.t, y_m_mc)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$\\mathbb{E}[y(t)]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the variance\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(solver.t, y_v_mc)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$\\mathbb{V}[y(t)]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's draw the predictive envelop\n",
    "# We need the standard deviation:\n",
    "y_s_mc = np.sqrt(y_v_mc)\n",
    "# A lower bound for the prediction\n",
    "y_l_mc = np.percentile(data_mc, 2.75, axis=0)\n",
    "# An upper bound for the prediction\n",
    "y_u_mc = np.percentile(data_mc, 97.5, axis=0)\n",
    "# And let's plot it:\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(solver.t, y_m_mc)\n",
    "ax.fill_between(solver.t, y_l_mc, y_u_mc, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagating Uncertainties with LHS\n",
    "\n",
    "Let's propagate the uncertainties through the ODE using LHS.\n",
    "All we have to do is to copy and paste the above code and make some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now do LHS to compute the mean and the variance\n",
    "# This is to accumulate the sum of all outputs\n",
    "y_lhs = np.zeros(solver.num_output)\n",
    "# This is to accumlate the square of all outputs\n",
    "y2_lhs = np.zeros(solver.num_output)\n",
    "# Pick the number of samples you wish to do:\n",
    "num_samples = 10000\n",
    "# You have to create the LHS design prior to looping:\n",
    "X = design.latin_center(num_samples, 2)\n",
    "# Let's do it\n",
    "data_lhs = []\n",
    "for i in xrange(num_samples):\n",
    "    if i % 1000 == 0:\n",
    "        print 'sample', i + 1, 'from', num_samples\n",
    "    x = X[i, :]\n",
    "    y = solver(x)\n",
    "    y_lhs += y\n",
    "    y2_lhs += y ** 2\n",
    "    data_lhs.append(y)\n",
    "data_lhs = np.array(data_lhs)\n",
    "# Now we are ready for the mean estimate:\n",
    "y_m_lhs = y_lhs / num_samples\n",
    "# And the variance estimate\n",
    "y_v_lhs = y2_lhs / num_samples - y_m_lhs ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the mean\n",
    "plt.plot(solver.t, y_m_lhs)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$\\mathbb{E}[y(t)]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the variance\n",
    "plt.plot(solver.t, y_v_lhs)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$\\mathbb{V}[y(t)]$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's draw the predictive envelop\n",
    "# We need the standard deviation:\n",
    "y_s_lhs = np.sqrt(y_v_lhs)\n",
    "y_l_lhs = np.percentile(data_lhs, 2.75, axis=0)\n",
    "# An upper bound for the prediction\n",
    "y_u_lhs = np.percentile(data_lhs, 97.5, axis=0)\n",
    "# And let's plot it:\n",
    "plt.plot(solver.t, y_m_lhs)\n",
    "plt.fill_between(solver.t, y_l_lhs, y_u_lhs, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing MC to LHS\n",
    "\n",
    "So, which one is better? MC or LHS? To test this, we need to establish a ground truth.\n",
    "Fortunately, we can get an analytic solution to:\n",
    "$$\n",
    "\\dot{y} = -0.1 x_1 y, y(0) = 8 + 2x_2.\n",
    "$$\n",
    "It is:\n",
    "$$\n",
    "y(t;x_1,x_2) = (8 + 2x_2)e^{-0.1 x_1 t}.\n",
    "$$\n",
    "We can integrate $x_1$ and $x_2$ analytically from this expression.\n",
    "\n",
    "Doing so, we get the following mean solution and variance:\n",
    "$$\n",
    "\\mu = \\mathbb{E}[y] = \\frac{90}{t} (1 - \\exp^{-0.1t}), \\\\\n",
    "\\mathrm{S} = \\mathbb{V}[y] =  \\frac{2440}{6t}(1 - \\exp^{-0.2t}) - \\mu^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E_y(t):\n",
    "    if t == 0:\n",
    "        return 9.\n",
    "    return (90*(1-np.exp(-0.1*t))) / t\n",
    "E_y =  np.vectorize(E_y)\n",
    "\n",
    "def V_y(t):\n",
    "    if t == 0:\n",
    "        return (2440*0.2)/6. - E_y(t) **2\n",
    "    \n",
    "    return (2440.*(1-np.exp(-0.2*t)))/(6.*t) - E_y(t)**2\n",
    "V_y = np.vectorize(V_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start by doing some plots that compare the variance obtained by MC and LHS to the true one\n",
    "plt.plot(solver.t, V_y(solver.t), label='True variance')\n",
    "plt.plot(solver.t, y_v_mc, '--', label='MC (10,000)')\n",
    "plt.plot(solver.t, y_v_lhs, '-.', label='LHS (10,000))')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem close. But this is for 10,000 samples.\n",
    "Let's test the convergence of each method. To do this, we will compute the evolution of the root square error in the variance:\n",
    "$$\n",
    "\\mbox{RSE}_\\alpha(n) = \\left(\\sum_{i=1}^{n_t}\\left(V[y(t_i)] - V_{\\alpha,n}[y(t_i)]\\right)^2\\right)^{\\frac{1}{2}},\n",
    "$$\n",
    "where $\\alpha = \\{\\mbox{MC}, \\mbox{LHS}\\}$ stands for the method, $n$ for the number of samples used, and $V_{\\alpha,n}[y(t_i)]$ for the estimate of the variance of $y(t_i)$.\n",
    "It will probably take a while to develop the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_MC_rse(max_num_samples=100):\n",
    "    \"\"\"\n",
    "    Get the maximum error of MC.\n",
    "    \"\"\"\n",
    "    y_v_true = V_y(solver.t)\n",
    "    y = np.zeros(solver.num_output)\n",
    "    y2 = np.zeros(solver.num_output)\n",
    "    n = []\n",
    "    rse = []\n",
    "    for i in xrange(max_num_samples):\n",
    "        x = np.random.rand(2)\n",
    "        y_sample = solver(x)\n",
    "        y += y_sample\n",
    "        y2 += y_sample ** 2\n",
    "        if i % 1 == 0:    # Produce estimate every 100 steps\n",
    "            n.append(i + 1)\n",
    "            y_m = y / (i + 1)\n",
    "            y_v = y2 / (i + 1) - y_m ** 2\n",
    "            rse.append(np.linalg.norm(y_v_true - y_v))\n",
    "    return n, rse\n",
    "\n",
    "def _get_LHS_rse(max_num_samples=100):\n",
    "    y_v_true = V_y(solver.t)\n",
    "    y = np.zeros(solver.num_output)\n",
    "    y2 = np.zeros(solver.num_output)\n",
    "    n = []\n",
    "    rse = []\n",
    "    X = design.latin_center(max_num_samples, 2)\n",
    "    for i in xrange(max_num_samples):\n",
    "        x = X[i, :]\n",
    "        y_sample = solver(x)\n",
    "        y += y_sample\n",
    "        y2 += y_sample ** 2\n",
    "        if i % 1 == 0:    # Produce estimate every 100 steps\n",
    "            n.append(i + 1)\n",
    "            y_m = y / (i + 1)\n",
    "            y_v = y2 / (i + 1) - y_m ** 2\n",
    "            rse.append(np.linalg.norm(y_v_true - y_v))\n",
    "    return n, rse\n",
    "\n",
    "def get_LHS_rse(max_num_samples=100):\n",
    "    n = []\n",
    "    rse = []\n",
    "    for i in xrange(max_num_samples):\n",
    "        if i % 1 == 0:\n",
    "            _n, _rse = _get_LHS_rse(i + 1)\n",
    "            n.append(_n[-1])\n",
    "            rse.append(_rse[-1])\n",
    "    return n, rse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_mc, rse_mc = get_MC_rse(50)\n",
    "n_lhs, rse_lhs = get_LHS_rse(50)\n",
    "plt.plot(n_mc, rse_mc, label='MC RSE')\n",
    "plt.plot(n_lhs, rse_lhs, label='LHS RSE')\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('RSE')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to tell which one is better...\n",
    "There is a considerable ammount of epistemic uncertainty.\n",
    "How would you go about quantifying it?\n",
    "Notice, that everytime you run the above code, it produces a different estimate. This suggests, that if you repeat the procedure, say 100 times, you will be able to get predictive error bars for the error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_exper = 1\n",
    "num_samples = 20\n",
    "rse_mc_samples = []\n",
    "rse_lhs_samples = []\n",
    "for i in xrange(num_exper):\n",
    "    print i\n",
    "    n_mc, rse_mc = get_MC_rse(num_samples)\n",
    "    n_lhs, rse_lhs = get_LHS_rse(num_samples)\n",
    "    rse_mc_samples.append(rse_mc)\n",
    "    rse_lhs_samples.append(rse_lhs)\n",
    "rse_mc_samples = np.array(rse_mc_samples)\n",
    "rse_lhs_samples = np.array(rse_lhs_samples)\n",
    "rse_mc_m = np.mean(rse_mc_samples, axis=0)\n",
    "rse_mc_l = np.percentile(rse_mc_samples, 2.75, axis=0)\n",
    "rse_mc_u = np.percentile(rse_mc_samples, 97.5, axis=0)\n",
    "rse_lhs_m = np.mean(rse_lhs_samples, axis=0)\n",
    "rse_lhs_l = np.percentile(rse_lhs_samples, 2.75, axis=0)\n",
    "rse_lhs_u = np.percentile(rse_lhs_samples, 97.5, axis=0)\n",
    "plt.plot(n_mc, rse_mc_m, label='MC')\n",
    "plt.plot(n_lhs, rse_lhs_m, '--', label='LHS')\n",
    "plt.fill_between(n_mc, rse_mc_l, rse_mc_u, alpha=0.25)\n",
    "plt.fill_between(n_lhs, rse_lhs_l, rse_lhs_u, color='g', alpha=0.25)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('RSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ To get a more accurate picture of the convergence rate of the two methods increaes the number of trials ``num_expr``. You will have to wait...\n",
    "\n",
    "+ LHS seems to perform better. Does it have a different convergence rate than MC?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
