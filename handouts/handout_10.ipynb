{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 - Priors on Function Spaces: Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "+ Express prior knowledge/beliefs about model outputs using Gaussian process (GP)\n",
    "\n",
    "+ Sample functions from the probability measure defined by GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Readings:\n",
    "\n",
    "Please read the following before lecture: \n",
    "\n",
    "+ [Chapter 1 from C.E. Rasmussen's textbook on Gaussian processes](http://www.gaussianprocess.org/gpml/chapters/RW1.pdf).\n",
    "\n",
    "+ (Optional video lecture?) [Neil Lawrence's video lecture on Introduction to Gaussian processes](https://www.youtube.com/watch?v=ewJ3AxKclOg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling prior knowledge in Gaussian processes\n",
    "\n",
    "An experienced scientist or engineer typically has some knowledge about a function of interest $f(\\cdot)$ even before observing it anywhere. For example, he/she might know that $f(\\cdot)$ cannot exceed, or be smaller than, certain values or that it is periodic or that it shows translational invariance. Such knowledge is known as the *prior knowledge*. \n",
    "Prior knowledge may be *precise*, e.g., the response is twice differentiable, or it may be vague, e.g., the probability that the periodicity is $T$ is $p(T)$. When one is dealing with vague prior knowledge, he/she may refer to it as *prior belief*. Almost always, prior knowledge a field quantity is a _prior belief_.\n",
    "\n",
    "\n",
    "Prior beliefs about $f(\\cdot)$ can be modeled by a probability measure on the space of functions from $\\mathcal{X}$ to $\\mathbb{R}$.\n",
    "A Gaussian process (GP) is a great way to represent this probability measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Gaussian Processes.\n",
    "In many engineering problems we have to deal with functions that are unknown.\n",
    "For example, in oil reservoir modeling, the permeability tensor or the porosity  of\n",
    "the ground are, generally, unknown quantities.\n",
    "Therefore, we would like to treat them as if they where random.\n",
    "That is, we have to talk about probabilities on function spaces.\n",
    "Such a thing is achieved via the theory of *random fields*.\n",
    "However, instead of developing the generic mathematical theory of random fields,\n",
    "we concentrate on a special class of random fields, the *Gaussian random fields*\n",
    "or *Gaussian processes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian process (GP) is a generalization of a multivariate Gaussian distribution to\n",
    "*infinite* dimensions.\n",
    "It essentially defines a probability measure on a function space.\n",
    "When we say that $f(\\cdot)$ is a GP, we mean that it is a random variable that is actually\n",
    "a function.\n",
    "Mathematically, we write:\n",
    "\\begin{equation}\n",
    "f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\sim \\mbox{GP}\\left(f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\right),\n",
    "\\end{equation}\n",
    "where \n",
    "$m:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the *mean function* and \n",
    "$k:\\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the *covariance function*.\n",
    "So, compared to a multivariate normal we have:\n",
    "\n",
    "+ A random function $f(\\cdot)$ instead of a random vector $\\mathbf{x}$.\n",
    "+ A mean function $m(\\cdot)$ instead of a mean vector $\\boldsymbol{\\mu}$.\n",
    "+ A covariance function $k(\\cdot,\\cdot)$ instead of a covariance matrix $\\mathbf{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what does this definition actually mean? Actually, it gets its meaning from the multivariate Gaussian distribution. Here is how: \n",
    "\n",
    "+ Let $\\mathbf{x}_{1:n}=\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\}$ be $n$ points in $\\mathbb{R}^d$.\n",
    "+ Let $\\mathbf{f}\\in\\mathbb{R}^n$ be the outputs of $f(\\cdot)$ on each one of the elements of $\\mathbf{x}_{1:n}$, i.e.,\n",
    "$$\n",
    "\\mathbf{f} =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "f(\\mathbf{x}_1)\\\\\n",
    "\\vdots\\\\\n",
    "f(\\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "+ The fact that $f(\\cdot)$ is a GP with mean and covariance function $m(\\cdot)$ and $k(\\cdot,\\cdot)$, respectively, *means* that the vector of outputs $\\mathbf{f}$ at\n",
    "the arbitrary inputs in $\\mathbf{X}$ is the following multivariate-normal:\n",
    "$$\n",
    "\\mathbf{f} | \\mathbf{x}_{1:n}, m(\\cdot), k(\\cdot, \\cdot) \\sim \\mathcal{N}\\left(\\mathbf{f} | \\mathbf{m}(\\mathbf{x}_{1:n}), \\mathbf{K}(\\mathbf{x}_{1:n}, \\mathbf{x}_{1:n}) \\right),\n",
    "$$\n",
    "with mean vector:\n",
    "$$\n",
    "\\mathbf{m}(\\mathbf{x}_{1:n}) =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "m(\\mathbf{x}_1)\\\\\n",
    "\\vdots\\\\\n",
    "m(\\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$\n",
    "and covariance matrix:\n",
    "$$\n",
    "\\mathbf{K}(\\mathbf{x}_{1:n},\\mathbf{x}_{1:n}) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1, \\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "k(\\mathbf{x}_n, \\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n, \\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Now that we have defined a Gaussian process (GP), let us talk about we encode our prior beliefs into a GP. \n",
    "We do so through the mean and covariance functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the mean function.\n",
    "What is the meaning of $m(\\cdot)$?\n",
    "Well, it is quite easy to grasp.\n",
    "For any point $\\mathbf{x}\\in\\mathbb{R}^d$, $m(\\mathbf{x})$ should give us the value we believe is more probable for \n",
    "$f(\\mathbf{x})$.\n",
    "Mathematically, $m(\\mathbf{x})$ is nothing more than the expected value of the random variable $f(\\mathbf{x})$.\n",
    "That is:\n",
    "\\begin{equation}\n",
    "m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})].\n",
    "\\end{equation}\n",
    "\n",
    "The mean function can be any arbitrary function. Essentially, it tracks generic trends in the response as the input is varied. In practice, we try and make a suitable choice for the mean function that is easy to work with. Such choices include: \n",
    "\n",
    "+ zero, i.e.,\n",
    "$$\n",
    "m(\\mathbf{x}) = 0.\n",
    "$$\n",
    "\n",
    "+ a constant, i.e.,\n",
    "$$\n",
    "m(\\mathbf{x}) = c,\n",
    "$$\n",
    "where $c$ is a parameter.\n",
    "\n",
    "+ linear, i.e.,\n",
    "$$\n",
    "m(\\mathbf{x}) = c_0 + \\sum_{i=1}^dc_ix_i,\n",
    "$$\n",
    "where $c_i, i=0,\\dots,d$ are parameters.\n",
    "\n",
    "+ using a set of $m$ basis functions (generalized linear model), i.e.,\n",
    "$$\n",
    "m(\\mathbf{x}) = \\sum_{i=1}^mc_i\\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "where $c_i$ and $\\phi_i(\\cdot)$ are parameters and basis functions.\n",
    "\n",
    "+ generalized polynomial chaos (gPC), i.e., \n",
    "using a set of $d$ polynomial basis functions upto a given degree $\\rho$\n",
    "$m(\\mathbf{x}) = \\sum_{i=1}^{d}c_i\\phi_i(\\mathbf{x})$ \n",
    "where the basis functions $\\phi_i$ are mutually orthonormal with respect to some \n",
    "measure $\\mu$:\n",
    "$$\n",
    "\\int \\phi_{i}(\\mathbf{x}) \\phi_{j}(\\mathbf{x}) d\\mu(\\mathbf{x}) = \\delta_{ij}\n",
    "$$\n",
    "\n",
    "+ and many other possibilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the covariance function.\n",
    "What is the meaning of $k(\\cdot, \\cdot)$?\n",
    "This concept is considerably more challenging than the mean.\n",
    "\n",
    "Let's try to break it down:\n",
    "\n",
    "+ Let $\\mathbf{x}\\in\\mathbb{R}^d$. Then $k(\\mathbf{x}, \\mathbf{x})$ is the variance of the random variable $f(\\mathbf{x})$, i.e.,\n",
    "$$\n",
    "\\mathbb{V}[f(\\mathbf{x})] = \\mathbb{E}\\left[\\left(f(\\mathbf{x}) - m(\\mathbf{x}) \\right)^2 \\right].\n",
    "$$\n",
    "In other words, we believe that there is about $95\\%$ probability that the value of\n",
    "the random variable $f(\\mathbf{x})$ fall within the interval:\n",
    "$$\n",
    "\\left((m(\\mathbf{x}) - 2\\sqrt{k(\\mathbf{x}, \\mathbf{x})}, m(\\mathbf{x}) + 2\\sqrt{k(\\mathbf{x},\\mathbf{x})}\\right).\n",
    "$$\n",
    "\n",
    "+ Let $\\mathbf{x},\\mathbf{x}'\\mathbb{R}^d$. Then $k(\\mathbf{x}, \\mathbf{x}')$ tells us how the random variable $f(\\mathbf{x})$ and\n",
    "$f(\\mathbf{x}')$ are correlated. In particular, $k(\\mathbf{x},\\mathbf{x}')$ is equal to the covariance\n",
    "of the random variables $f(\\mathbf{x})$ and $f(\\mathbf{x}')$, i.e.,\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = \\mathbb{C}[f(\\mathbf{x}), f(\\mathbf{x}')]\n",
    "= \\mathbb{E}\\left[\n",
    "\\left(f(\\mathbf{x}) - m(\\mathbf{x})\\right)\n",
    "\\left(f(\\mathbf{x}') - m(\\mathbf{x}')\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Essentially, a covariance function (or covariance kernel) defines a nearness or similarity measure on the input space. We cannot choose any arbitrary function of two variables as a covariance kernel. How we go about choosing a covariance function is discussed in great detail [here](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf). We briefly discuss some properties of covariance functions here and then we shall move onto a discussion of what kind of prior beliefs we can encode through the covariance function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the covariance function\n",
    "\n",
    "+ There is one property of the covariance function that we can note right away.\n",
    "Namely, that for any $\\mathbf{x}\\in\\mathbb{R}^d$, $k(\\mathbf{x}, \\mathbf{x}) > 0$.\n",
    "This is easly understood by the interpretation of $k(\\mathbf{x}, \\mathbf{x})$ as the variance\n",
    "of the random variable $f(\\mathbf{x})$.\n",
    "\n",
    "+ $k(\\mathbf{x}, \\mathbf{x}')$ becomes smaller as the distance between $\\mathbf{x}$ and $\\mathbf{x}'$ grows.\n",
    "\n",
    "+ For any choice of points $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$, the covariance matrix: $\\mathbf{K}(\\mathbf{X}, \\mathbf{X})$ has\n",
    "to be positive-definite (so that the vector of outputs $\\mathbf{f}$ is indeed a multivariate\n",
    "normal distribution).\n",
    "\n",
    "\n",
    "### Encoding prior beliefs in the covariance function. \n",
    "\n",
    "+ **Modeling regularity**. The choice of the covariance function controls the regularity properties of the functions sampled from the probability induced by the GP. For example, if the covariance kernel chosen is the squared exponential kernel, which is infinitely differentiable, then the functions sampled from the GP will also be infinitely differentiable. \n",
    "\n",
    "+ **Modeling invariance** If the covariance kernel is invariant w.r.t. a transformation $T$, i.e., $k(\\mathbf{x}, T\\mathbf{x}')=k(T\\mathbf{x}, \\mathbf{x}')=k(\\mathbf{x}, \\mathbf{x}')$ then samples from the GP will be invariant w.r.t. the same transformation. \n",
    "\n",
    "+ Other possibilities include periodicity, additivity etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared exponential covariance function\n",
    "\n",
    "Squared expnential (SE) is the most commonly used covariance function.\n",
    "Its formula is as follows:\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = v\\exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^d\\frac{(x_i - x_i')^2}{\\ell_i^2}\\right\\},\n",
    "$$\n",
    "where $v,\\ell_i>0, i=1,\\dots,d$ are parameters.\n",
    "The interpretation of the parameters is as follows:\n",
    "\n",
    "+ $v$ is known as the *signal strength*. The bigger it is, the more the GP $f(\\cdot)$ will vary\n",
    "about the mean.\n",
    "\n",
    "+ $\\ell_i$ is known as the *length scale* of the $i$-th input dimension of the GP.\n",
    "The bigger it is, the smoother the samples of $f(\\cdot)$ appear along the $i$-th input dimension.\n",
    "\n",
    "Let's experiment with this for a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns # Comment this out if you don't have it\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "import GPy\n",
    "# The input dimension\n",
    "dim = 1\n",
    "# The variance of the covariance kernel\n",
    "variance = 1.\n",
    "# The lengthscale of the covariance kernel\n",
    "ell = 0.3\n",
    "# Generate the covariance object\n",
    "k = GPy.kern.RBF(dim, variance=variance, lengthscale=ell)\n",
    "# Print it\n",
    "print k\n",
    "# and plot it\n",
    "k.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Plotting a covariance function\n",
    "Remember:\n",
    "> The covariance function $k(x,x')$ measures the similarity of $f(x)$ and $f(x')$.\n",
    "\n",
    "The interactive tools provided, draw $k(\\mathbf{x}, \\mathbf{x}'=0)$ in one and two dimensions. \n",
    "Use them to answer the following questions:\n",
    "+ What is the intuitive meaning of $\\ell$?\n",
    "+ What is the intuitive meaning of $v$?\n",
    "+ There are many other covariance functions that we could be using. Try changing ``RBF`` to ``Exponential``. What changes do you nottice.\n",
    "+ Repeat the previous steps on a 2D covariance function.\n",
    "+ If you still have time, try a couple of other covariances, e.g., ``Matern32``, ``Matern52``.\n",
    "+ If you still have time, explore ``help(GPy.kern)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "def plot_kernel(variance=1., ell=0.3):\n",
    "    k = GPy.kern.RBF(dim, variance=variance, lengthscale=ell)\n",
    "    k.plot()\n",
    "    plt.ylim(0, 10)\n",
    "interactive(plot_kernel, variance=(1e-3, 10., 0.01), ell=(1e-3, 10., 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "def plot_kernel(variance=1., ell1=0.3, ell2=0.3):\n",
    "    k = GPy.kern.RBF(2, ARD=True, variance=variance,\n",
    "                     lengthscale=[ell1, ell2])  # Notice that I just changed the dimension here\n",
    "    k.plot()\n",
    "interactive(plot_kernel, variance=(1e-3, 10., 0.01), ell1=(1e-3, 10., 0.01), ell2=(1e-3, 10., 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Properties of the covariance matrix\n",
    "Let $\\mathbf{x}_{1:n}$ be an arbitrary set of input points. The covariance matrix $\\mathbf{K}\\in\\mathbb{R}^{n\\times n}$ defined by:\n",
    "$$\n",
    "\\mathbf{K}\\equiv\\mathbf{K}(\\mathbf{x}_{1:n}, \\mathbf{x}_{1:n}) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1, \\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "k(\\mathbf{x}_n, \\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n, \\mathbf{x}_n)\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$\n",
    "must be [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix). Mathematically this can be expressed in two equivalent ways:\n",
    "\n",
    "+ For all vectors $\\mathbf{v}\\in\\mathbb{R}^T$, we have:\n",
    "$$\n",
    "\\mathbf{v}^t\\mathbf{K}\\mathbf{v} > 0,\n",
    "$$\n",
    "+ All the eigenvalues of $\\mathbf{K}$ are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code provided:\n",
    "+ Verify that the the sum of two covariance functions is a valid covariance function.\n",
    "+ Verify that the product of two covariance functions is a valid covariance function.\n",
    "+ Is the following function a covariance function:\n",
    "$$\n",
    "k(x, x') = k_1(x, x')k_2(x, x') + k_3(x, x') + k_4(x, x'),\n",
    "$$\n",
    "where all $k_i(x, x')$'s are covariance functions.\n",
    "+ What about:\n",
    "$$\n",
    "k(x, x') = k_1(x, x') / k_2(x, x')?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of dimensions\n",
    "dim = 1\n",
    "\n",
    "# Number of input points\n",
    "n = 20\n",
    "\n",
    "# The lengthscale\n",
    "ell = .1\n",
    "\n",
    "# The variance \n",
    "variance = 1.\n",
    "\n",
    "# The covariance function\n",
    "k1 = GPy.kern.RBF(dim, lengthscale=ell, variance=variance)\n",
    "\n",
    "# Draw a random set of inputs points in [0, 1]^dim\n",
    "X = np.random.rand(n, dim)\n",
    "\n",
    "# Evaluate the covariance matrix on these points\n",
    "K = k1.K(X)\n",
    "\n",
    "# Compute the eigenvalues of this matrix\n",
    "eig_val, eig_vec = np.linalg.eigh(K)\n",
    "\n",
    "# Plot the eigenvalues (they should all be positive)\n",
    "print '> plotting eigenvalues of K'\n",
    "print '> they must all be positive'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1, n+1), eig_val, '.')\n",
    "ax.set_xlabel('$i$', fontsize=16)\n",
    "ax.set_ylabel('$\\lambda_i$', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now create another (arbitrary) covariance function\n",
    "k2 = GPy.kern.Exponential(dim, lengthscale=0.2, variance=2.1)\n",
    "\n",
    "# Create a new covariance function that is the sum of these two:\n",
    "k_new = k1 + k2\n",
    "\n",
    "# Let's plot the new covariance\n",
    "fig, ax = plt.subplots()\n",
    "k1.plot(ax=ax, label='$k_1$')\n",
    "k2.plot(ax=ax, label='$k_2$')\n",
    "k_new.plot(ax=ax, label='$k_1 + k_2$')\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If this is a valid covariance function, then it must \n",
    "# be positive definite\n",
    "# Compute the covariance matrix:\n",
    "K_new = k_new.K(X)\n",
    "\n",
    "# and its eigenvalues\n",
    "eig_val_new, eig_vec_new = np.linalg.eigh(K_new)\n",
    "\n",
    "# Plot the eigenvalues (they should all be positive)\n",
    "print '> plotting eigenvalues of K'\n",
    "print '> they must all be positive'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1, n+1), eig_val_new, '.')\n",
    "ax.set_xlabel('$i$', fontsize=16)\n",
    "ax.set_ylabel('$\\lambda_i$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Sampling from a Gaussian Process. \n",
    "\n",
    "Samples from a Gaussian process are functions. But, functions are infinite dimensional objects?\n",
    "We cannot sample directly from a GP....\n",
    "However, if we are interested in the values of $f(\\cdot)$ at any given set of test points $\\mathbf{x}_{1:n} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_b\\}$, then we have that:\n",
    "$$\n",
    "\\mathbf{f} | \\mathbf{x}_{1:n}, m(\\cdot), k(\\cdot, \\cdot) \\sim \\mathcal{N}\\left(\\mathbf{f} | \\mathbf{m}(\\mathbf{x}_{1:n}), \\mathbf{K}(\\mathbf{x}_{1:n}, \\mathbf{x}_{1:n}) \\right),\n",
    "$$\n",
    "where all the quantities have been introduced above.\n",
    "This is\n",
    "What we are going to do is pick a dense set of points $\\mathbf{x}_{1:n}\\in\\mathbb{R}^{n\\times d}$\n",
    "sample the value of the GP, $\\mathbf{f} = (f(\\mathbf{x}_1),\\dots,f(\\mathbf{x}_n))$ on these points.\n",
    "We saw above that the probability density of $\\mathbf{f}$ is just a multivariate normal\n",
    "with a mean vector that is specified from the mean function and a covariance matrix\n",
    "that is specified by the covariance function.\n",
    "Therefore, all we need to know is how to sample from the multivariate normal.\n",
    "This is how we do it:\n",
    "+ Compute the Cholesky of $\\mathbf{L}$:\n",
    "$$\n",
    "\\mathbf{K} = \\mathbf{L}\\mathbf{L}^T.\n",
    "$$\n",
    "+ Draw $n$ random samples $\\mathbf{z} = (z_1,\\dots,z_n)$ independently from a standard normal.\n",
    "+ Get one sample by:\n",
    "$$\n",
    "\\mathbf{f} = \\mathbf{m} + \\mathbf{L}\\mathbf{z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To gaurantee reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# Number of test points\n",
    "num_test = 10\n",
    "\n",
    "# Pick a covariance function\n",
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=.1)\n",
    "\n",
    "# Pick a mean function\n",
    "mean_func = lambda(x): np.zeros(x.shape)\n",
    "\n",
    "# Pick a bunch of points over which you want to sample the GP\n",
    "X = np.linspace(0, 1, num_test)[:, None]\n",
    "\n",
    "# Evaluate the mean function at X\n",
    "m = mean_func(X)\n",
    "\n",
    "# Compute the covariance function at these points\n",
    "nugget = 1e-6 # This is a small number required for stability\n",
    "C = k.K(X) + nugget * np.eye(X.shape[0])\n",
    "\n",
    "# Compute the Cholesky of the covariance\n",
    "# Notice that we need to do this only once\n",
    "L = np.linalg.cholesky(C)\n",
    "\n",
    "# Number of samples to take\n",
    "num_samples = 3\n",
    "\n",
    "# Take 3 samples from the GP and plot them:\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the mean function\n",
    "ax.plot(X, m)\n",
    "for i in xrange(num_samples):\n",
    "    z = np.random.randn(X.shape[0], 1)    # Draw from standard normal\n",
    "    f = m + np.dot(L, z)                  # f = m + L * z\n",
    "    ax.plot(X, f, color=sns.color_palette()[1], linewidth=1)\n",
    "#ax.set_ylim(-6., 6.)\n",
    "ax.set_xlabel('$x$', fontsize=16)\n",
    "ax.set_ylabel('$y$', fontsize=16)\n",
    "ax.set_ylim(-5, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid line is the mean function and the dashed lines are 3 samples of   f . These don’t look like functions yet. This is because we have used only 10 test points to represent the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. Edit the code above changing the number of test points ``num_test`` to 20, 50, 100. Rerun the example. How do your samples of   f look like now? Do they look more like functions to you? Imagine that the true nature of the GP appears when these test points become infinitely dense.\n",
    "\n",
    "2. Edit the code above and change the random seed to an arbitrary integer (just make up one). Rerun the example and notice how the sampled functions change.\n",
    "\n",
    "3. Edit the code above and change the variance first to 0.1 and then to 5 each time rerunning the example. Notice the values on the vertical axis of the plot. What happens to the sampled functions as you do this? What does the variance parameter of the SE control?\n",
    "\n",
    "4. Edit the code above and now change the length-scale parameter first to 0.05 and then to 1. What happens to the sampled functions as you do this? What does the length- scale parameter of the SE control?\n",
    "\n",
    "5. Now set the variance and the length-scale back to their original values (1. and 0.1, respectively). Edit the code and change the mean function to:\n",
    "```\n",
    "     mean_fun = lambda(x): 5 * x\n",
    "```\n",
    "Re-run the example. What do you observe? Try a couple more. For example, try:\n",
    "```\n",
    "     mean_fun = lambda(x): np.sin(5 * np.pi * x)\n",
    "```\n",
    "\n",
    "6. So far, all the samples we have seen are smooth. There is this theorem that says that the samples of the GP will be as smooth as the covariance function we use. Since the SE covariance is infinitely smooth, the samples we draw are infinitely smooth. The [Matern 3-2 covariance function](https://en.wikipedia.org/wiki/Matérn_covariance_function) is twice differentiable. Edit the code and\n",
    "change ``RBF`` to ``Matern32``. Rerun the example. How smooth are the samples now?\n",
    "\n",
    "7. The exponential covariance function is continuous but not differentiable. Edit the code and change ``RBF`` to ``Exponential``. Rerun the example. How smooth are the samples now?\n",
    "\n",
    "8. The covariance function can also be used to model invariances. The periodic exponential covariance function is... a periodic covariance function. Edit line 29 and change ``RBF`` to \n",
    "```\n",
    "k = GPy.kern.PeriodicMatern32(input_dim=1, variance=500., lengthscale=0.01, period=0.1)\n",
    "```\n",
    "Rerun the example. Do you notice the periodic pattern?\n",
    "\n",
    "9. How can you encode the information that there are two lengthscales in $f(\\cdot)$. There are many ways to do this.\n",
    "Try summing or multiplying covariance functions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
