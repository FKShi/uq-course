{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Generalized Linear Models (Part II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Maximum Posterior Estimate\n",
    "+ Bayesian Linear Regression\n",
    "+ Evidence Approximation\n",
    "+ Automatic Relevance Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "Before coming to class, please read the following:\n",
    "\n",
    "+ [Ch. 3 of Bishop, 2006](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)\n",
    "\n",
    "+ [Ohio State University, Bayesian Linear Regression](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0ahUKEwikxsiPuJPKAhVE32MKHRoMCtsQFggyMAI&url=http%3A%2F%2Fweb.cse.ohio-state.edu%2F~kulis%2Fteaching%2F788_sp12%2Fscribe_notes%2Flecture5.pdf&usg=AFQjCNFvxuyBfFkRN8bdJAvd_dlZdsShEw&sig2=UqakvfANehNUUK1J9rXIiQ)\n",
    "\n",
    "You can also check out this 10 minutes short Youtube video on Bayesian Linear Regression - \n",
    "+ [Mathematicalmonk, Bayesian Linear Regression](https://www.youtube.com/watch?v=dtkGq9tdYcI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plese see the 7th handout before you start on this one.\n",
    "We just repeate some of the code that we developed there.\n",
    "In particular, we load the essential modules and we redefine the basis function classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Basis\n",
    "class LinearBasis(object):\n",
    "    \"\"\"\n",
    "    Represents a 1D linear basis.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_basis = 2 # The number of basis functions\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        ``x`` should be a 1D array.\n",
    "        \"\"\"\n",
    "        return [1., x[0]]\n",
    "    \n",
    "# We need a generic function that computes the design matrix\n",
    "def compute_design_matrix(X, phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    X   -  The observed inputs (1D array)\n",
    "    phi -  The basis functions.\n",
    "    \"\"\"\n",
    "    num_observations = X.shape[0]\n",
    "    num_basis = phi.num_basis\n",
    "    Phi = np.ndarray((num_observations, num_basis))\n",
    "    for i in xrange(num_observations):\n",
    "        Phi[i, :] = phi(X[i, :])\n",
    "    return Phi\n",
    "\n",
    "\n",
    "# Here is a class for the polynomials:\n",
    "class PolynomialBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    degree  -  The degree of the polynomial.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        self.num_basis = degree + 1\n",
    "    def __call__(self, x):\n",
    "        return np.array([x[0] ** i for i in range(self.degree + 1)])\n",
    "    \n",
    "    \n",
    "class FourierBasis(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    num_terms  -  The number of Fourier terms.\n",
    "    L          -  The period of the function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_terms, L):\n",
    "        self.num_terms = num_terms\n",
    "        self.L = L\n",
    "        self.num_basis = 2 * num_terms\n",
    "    def __call__(self, x):\n",
    "        res = np.ndarray((self.num_basis,))\n",
    "        for i in xrange(num_terms):\n",
    "            res[2 * i] = np.cos(2 * i * np.pi / self.L * x[0])\n",
    "            res[2 * i + 1] = np.sin(2 * (i+1) * np.pi / self.L * x[0])\n",
    "        return res\n",
    "    \n",
    "\n",
    "class RadialBasisFunctions(object):\n",
    "    \"\"\"\n",
    "    A set of linear basis functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the radial basis functions.\n",
    "    ell -  The assumed lengthscale.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, ell):\n",
    "        self.X = X\n",
    "        self.ell = ell\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        return np.exp(-.5 * (x - self.X) ** 2 / self.ell ** 2).flatten()\n",
    "    \n",
    "\n",
    "\n",
    "class StepFunctionBasis(object):\n",
    "    \"\"\"\n",
    "    A set of step functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the step functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        self.num_basis = X.shape[0]\n",
    "    def __call__(self, x):\n",
    "        res = np.ones((self.num_basis, ))\n",
    "        res[x < self.X.flatten()] = 0.\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the motorcycle data set of lecture 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('motor.dat')\n",
    "X = data[:, 0][:, None]\n",
    "Y = data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Regression - Version 2\n",
    "+ We wish to model the data using some **fixed** basis/features:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "}\n",
    "$$\n",
    "\n",
    "+ We *model the measurement process* using a **likelihood** function:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w} \\sim p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}).\n",
    "$$\n",
    "\n",
    "+ We *model the uncertainty in the model parameters* using a **prior**:\n",
    "$$\n",
    "\\mathbf{w} \\sim p(\\mathbf{w}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Prior on the Weights\n",
    "+ Consider the following **prior** on $\\mathbf{w}$:\n",
    "$$\n",
    "p(\\mathbf{w}|\\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}\\mathbf{I}\\right) = \n",
    "\\left(\\frac{\\alpha}{2\\pi}\\right)^{\\frac{m}{2}}\\exp\\left\\{-\\frac{\\alpha}{2}\\lVert\\mathbf{w}\\rVert^2\\right\\}.\n",
    "$$\n",
    "+ We say:\n",
    "\n",
    "> Before we see the data, we beleive that $\\mathbf{w}$ must be around zero with a precision of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Posterior of the Weights\n",
    "+ Combining the likelihood and the prior, we get using Bayes rule:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma,\\alpha) = \n",
    "\\frac{p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma)p(\\mathbf{w}|\\alpha)}\n",
    "{\\int p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}', \\sigma)p(\\mathbf{w}'|\\alpha)d\\mathbf{w}'}.\n",
    "$$\n",
    "+ We say\n",
    "> The posterior summarizes our state of knowledge about $\\mathbf{w}$ after we see the data,\n",
    "if we know $\\alpha$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Posterior Estimate\n",
    "+ We can find a point estimate of $\\mathbf{w}$ by solving:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MPE}} = \\arg\\max_{\\mathbf{w}} p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma)p(\\mathbf{w}|\\alpha).\n",
    "$$\n",
    "+ For Gaussian likelihood and weights:\n",
    "$$\n",
    "\\log p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma,\\alpha) = \n",
    "- \\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2 -\\frac{\\alpha}{2}\\lVert\\mathbf{w}\\rVert^2.\n",
    "$$\n",
    "+ With maximum:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MPE}} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "+ But, no analytic formula for $\\sigma$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stable Way to Compute the MAP Estimate\n",
    "+ Construct the positive-definite matrix:\n",
    "$$\n",
    "\\mathbf{A} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)\n",
    "$$\n",
    "+ Compute the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\mathbf{A}$:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T,\n",
    "$$\n",
    "where $\\mathbf{L}$ is lower triangular.\n",
    "+ Then, solve the system:\n",
    "$$\n",
    "\\mathbf{L}\\mathbf{L}^T\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{y}_{1:n},\n",
    "$$\n",
    "doing a forward and a backward substitution.\n",
    "+ [scipy.linalg.cho_factor](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_factor.html#scipy.linalg.cho_factor) and [scipy.linalg.cho_solve](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_solve.html) can be used for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "ell = 2.\n",
    "alpha = 5\n",
    "sigma = 20.28\n",
    "Xc = np.linspace(0, 60, 20)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "A = np.dot(Phi.T, Phi) / sigma ** 2. + alpha * np.eye(Phi.shape[1])\n",
    "L = scipy.linalg.cho_factor(A)\n",
    "w_MPE = scipy.linalg.cho_solve(L, np.dot(Phi.T, Y))\n",
    "print 'w_MPE:'\n",
    "print w_MPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's predict on these points:\n",
    "X_p = np.linspace(0, 60, 100)[:, None]\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, w_MPE)\n",
    "Y_l = Y_p - 2. * sigma # Lower predictive bound\n",
    "Y_u = Y_p + 2. * sigma # Upper predictive bound\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='MPE Prediction (Radial Basis Functions, alpha=%1.2f)' % alpha)\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with different alphas.\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which one (if any) gives you the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Maximum Posterior Estimate\n",
    "+ How many basis functions should I use?\n",
    "+ Which basis functions should I use?\n",
    "+ How do I pick the parameters of the basis functions, e.g., the lengthscale $\\ell$ of the RBFs, $\\alpha$, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Regression - Version 3 - Bayesian Linear Regression\n",
    "+ For Gaussian likelihood and weights, the posterior is Gaussian:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma, \\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{S} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)^{-1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{m} = \\sigma^{-2}\\mathbf{S}\\Phi^T\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "+ In general: [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution\n",
    "+ Using probability theory, we ask: What do we know about $y$ at a new $\\mathbf{x}$ after seeing the data.\n",
    "+ We have using the sum rule:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}, \\sigma, \\alpha) = \n",
    "\\int p(y | \\mathbf{x}, \\mathbf{w}, \\sigma) p(\\mathbf{w}|\\mathbf{x}_{1:n}, \\mathbf{y}_{1:n},\\sigma,\\alpha)d\\mathbf{w}.\n",
    "$$\n",
    "+ For Gaussian likelihood and prior:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}, \\sigma, \\alpha) = \\mathcal{N}\\left(y|m(\\mathbf{x}), s^2(\\mathbf{x})\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m(\\mathbf{x}) = \\mathbf{m}^T\\boldsymbol{\\phi}(\\mathbf{x})\\;\\mbox{and}\\;s(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x}) + \\sigma^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Uncertainty\n",
    "+ The **predictive uncertainty** is:\n",
    "$$\n",
    "s^2(\\mathbf{x}) = \\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x}) + \\sigma^2.\n",
    "$$\n",
    "+ $\\sigma^2$ corresponds to the measurement noise.\n",
    "+ $\\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{S}\\boldsymbol{\\phi}(\\mathbf{x})$ is the epistemic uncertainty induced by limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "ell = 2.\n",
    "alpha = 0.001\n",
    "sigma = 20.28\n",
    "Xc = np.linspace(0, 60, 20)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "A = np.dot(Phi.T, Phi) / sigma ** 2. + alpha * np.eye(Phi.shape[1])\n",
    "L = scipy.linalg.cho_factor(A)\n",
    "m = scipy.linalg.cho_solve(L, np.dot(Phi.T, Y) / sigma ** 2)  # The posterior mean of w\n",
    "S = scipy.linalg.cho_solve(L, np.eye(Phi.shape[1]))           # The posterior covariance of w\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, m) # The mean prediction\n",
    "V_p_ep = np.einsum('ij,jk,ik->i', Phi_p, S, Phi_p) # The epistemic uncertainty\n",
    "S_p_ep = np.sqrt(V_p_ep)\n",
    "V_p = V_p_ep + sigma ** 2 # Full uncertainty\n",
    "S_p = np.sqrt(V_p)\n",
    "Y_l_ep = Y_p - 2. * S_p_ep  # Lower epistemic predictive bound\n",
    "Y_u_ep = Y_p + 2. * S_p_ep  # Upper epistemic predictive bound\n",
    "Y_l = Y_p - 2. * S_p # Lower predictive bound\n",
    "Y_u = Y_p + 2. * S_p # Upper predictive bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='Bayesian Prediction (Radial Basis Functions, alpha=%1.2f)' % alpha)\n",
    "ax.fill_between(X_p.flatten(), Y_l_ep, Y_u_ep, color=sns.color_palette()[2], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_l_ep, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_u_ep, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Posterior Models\n",
    "+ We can actually sample models (functions) from the posterior. Here is how:\n",
    "    + Sample a $\\mathbf{w}$ from $p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma, \\alpha)$.\n",
    "    + Look at the sampled model:\n",
    "    $$\n",
    "    y(\\mathbf{x};\\mathbf{w}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have m, S, X_p, and Phi_p from before\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "for i in xrange(10):\n",
    "    w = np.random.multivariate_normal(m, S)\n",
    "    Y_p_s = np.dot(Phi_p, w)\n",
    "    ax.plot(X_p, Y_p_s, color=sns.color_palette()[2], linewidth=0.5);\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Bayesian Linear Regression\n",
    "+ How many basis functions should I use?\n",
    "+ Which basis functions should I use?\n",
    "+ How do I pick the parameters of the basis functions, e.g., the lengthscale $\\ell$ of the RBFs, $\\alpha$, etc.?+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Experiment with different alphas, ells, and sigmas.\n",
    "+ When are we underfitting?\n",
    "+ When are we overfitting?\n",
    "+ Which one (if any) gives you the best fit?\n",
    "+ In the figure, right above: Increase the number of posterior $\\mathbf{w}$ samples to get a sense of the epistemic uncertainty induced by the limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Regression - Version 4 - Hierarchical Priors\n",
    "+ So, how do we find all the parameters like $\\sigma$, $\\alpha$, $\\ell$, etc?\n",
    "+ These are all called **hyper-parameters** of the model.\n",
    "+ Call all of them\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\{\\sigma, \\alpha, \\ell,\\dots\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Priors\n",
    "+ Model:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}\n",
    "$$\n",
    "+ Likelihood:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\boldsymbol{\\theta} \\sim p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}, \\boldsymbol{\\theta}).\n",
    "$$\n",
    "+ Weight prior:\n",
    "$$\n",
    "\\mathbf{w} | \\boldsymbol{\\theta} \\sim p(\\mathbf{w}| \\boldsymbol{\\theta}).\n",
    "$$\n",
    "+ Hyper-prior:\n",
    "$$\n",
    "\\boldsymbol{\\theta} \\sim p(\\boldsymbol{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Bayesian Solution\n",
    "+ Just write down the posterior of everything:\n",
    "$$\n",
    "p(\\mathbf{w}, \\boldsymbol{\\theta}|\\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}) \\propto p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}|\\mathbf{w},\\boldsymbol{\\theta})p(\\mathbf{w}|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta}).\n",
    "$$\n",
    "+ and, somehow, sample from it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evidence Approximation\n",
    "+ Look at the marginal posterior of $\\boldsymbol{\\theta}$:\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}|\\mathbf{x}_{1:n}, \\mathbf{y}_{1:n}) \\propto \n",
    "\\int p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}|\\mathbf{w},\\boldsymbol{\\theta})p(\\mathbf{w}|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})d\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "+ Assume that the hyper-prior is relatively flat:\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}) \\propto 1,\n",
    "$$\n",
    "\n",
    "+ Use a MAP estimate for $\\boldsymbol{\\theta}$:\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{\\mbox{EV}} = \\arg\\max_{\\boldsymbol{\\theta}}\\int p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}|\\mathbf{w},\\boldsymbol{\\theta})p(\\mathbf{w}|\\boldsymbol{\\theta})d\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "+ Analytical for Gaussian likelihood and prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Evidence Approximation\n",
    "+ There is a fast algorithm for the evidence approximation for Bayesian linear regression.\n",
    "+ It would take about an hour to go over it. See Ch. 3 of (Bishop, 2006).\n",
    "+ We will use the implementation found in [scikit-learn](http://scikit-learn.org).\n",
    "+ If you don't have it:\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "ell = 2.\n",
    "Xc = np.linspace(0, 60, 50)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "regressor = BayesianRidge()\n",
    "regressor.fit(Phi, Y)\n",
    "# They are using different names:\n",
    "sigma = np.sqrt(1. / regressor.alpha_)\n",
    "print 'best sigma:', sigma\n",
    "alpha = regressor.lambda_\n",
    "print 'best alpha:', alpha\n",
    "A = np.dot(Phi.T, Phi) / sigma ** 2. + alpha * np.eye(Phi.shape[1])\n",
    "L = scipy.linalg.cho_factor(A)\n",
    "m = scipy.linalg.cho_solve(L, np.dot(Phi.T, Y) / sigma ** 2)  # The posterior mean of w\n",
    "S = scipy.linalg.cho_solve(L, np.eye(Phi.shape[1]))           # The posterior covariance of w\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, m) # The mean prediction\n",
    "V_p_ep = np.einsum('ij,jk,ik->i', Phi_p, S, Phi_p) # The epistemic uncertainty\n",
    "S_p_ep = np.sqrt(V_p_ep)\n",
    "V_p = V_p_ep + sigma ** 2 # Full uncertainty\n",
    "S_p = np.sqrt(V_p)\n",
    "Y_l_ep = Y_p - 2. * S_p_ep  # Lower epistemic predictive bound\n",
    "Y_u_ep = Y_p + 2. * S_p_ep  # Upper epistemic predictive bound\n",
    "Y_l = Y_p - 2. * S_p # Lower predictive bound\n",
    "Y_u = Y_p + 2. * S_p # Upper predictive bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='Bayesian Prediction (Radial Basis Functions, alpha=%1.2f)' % alpha)\n",
    "ax.fill_between(X_p.flatten(), Y_l_ep, Y_u_ep, color=sns.color_palette()[2], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_l_ep, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_u_ep, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Bayesian Linear Regression\n",
    "+ How many basis functions should I use?\n",
    "+ Which basis functions should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Try the evidence approximation with the Fourier basis.\n",
    "+ Try the evidence approximation with the Step function basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Regression - Version 5 - Automatic Relevance Determination\n",
    "+ Use a different precision $\\alpha_i$ for each weight:\n",
    "$$\n",
    "p(w_j | \\alpha_j) \\propto \\exp\\left\\{-\\alpha_jw_j^2\\right\\},\n",
    "$$\n",
    "+ so that:\n",
    "$$\n",
    "p(\\mathbf{w}|\\boldsymbol{\\alpha}) = \\propto \\prod_{j=1}^mp(w_j|\\alpha_j).\n",
    "$$\n",
    "+ Then maximize the **evidence** with respect to all the $\\alpha_j$'s.\n",
    "+ **Sparsity**: When $\\alpha_j\\rightarrow\\infty$, $w_j=0$ identically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "ell = 2.\n",
    "Xc = np.linspace(0, 60, 50)\n",
    "phi = RadialBasisFunctions(Xc, ell)\n",
    "Phi = compute_design_matrix(X, phi)\n",
    "regressor = ARDRegression()\n",
    "regressor.fit(Phi, Y)\n",
    "# They are using different names:\n",
    "sigma = np.sqrt(1. / regressor.alpha_)\n",
    "print 'best sigma:', sigma\n",
    "alpha = regressor.lambda_\n",
    "print 'best alpha:', alpha\n",
    "A = np.dot(Phi.T, Phi) / sigma ** 2. + alpha * np.eye(Phi.shape[1])\n",
    "L = scipy.linalg.cho_factor(A)\n",
    "m = scipy.linalg.cho_solve(L, np.dot(Phi.T, Y) / sigma ** 2)  # The posterior mean of w\n",
    "S = scipy.linalg.cho_solve(L, np.eye(Phi.shape[1]))           # The posterior covariance of w\n",
    "Phi_p = compute_design_matrix(X_p, phi)\n",
    "Y_p = np.dot(Phi_p, m) # The mean prediction\n",
    "V_p_ep = np.einsum('ij,jk,ik->i', Phi_p, S, Phi_p) # The epistemic uncertainty\n",
    "S_p_ep = np.sqrt(V_p_ep)\n",
    "V_p = V_p_ep + sigma ** 2 # Full uncertainty\n",
    "S_p = np.sqrt(V_p)\n",
    "Y_l_ep = Y_p - 2. * S_p_ep  # Lower epistemic predictive bound\n",
    "Y_u_ep = Y_p + 2. * S_p_ep  # Upper epistemic predictive bound\n",
    "Y_l = Y_p - 2. * S_p # Lower predictive bound\n",
    "Y_u = Y_p + 2. * S_p # Upper predictive bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x', markeredgewidth=2, label='Observations')\n",
    "ax.plot(X_p, Y_p, label='Bayesian Prediction (Radial Basis Functions, ARD)')\n",
    "ax.fill_between(X_p.flatten(), Y_l_ep, Y_u_ep, color=sns.color_palette()[2], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_l, Y_l_ep, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.fill_between(X_p.flatten(), Y_u_ep, Y_u, color=sns.color_palette()[1], alpha=0.25)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Automatic Relevance Determination\n",
    "+ What about the input-dependent (heteroscedastic) noise? (ADVANCED).<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "+ Try ARD with the Fourier basis.\n",
    "+ Try ARD with the Step function basis.\n",
    "+ Try ARD with a basis that consists both of Fourier and RBFs. Which one's survive?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
